---
{"dg-publish":true,"permalink":"/notion/class/artificial-intelligence/mathematics-for-ai-chinese/"}
---

好的，这是您提供的全部讲义内容的中文翻译版本。翻译力求准确传达原文的数学概念，并保留了原有的结构和格式。

***

# 第一讲：线性代数：线性方程组、矩阵、向量空间、线性无关

## 第一部分：概念

*   **向量 (Vector):**
    可以进行加法和标量乘法（与标量相乘）运算的对象。这些运算必须满足特定的公理（例如，加法交换律，标量乘法对向量加法的分配律）。
    *   **示例：** 几何向量（二维/三维空间中的箭头）、多项式 ($ax^2+bx+c$)、音频信号、$\mathbb{R}^n$ 中的元组（例如 $(x_1, x_2, \ldots, x_n)$）。

*   **封闭性 (Closure):**
    一个集合相对于特定运算（这里指向量加法和标量乘法）的基本性质。它意味着，如果你从集合中任意取出两个元素进行运算，其结果*永远*也是该集合中的一个元素。如果一个非空向量集合在向量加法和标量乘法下满足封闭性（以及其他公理），它就构成一个**向量空间**。
    *   **重要性：** 封闭性确保了代数结构（集合及其运算）是自洽和一致的。它是定义向量空间的基石。
*   **线性方程组的解 (Solution of the linear equation system):**
    一个能够同时满足给定线性方程组中*所有*方程的 *n*-元组 $(x_1,\cdots,x_n)\in\mathbb R^n$。每个分量 $x_i$ 代表对应变量的值。
    *   **与向量的联系：** 每个这样的 *n*-元组本身就是 $\mathbb{R}^n$ 中的一个**向量**。因此，求解线性方程组等价于寻找满足给定条件的特定向量。
*   **一个方程组可以有：**
	*   **无解 (inconsistent)**
	*   **唯一解 (unique)**
	*   **无穷多解 (underdetermined)**
*   **矩阵表示法 (Matrix Notation):** ![Image/Class/Mathematics-for-AI/1.png](/img/user/Image/Class/Mathematics-for-AI/1.png)
    一个线性方程组可以用矩阵乘法紧凑地表示。对于一个包含 $m$ 个方程和 $n$ 个未知数的系统：
    $$\begin{aligned}
    a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \\
    a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2 \\
    \vdots \\
    a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_m
    \end{aligned}
    $$
    *   **可以写作：** $Ax=b$
        其中：
        *   $A = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{bmatrix}$ 是**系数矩阵**。
        *   $x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$ 是**变量向量**（或未知数向量）。
        *   $b = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{bmatrix}$ 是**常数向量**（或右侧向量）。

*   **增广矩阵 (Augmented Matrix):**
    在使用行变换（如高斯消元法）求解线性方程组 $Ax=b$ 时，将系数矩阵 $A$ 和常数向量 $b$ 组合成一个名为**增广矩阵**的单一矩阵会非常方便。
    *   **表示法：** 通常写作 `[A | b]`，其中竖线将系数矩阵与常数向量分开。
    *   **结构：**
        如果 $A$ 是一个 $m \times n$ 矩阵， $b$ 是一个 $m \times 1$ 的列向量，那么增广矩阵 `[A | b]` 是一个 $m \times (n+1)$ 的矩阵。
    $$[A | b] = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} & \bigm| & b_1 \\
        a_{21} & a_{22} & \cdots & a_{2n} & \bigm| & b_2 \\
        \vdots & \vdots & \ddots & \vdots & \bigm| & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn} & \bigm| & b_m
        \end{bmatrix}
        $$
    *   **目的：**
        这种表示法允许我们对整个系统（包括系数和常数）同时执行初等行变换，从而简化求解过程。增广矩阵的每一行直接对应线性系统中的一个方程。

	* **可以写作：** $Wx=b$

## 第二部分：矩阵运算

*   **矩阵加法:**
	* 对于 $A,B\in \mathbb R^{n\times m}$：$(A+B)_{ij}=a_{ij}+b_{ij}$
*   **矩阵乘法**
	* 对于 $A\in\mathbb R^{m\times n},B\in\mathbb R^{n\times k},C=AB\in\mathbb R^{m\times k}$：
	$$c_{ij}=\sum_{l=1}^na_{il}b_{lj}$$
	* **仅当内部维度匹配时才定义乘法：**  
	$$A_{m\times n}B_{n\times k}$$
	* **逐元素乘法**称为**哈达玛积 (Hadamard product)：**
	$$(A\circ B)_{ij}=a_{ij}b_{ij}$$
	* **单位矩阵 (Identity matrix):**
	$$I_n=\begin{pmatrix}1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1\end{pmatrix}\in \mathbb R^{n\times n}$$
	* **乘法单位元:**
	$$I_nA=AI_n=A\in \mathbb R^{m\times n}$$
	* **代数性质:**
		* 结合律: $(AB)C=A(BC)$
		* 分配律: $(A+B)C=AC+BC,A(C+D)=AC+AD$
*   **矩阵的逆 (Matrix Inverse):** 对于 $A,B\in R^{n\times n}$，如果 $AB=I_n=BA$，则称 $B$ 是 $A$ 的逆矩阵。记作 $A^{-1}$。
	* **可逆性：** 如果 $A^{-1}$ 存在，则称 $A$ 是正则的/可逆的/非奇异的。否则，它是奇异的/不可逆的。
	* **唯一性：** 如果 $A^{-1}$ 存在，则它是唯一的。
	* $A^{-1}$ 存在 $\iff$ $|A|\neq 0:$ 
	$$A^{-1}=\dfrac{1}{|A|}\text{adj}(A)$$
	* **性质：**
		* $AA^{-1}=I=A^{-1}A$
		* $(AB)^{-1}=B^{-1}A^{-1}$
*   **矩阵转置 (Matrix Transpose)**
	* **转置：** 对于 $A\in R^{m\times n}$ , $A^T\in R^{n\times m}$ 定义为 $(A^T)_{ij}=a_{ji}$。
	* **性质：**
		* $(A^T)^T=A$
		* $(AB)^T=B^TA^T$
		* $(A+B)^T=A^T+B^T$
		* 如果 $A$ 可逆，则 $(A^{-1})^T=(A^T)^{-1}$
*   **对称矩阵 (Symmetric Matrix):** 如果 $A=A^T$，则 $A\in \mathbb R^{n\times n}$ 是对称的。
	* **和：** 对称矩阵的和仍然是对称的。
	* **性质：**
		* **合同变换下的对称性:** 
		$$PAP^T=(PAP^T)^T$$
		* **实对称矩阵的可对角化性：**
			* 每个实对称矩阵都是**可正交对角化的**。这意味着存在一个正交矩阵 $Q$ (其中 $Q^TQ=I$) 和一个对角矩阵 $D$，使得 $A=QDQ^T$。$D$ 的对角线元素是 $A$ 的特征值，而 $Q$ 的列是对应的标准正交特征向量。
*   **标量乘法:** 
$$(\lambda A)_{ij}=\lambda(A_{ij})$$
*  **解：**
	*   **考虑系统：**
	$$ \begin{bmatrix}1 & 0 & 8 & -4 \\ 0 & 1 & 2 & 12 \end{bmatrix}
	    \begin{bmatrix}
	    x_1 \\
	    x_2 \\
	    x_3 \\
	    x_4
	    \end{bmatrix}
	    =
	    \begin{bmatrix}
	    42 \\
	    8
	    \end{bmatrix}
	    $$
	*   **两个方程，四个未知数：** 系统是**欠定的 (underdetermined)**，因此我们期望有无穷多解。
	*   前两列构成一个单位矩阵。这意味着 $x_1$ 和 $x_2$ 是**主元变量**（或基本变量），而 $x_3$ 和 $x_4$ 是**自由变量**。
	    *   为了找到一个特解，我们可以将自由变量设为零。
	    *   设 $x_3 = 0$ 和 $x_4 = 0$ 得到：
	        *   从第一行：$1 \cdot x_1 + 0 \cdot x_2 + 8 \cdot 0 - 4 \cdot 0 = 42 \implies x_1 = 42$
	        *   从第二行：$0 \cdot x_1 + 1 \cdot x_2 + 2 \cdot 0 + 12 \cdot 0 = 8 \implies x_2 = 8$
	*   **因此，$[42, 8, 0, 0]^T$ 是一个特解（也称为特殊解）。**
	*   为了找到非齐次系统 `Ax = b` 的**通解**（描述所有无穷多解），我们需要理解相关的**齐次系统** `Ax = 0` 的解。
	*   **考虑齐次系统：**
	$$\begin{bmatrix}
	    1 & 0 & 8 & -4 \\
	    0 & 1 & 2 & 12
	    \end{bmatrix}
	    \begin{bmatrix}
	    x_1 \\
	    x_2 \\
	    x_3 \\
	    x_4
	    \end{bmatrix}
	    =
	    \begin{bmatrix}
	    0 \\
	    0
	    \end{bmatrix}
	    $$
	*   同样，$x_1$ 和 $x_2$ 是主元变量，$x_3$ 和 $x_4$ 是自由变量。我们用自由变量来表示主元变量：
	    *   从第一行：$x_1 + 8x_3 - 4x_4 = 0 \implies x_1 = -8x_3 + 4x_4$
	    *   从第二行：$x_2 + 2x_3 + 12x_4 = 0 \implies x_2 = -2x_3 - 12x_4$
	*   设自由变量为参数：$x_3 = s$ 和 $x_4 = t$，其中 $s, t \in \mathbb{R}$。
	*   **齐次解 ($x_h$)** 可以写成向量形式：
	$$x_h = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix} = \begin{bmatrix} -8s + 4t \\ -2s - 12t \\ s \\ t \end{bmatrix} = s \begin{bmatrix} -8 \\ -2 \\ 1 \\ 0 \end{bmatrix} + t \begin{bmatrix} 4 \\ -12 \\ 0 \\ 1 \end{bmatrix}
	 $$
	    向量 $\begin{bmatrix} -8 \\ -2 \\ 1 \\ 0 \end{bmatrix}$ 和 $\begin{bmatrix} 4 \\ -12 \\ 0 \\ 1 \end{bmatrix}$ 构成了矩阵 $A$ 的**零空间 (null space)** 的一组基，记为 $N(A)$。这些有时也被称为 $Ax=0$ 的**特殊解**。
	
	*   **`Ax = b` 的通解：**
	    一个相容的线性系统 `Ax = b` 的完整解集是任意一个特解 $x_p$ 与整个零空间 $N(A)$ 的和。
	$$\mathbf{x} = x_p + x_h = x_p + N(A)
	    $$
	    使用我们的具体例子：
	$$\mathbf{x} = \begin{bmatrix} 42 \\ 8 \\ 0 \\ 0 \end{bmatrix} + s \begin{bmatrix} -8 \\ -2 \\ 1 \\ 0 \end{bmatrix} + t \begin{bmatrix} 4 \\ -12 \\ 0 \\ 1 \end{bmatrix} \quad \text{对于任意 } s, t \in \mathbb{R}
	    $$
	    这个公式描述了原始系统 `Ax = b` 的所有无穷多解。

*  **秩-零度定理 (Rank-Nullity Theorem)**

	*   **定理陈述：**
	    对于一个 $m \times n$ 矩阵 $A$， $A$ 的**秩**加上 $A$ 的**零度**等于列数 $n$。
	    即：
	    `rank(A) + nullity(A) = n`
	    这也意味着：
	    `nullity(A) = n - rank(A)`
	
	*   **术语解释：**
	    *   **A 的秩 (rank(A)):**
	        *   定义：矩阵 $A$ 的**列空间 (Col(A))** 的维度。它等于 $A$ 的行简化阶梯型 (RREF) 中**主元变量**的数量。
	    *   **A 的零度 (nullity(A)):**
	        *   定义：矩阵 $A$ 的**零空间 (Nul(A))** 的维度。它等于 $A$ 的行简化阶梯型 (RREF) 中**自由变量**的数量。
	    *   **$n$ (列数 / 变量数):**
	        *   定义：矩阵 $A$ 的列数，代表系统中的未知数总数。
	
	*   **直观意义：**
	    该定理从根本上表明，一个系统中的总变量数 ($n$) 被分为两部分：一部分受方程约束，其数量为**秩**；另一部分是解中可以自由选择的变量，其数量为**零度**。即，**（主元变量数）+（自由变量数）=（总变量数）**。
	
	*   **示例（使用 2x4 矩阵）：**
	    *   考虑我们之前讨论的矩阵 $A = \begin{bmatrix} 1 & 0 & 8 & -4 \\ 0 & 1 & 2 & 12 \end{bmatrix}$。
	    *   这里，列数 $n = 4$（因为有四个未知数 $x_1, x_2, x_3, x_4$）。
	    *   该矩阵已经处于行简化阶梯型。
	        *   **主元变量：** $x_1, x_2$（对应于每行中的首个 1）。因此，`rank(A) = 2`。
	        *   **自由变量：** $x_3, x_4$（不对应主元位置的变量）。因此，`nullity(A) = 2`。
	    *   **验证定理：**
	        *   `rank(A) + nullity(A) = 2 + 2 = 4`。这与列数 $n=4$ 相符。
	        *   `nullity(A) = n - rank(A) \implies 2 = 4 - 2`。这也完全成立。
	    *   这个例子完美地说明了秩-零度定理。

* **初等行变换 (Elementary Row Transformations)**
	*   **定义：**
	    初等行变换是可以对矩阵的行执行的一组操作。这些操作至关重要，因为它们将一个矩阵转换为一个等价矩阵（意味着它们保留了相应线性系统的解集，以及矩阵的行空间、列空间维度和零空间）。
	
	*   **初等行变换的类型：**
	    有三种基本的初等行变换：
	
	    1.  **行交换 (Interchange Two Rows):**
	        *   **描述：** 交换两行的位置。
	        *   **表示法：** $R_i \leftrightarrow R_j$ (交换第 $i$ 行和第 $j$ 行)
	
	    2.  **行缩放 (Multiply a Row by a Non-zero Scalar):**
	        *   **描述：** 将一行中的所有元素乘以一个非零常数标量。
	        *   **表示法：** $k R_i \to R_i$ (将第 $i$ 行乘以标量 $k$，其中 $k \neq 0$)
	
	    3.  **行加法 (Add a Multiple of One Row to Another Row):**
	        *   **描述：** 将一行的标量倍数加到另一行上。被加的行被结果替换。
	        *   **表示法：** $R_i + k R_j \to R_i$ (将 $k$ 倍的第 $j$ 行加到第 $i$ 行，并替换第 $i$ 行)
	
	*   **目的和重要性：**
	    *   **求解线性系统：** 初等行变换是**高斯消元法**和**高斯-若尔当消元法**的基础，这些算法通过将增广矩阵转换为行阶梯型或行简化阶梯型来求解线性方程组。
	    *   **求矩阵的逆：** 它们可用于求方阵的逆。
	    *   **确定秩：** 它们有助于找到矩阵的秩（[[Notion/Class/Concept/REF, RREF\|REF, RREF]] 中主元/非零行的数量）。
	    *   **寻找零空间基：** 它们对于将矩阵转换为 RREF 以识别自由变量并确定零空间的基至关重要。
	    *   **等价性：** 如果一个矩阵可以通过一系列初等行变换转换为另一个矩阵，则这两个矩阵是**行等价**的。行等价的矩阵具有相同的行空间、零空间，因此具有相同的秩。
	*   **重要性：**
		* 如果矩阵是：
	$$\begin{bmatrix}
		\mathbf{1} & 0 & 0 & 5 & \bigm| & 10 \\
		0 & \mathbf{1} & 0 & -2 & \bigm| & 7 \\
		0 & 0 & 0 & 0 & \bigm| & a+1
		\end{bmatrix}$$
		* 当且仅当 $a=-1$ 时，系统有解。
		*  **`[0 0 0 0 | 0]` 这一行意味着什么？**
		    *   一行全为零，包括常数项，意味着与此行对应的原始方程是系统中**其他方程的线性组合**。换句话说，这个方程是多余的，没有提供关于变量的新信息。
		    *   关键的是，`0 = 0` 永远是一个真命题。这表明系统是**相容的**（有解）。它并**不**意味着没有解（一个不相容的系统会有一行像 `[0 0 0 0 | c]`，其中 `c ≠ 0`）。

* **行等价矩阵 (Row Equivalent Matrices)**
	*   **定义：**
	    如果一个矩阵可以通过有限次的初等行变换从另一个矩阵得到，则称这两个矩阵是**行等价**的。
	*   **表示法：**
	    如果矩阵 $A$ 与矩阵 $B$ 行等价，我们写作 $A \sim B$，$B$ 也可写作 $\overset{\sim}A$。
	*   **行等价矩阵的关键性质（保持不变的特性）：**
	    初等行变换之所以强大，是因为它们保留了矩阵的几个基本性质，这些性质对于求解线性系统和理解矩阵空间至关重要：
	
	    1.  **线性系统具有相同的解集：** 如果增广矩阵 $[A | b]$ 与另一个增广矩阵 $[A' | b']$ 行等价，那么线性系统 $Ax=b$ 与 $A'x=b'$ 具有完全相同的解集。
	    2.  **相同的行空间：** 行空间（由矩阵行向量张成的向量空间）在初等行变换下保持不变。
	    3.  **相同的零空间：** 零空间（齐次方程 $Ax=0$ 的所有解的集合）保持不变。
	    4.  **相同的秩：** 由于行空间和零空间的维度被保留，矩阵的秩（等于列空间的维度，也等于行空间的维度）也被保留。
	    5.  **相同的行简化阶梯型 (RREF)：** 每个矩阵都行等价于一个唯一的行简化阶梯型 (RREF)。
	
*   **通过增广矩阵计算逆矩阵：**
    行简化阶梯型 (RREF) 对于求矩阵的逆非常有用。这种策略也被称为**高斯-若尔当消元法求逆**。
    *   **要求：** 矩阵 **A 必须是方阵** ($A \in \mathbb{R}^{n \times n}$)。
    *   **核心思想：** 计算 $A^{-1}$ 实质上是求解矩阵方程 $AX = I_n$。
    *   **步骤：**
        1.  **写出增广矩阵 `[A | I_n]`：**
        2.  **执行高斯消元法（行化简）：** 使用初等行变换将左侧的 $A$ 变为单位矩阵 $I_n$。
            $$ [A | I_n] \xrightarrow{\text{高斯消元}} [I_n | A^{-1}] $$
        3.  **读取逆矩阵：** 如果左侧成功变为 $I_n$，那么右侧的矩阵就是 $A^{-1}$。
        4.  **不可逆的情况：** 如果在行化简过程中，左侧无法变为 $I_n$（例如，出现一个全零行），则矩阵 $A$ 是奇异的（不可逆），$A^{-1}$ 不存在。
        5.  **证明：** [[Notion/Class/Proof/Compatibility of Matrix Multiplication with Partitioned Matrices\|Compatibility of Matrix Multiplication with Partitioned Matrices]]
	        $$C[A|I_n]=[CA|CI_n]=[I_n|C]\Rightarrow C=A^{-1}$$
    *   **限制：** 对于非方阵，此方法不适用。

* **求解线性系统 (`Ax = b`) 的算法：直接法**
	*   **1. 直接求逆法：**
	    *   **适用性：** 当系数矩阵 $A$ 是**方阵且可逆**时。
	    *   **公式：** $x = A^{-1}b$
	
	*   **2. 伪逆法 ([[Notion/Class/Proof/Moore Penrose Pseudo inverse\|Moore Penrose Pseudo inverse]]):**
	    *   **适用性：** 当 $A$ **不是方阵但列线性无关**（即满列秩）时。这在超定系统（方程多于未知数, $m > n$）中很常见。
	    *   **公式：** $x = (A^T A)^{-1} A^T b$
	    *   **结果：** 这个公式给出了**最小范数最小二乘解**。它找到使残差的欧几里得范数 $\|Ax - b\|_2^2$ 最小化的向量 $x$。
	
	*   **局限性 (求逆法和伪逆法的共同点):**
	    *   **计算成本高：** 计算矩阵的逆或伪逆通常**计算成本很高**，对于一个 $n \times n$ 矩阵，其计算成本通常为 $O(n^3)$。
	    *   **数值不稳定：** 对于大型或病态系统，这些方法可能**数值不稳定**。
	
	*   **3. 高斯消元法：**
	    *   **机制：** 一种系统性的方法，通过一系列初等行变换将增广矩阵 `[A | b]` 简化为行阶梯型或行简化阶梯型来求解 `Ax = b`。
	    *   **可扩展性：** 高斯消元法对于数千个变量通常是**高效的**。但对于非常大的系统（例如数百万个变量）则**不实用**，因为其计算成本随变量数量呈立方级增长 ($O(n^3)$)。

## 第三部分：向量空间与群

*   **群 (Group):**
	一个**群**是一个集合 $G$ 与一个二元运算 $*$ 的组合，该运算满足以下四个公理：
	
	1.  **封闭性：** 对所有 $a, b \in G$，$a * b$ 的结果也在 $G$ 中。
	2.  **结合律：** 对所有 $a, b, c \in G$，$(a * b) * c = a * (b * c)$。
	3.  **单位元：** 存在一个元素 $e \in G$（称为单位元），使得对每个 $a \in G$，$a * e = e * a = a$。
	4.  **逆元：** 对每个 $a \in G$，存在一个元素 $a^{-1} \in G$（称为 $a$ 的逆元），使得 $a * a^{-1} = a^{-1} * a = e$。
	
	---
	
	**附加术语：**
	*   **阿贝尔群 (Abelian Group)（交换群）：** 如果运算 $*$ 还满足**交换律**（即对所有 $a, b \in G$，$a * b = b * a$），则该群称为阿贝尔群。
	
	![Image/Class/Mathematics-for-AI/2.png](/img/user/Image/Class/Mathematics-for-AI/2.png)
    ![Image/Class/Mathematics-for-AI/3.png](/img/user/Image/Class/Mathematics-for-AI/3.png)
  ![Image/Class/Mathematics-for-AI/4.png](/img/user/Image/Class/Mathematics-for-AI/4.png)

### 笔记续

*   **向量空间 (Vector Space):**
    一个**向量空间**是一个由称为**向量**的对象组成的集合 ($V$)，以及一个**标量**集合（通常是实数 $\mathbb{R}$），配备了**向量加法**和**标量乘法**两种运算。这些运算必须满足十个公理。

    **向量空间的公理：**
    设 $\mathbf{u}, \mathbf{v}, \mathbf{w}$ 是 $V$ 中的向量， $c, d$ 是 $\mathbb{R}$ 中的标量。

    1.  **加法封闭性：** $\mathbf{u} + \mathbf{v}$ 在 $V$ 中。
    2.  **加法交换律：** $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$。
    3.  **加法结合律：** $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$。
    4.  **零向量（加法单位元）：** $V$ 中存在一个向量 $\mathbf{0}$，使得 $\mathbf{u} + \mathbf{0} = \mathbf{u}$。
    5.  **加法逆元：** 对每个向量 $\mathbf{u}$，$V$ 中存在一个向量 $-\mathbf{u}$，使得 $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$。
        *   **与群的联系：** 前五个公理意味着向量集合 $V$ 与加法运算 $(V, +)$ 构成一个**阿贝尔群**。

    6.  **标量乘法封闭性：** $c\mathbf{u}$ 在 $V$ 中。
    7.  **分配律：** $c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}$。
    8.  **分配律：** $(c+d)\mathbf{u} = c\mathbf{u} + d\mathbf{u}$。
    9.  **标量乘法结合律：** $c(d\mathbf{u}) = (cd)\mathbf{u}$。
    10. **标量单位元：** $1\mathbf{u} = \mathbf{u}$。

*   **子空间 (Subspace):**
    向量空间 $V$ 的一个**子空间**是 $V$ 的一个子集 $H$，它本身在 $V$ 上定义的相同加法和标量乘法运算下也是一个向量空间。
    *   **子空间判别法：** 要验证一个子集 $H$ 是否为子空间，只需检查三个条件：
        1.  **包含零向量：** $V$ 的零向量在 $H$ 中（$\mathbf{0} \in H$）。
        2.  **加法封闭性：** 对任意两个向量 $\mathbf{u}, \mathbf{v} \in H$，它们的和 $\mathbf{u} + \mathbf{v}$ 也在 $H$ 中。
        3.  **标量乘法封闭性：** 对任意向量 $\mathbf{u} \in H$ 和任意标量 $c$，向量 $c\mathbf{u}$ 也在 $H$ 中。
    *   **关键示例：**
        *   $\mathbb{R}^3$ 中任何穿过原点的直线或平面都是 $\mathbb{R}^3$ 的子空间。
        *   $m \times n$ 矩阵 $A$ 的**零空间**，记为 $N(A)$，是 $\mathbb{R}^n$ 的一个子空间。
        *   $m \times n$ 矩阵 $A$ 的**列空间**，记为 $Col(A)$，是 $\mathbb{R}^m$ 的一个子空间。

*   **线性组合 (Linear Combination):**
    给定向量空间 $V$ 中的向量 $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p$ 和标量 $c_1, c_2, \ldots, c_p$，由下式定义的向量 $\mathbf{y}$：
    $$\mathbf{y} = c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_p\mathbf{v}_p$$
    被称为 $\mathbf{v}_1, \ldots, \mathbf{v}_p$ 的一个**线性组合**，其权重为 $c_1, \ldots, c_p$。

*   **生成空间 (Span):**
    *   **定义：** 一组向量 $\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$ 的**生成空间**，记为 $\text{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$，是这些向量所有可能的**线性组合**的集合。
    *   **几何解释：**
        *   $\text{Span}\{\mathbf{v}\}$（其中 $\mathbf{v} \neq \mathbf{0}$）是穿过原点和 $\mathbf{v}$ 的直线。
        *   $\text{Span}\{\mathbf{u}, \mathbf{v}\}$（其中 $\mathbf{u}, \mathbf{v}$ 不共线）是包含原点、$\mathbf{u}$ 和 $\mathbf{v}$ 的平面。
    *   **性质：** 任何向量集的生成空间永远是一个**子空间**。

*   **线性无关与线性相关 (Linear Independence and Dependence):**
    *   **线性无关：** 如果向量方程 $c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_p\mathbf{v}_p = \mathbf{0}$ **只有平凡解**（$c_1 = c_2 = \cdots = c_p = 0$），则向量集 $\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$ 是**线性无关的**。
    *   **线性相关：** 如果存在**不全为零**的权重 $c_1, \ldots, c_p$ 使得该方程成立，则该集合是**线性相关的**。
    *   **直观意义：** 一组向量是线性相关的，当且仅当至少有一个向量可以写成其他向量的线性组合。线性无关的向量是无冗余的。

*   **基 (Basis):**
    向量空间 $V$ 的一个**基**是一个向量集合 $B = \{\mathbf{b}_1, \ldots, \mathbf{b}_n\}$，它满足两个条件：
    1.  集合 $B$ 是**线性无关的**。
    2.  集合 $B$ **生成**整个向量空间 $V$（即 $\text{Span}(B) = V$）。
    *   基是构建整个空间所需的“最小”向量集合。

*   **维度 (Dimension):**
    *   **定义：** 非零向量空间 $V$ 的**维度**，记为 $\text{dim}(V)$，是 $V$ 的**任意一个基中的向量数量**。零向量空间 $\{\mathbf{0}\}$ 的维度定义为 0。
    *   **唯一性：** 虽然一个向量空间可以有许多不同的基，但给定向量空间的所有基都具有相同数量的向量。
    *   **与秩-零度定理的联系：**
        *   矩阵 $A$ 的列空间的维度是其秩：$\text{dim}(\text{Col}(A)) = \text{rank}(A)$。
        *   矩阵 $A$ 的零空间的维度是其零度：$\text{dim}(N(A)) = \text{nullity}(A)$。
*   **使用高斯消元法检验线性无关性：**
    要检验一组向量 $\{\mathbf{v}_1, \ldots, \mathbf{v}_k\}$ 是否线性无关：
    1.  将这些向量作为列，构成一个矩阵 $A$。
    2.  对矩阵 $A$ 执行高斯消元，将其化为**行阶梯型**。

    *   与**主元列**对应的原始向量是线性无关的。
    *   与**非主元列**对应的向量可以写成前面主元列的线性组合。

    **示例：** 在以下行阶梯型矩阵中：
    $$
    \begin{pmatrix}
      \mathbf{1} & 3 & 0 \\
      0 & 0 & \mathbf{1}
    \end{pmatrix}
    $$
    第 1 列和第 3 列是**主元列**（它们对应的原始向量是无关的）；第 2 列是**非主元列**（它对应的原始向量是相关的）。

    因此，原始向量集（矩阵 $A$ 的列）**不是线性无关的**，因为至少有一个非主元列。换句话说，该集合是**线性相关的**。
*   **线性组合的线性无关性**
    假设我们有一组 $k$ 个线性无关的向量 $\{\mathbf{b}_1, \ldots, \mathbf{b}_k\}$，可以看作是 $k$ 维空间的一个基。我们可以构造一组新的 $m$ 个向量 $\{\mathbf{x}_1, \ldots, \mathbf{x}_m\}$，其中每个 $\mathbf{x}_j$ 都是基向量的线性组合：
    $$ \mathbf{x}_j = \sum_{i=1}^{k} \lambda_{ij} \mathbf{b}_i $$
    每组权重可以表示为一个**系数向量** $\boldsymbol{\lambda}_j \in \mathbb{R}^k$。

    *   **关键推论：** 新向量集 $\{\mathbf{x}_1, \ldots, \mathbf{x}_m\}$ 是线性无关的，*当且仅当* 它们对应的系数向量集 $\{\boldsymbol{\lambda}_1, \ldots, \boldsymbol{\lambda}_m\}$ 是线性无关的。
    
*   **生成集的维度定理 (一个基本定理)**
     向量空间的维度不能超过其任何生成集中的向量数量。一个直接的推论是，在维度为 $k$ 的向量空间中，任何包含超过 $k$ 个向量的集合必定是线性相关的。

*   **特殊情况：新向量数量多于基向量数量 ($m > k$)**

    **定理：** 如果你用 $k$ 个线性无关的向量来生成 $m$ 个新向量，并且 $m > k$，那么得到的新向量集 $\{\mathbf{x}_1, \ldots, \mathbf{x}_m\}$ **永远是线性相关的**。

    **证明 (使用矩阵的秩)：**

    1.  **关注系数向量：** 如上所述，$\{\mathbf{x}_j\}$ 的线性无关性等价于其系数向量 $\{\boldsymbol{\lambda}_j\}$ 的线性无关性。我们将证明集合 $\{\boldsymbol{\lambda}_1, \ldots, \boldsymbol{\lambda}_m\}$ 必定是线性相关的。

    2.  **构造系数矩阵：** 让我们将这些系数向量排列成一个矩阵 $\Lambda$ 的列：
        $$ \Lambda = [\boldsymbol{\lambda}_1, \boldsymbol{\lambda}_2, \ldots, \boldsymbol{\lambda}_m] $$
        由于每个系数向量 $\boldsymbol{\lambda}_j$ 都在 $\mathbb{R}^k$ 中，矩阵 $\Lambda$ 有 $k$ 行和 $m$ 列（它是一个 $k \times m$ 矩阵）。

    3.  **分析矩阵的秩：** 矩阵的**秩**有一个基本性质：它不能超过其行数或列数。具体来说，我们关心的是 $\text{rank}(\Lambda) \le k$（行数）。
        *   *（通过一个更基本的定理证明：秩是行空间的维度。行空间由 $k$ 个行向量生成。根据[[Notion/Class/Proof/The Dimension Theorem for Spanning Sets\|The Dimension Theorem for Spanning Sets]]，这个空间的维度不能超过 $k$）。*

    4.  **应用条件 $m > k$：** 我们已经确定了关于矩阵 $\Lambda$ 的两个关键事实：
        *   总列数为 $m$。
        *   秩，代表线性无关列的最大数量，最多为 $k$。即 $\text{rank}(\Lambda) \le k$。

    5.  **将秩与线性相关性联系起来：** 我们已知 $m > k$。这导致了一个关键的不等式：
        $$ \text{总列数 } (m) > \text{线性无关列的最大数量 } (\text{rank}(\Lambda)) $$
        这个不等式意味着 $\Lambda$ 的所有 $m$ 个列不可能都是线性无关的。如果你拥有的向量数量（$m$）超过了它们可以生成的空间的维度（秩，最多为 $k$），那么这个向量集必定是线性相关的。

    6.  **得出结论：** 因为 $\Lambda$ 的列（即系数向量 $\{\boldsymbol{\lambda}_j\}$）构成一个线性相关的集合，所以由它们定义的新向量集 $\{\mathbf{x}_j\}$ 也必定是**线性相关的**。证毕。

***

# 第二讲：线性代数：基与秩、线性映射、仿射空间

## 第一部分：基与秩

*   **生成集 (Generating Set or Spanning Set)**
    *   **定义：** 如果 $\text{Span}(S) = V$，则向量集 $S$ 被称为向量空间 $V$ 的一个**生成集**。
    *   **关键思想：** 生成集可能是**冗余的**；它可能包含线性相关的向量。
    *   **示例：** 集合 $S = \{(1, 0), (0, 1), (1, 1)\}$ 是 $\mathbb{R}^2$ 的一个生成集。它是冗余的，因为 $(1, 1)$ 是另外两个向量的线性组合。

*   **生成空间 (Span) 的附加属性**
    *   **与线性系统的联系：** 线性方程组 $A\mathbf{x} = \mathbf{b}$ 有解，当且仅当向量 $\mathbf{b}$ 位于矩阵 $A$ 的列的生成空间中。即 $\mathbf{b} \in \text{Col}(A)$。

*   **基 (Basis) 的附加属性**
    *   **唯一表示定理：** 基的一个关键性质是，空间中的每个向量 $\mathbf{v}$ 都可以用**唯一的方式**表示为基向量的线性组合。这个唯一组合的系数被称为 $\mathbf{v}$ 相对于该基的**坐标**。
    *   **示例（标准基）：** $\mathbb{R}^n$ 最常用的基是**标准基**，它由 $n \times n$ 单位矩阵 $I_n$ 的列组成。对于 $\mathbb{R}^3$，标准基是 $\{\mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3\} = \left\{ \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} \right\}$。
    
*   **基的特征**
    对于向量空间 $V$ 中的一个非空向量集 $B$，以下陈述是等价的（即如果一个为真，则全部为真）：
    1.  $B$ 是 $V$ 的一个**基**。
    2.  $B$ 是一个**最小生成集**（即它生成 $V$，但 $B$ 的任何真子集都不能生成 $V$）。
    3.  $B$ 是一个**最大线性无关集**（即它是线性无关的，但向其中添加任何其他来自 $V$ 的向量都会使该集线性相关）。

*   **维度的进一步性质**

    *   **存在性与规模的唯一性：** 每个非平凡的向量空间都有一个基。虽然一个空间可以有许多不同的基，但它们都将具有相同数量的向量。这使得维度的概念是明确定义的。
    *   **子空间维度：** 如果 $U$ 是向量空间 $V$ 的一个子空间，那么 $\text{dim}(U) \le \text{dim}(V)$。等号成立当且仅当 $U = V$。
    *   **重要澄清：** 空间的维度指的是其**基中向量的数量**，而不是每个向量中的分量数量。例如，由单个向量 $\begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$ 生成的子空间是一维的，尽管该向量存在于 $\mathbb{R}^3$ 中。

*   **如何为子空间找到一个基（基提取法）**

    为由一组向量 $\{\mathbf{v}_1, \ldots, \mathbf{v}_m\}$ 的生成空间定义的子空间 $U$ 找到一个基：
    1.  创建一个矩阵 $A$，其列为向量 $\{\mathbf{v}_1, \ldots, \mathbf{v}_m\}$。
    2.  将矩阵 $A$ 化简为其**行阶梯型**。
    3.  识别包含**主元**的列。
    4.  $U$ 的基由集合 $\{\mathbf{v}_1, \ldots, \mathbf{v}_m\}$ 中与这些主元列对应的**原始向量**组成。
    
*   **矩阵的秩**

    *   **定义：** 矩阵 $A$ 的**秩**，记为 $\text{rk}(A)$，是 $A$ 中线性无关列的数量。线性代数的一个基本定理指出，这个数字总是等于线性无关行的数量。
    *   **关键性质：** 矩阵的秩等于其转置的秩：$\text{rk}(A) = \text{rk}(A^T)$。

*   **秩及其与基本子空间的联系**

    *   **列空间 (像/值域):** $A$ 的秩是其列空间的维度。
        $\text{rk}(A) = \text{dim}(\text{Col}(A))$
    *   **零空间 (核):** 秩通过秩-零度定理确定了零空间的维度。对于一个 $m \times n$ 矩阵 $A$：
        $\text{dim}(\text{Nul}(A)) = n - \text{rk}(A)$

*   **秩的性质和应用**

    *   **方阵的可逆性：** 一个 $n \times n$ 矩阵 $A$ 是可逆的，当且仅当其秩等于其维度，即 $\text{rk}(A) = n$。
    *   **线性系统的可解性：** 系统 $A\mathbf{x} = \mathbf{b}$ 至少有一个解，当且仅当系数矩阵 $A$ 的秩等于增广矩阵 $[A|\mathbf{b}]$ 的秩。
    *   **满秩和秩亏：**
        *   如果一个矩阵的秩是其维度可能的最大值，则该矩阵为**满秩**：$\text{rk}(A) = \min(m, n)$。
        *   如果 $\text{rk}(A) < \min(m, n)$，则矩阵是**秩亏的**，表明其行或列之间存在线性相关性。

*   **为什么秩很重要**

    矩阵的秩是揭示其基本结构的核心概念。它告诉我们：
    *   线性无关行/列的最大数量。
    *   数据的维度（由列生成的子空间的维度）。
    *   线性系统是否相容（有解）。
    *   方阵是否有逆。
    *   它对于识别冗余和简化数据分析、优化和机器学习中的问题至关重要。

*   **总结：将秩、基和主元联系起来**
    1.  你从一组向量开始。
    2.  你将它们作为列放入矩阵 $A$ 中。
    3.  你执行高斯消元法找到**主元**。
    4.  **主元的数量**就是矩阵 $A$ 的**秩**。
    5.  这个秩也是由原始向量生成的子空间的**维度**。
    6.  与**主元列**对应的**原始向量**构成了该子空间的一个**基**。

## 第二部分：线性映射

*   **线性映射 (Linear Mappings / Linear Transformations)**
    *   **定义：** 从向量空间 $V$到向量空间 $W$ 的一个映射（或函数）$\Phi: V \to W$ 被称为**线性的**，如果它保持了两个基本的向量空间运算：
        1.  **可加性：** 对所有 $\mathbf{x}, \mathbf{y} \in V$，$\Phi(\mathbf{x} + \mathbf{y}) = \Phi(\mathbf{x}) + \Phi(\mathbf{y})$。
        2.  **齐次性：** 对任何标量 $\lambda$，$\Phi(\lambda\mathbf{x}) = \lambda\Phi(\mathbf{x})$。
    *   **矩阵表示：** 任何有限维向量空间之间的线性映射都可以通过矩阵乘法来表示：$\Phi(\mathbf{x}) = A\mathbf{x}$，其中 $A$ 是某个矩阵。

*   **映射的性质：单射、满射、双射**
    *   **单射 (Injective / One-to-one)：** 如果不同的输入总是映射到不同的输出，则映射是单射的。
    *   **满射 (Surjective / Onto)：** 如果映射的值域等于其陪域，则映射是满射的。这意味着目标空间 $W$ 中的每个元素都是起始空间 $V$ 中至少一个元素的像。
    *   **双射 (Bijective)：** 如果一个映射既是单射又是满射，则它是双射的。双射映射有一个唯一的逆映射 $\Phi^{-1}$。

*   **特殊类型的线性映射**
    *   **同态 (Homomorphism)：** 对于向量空间，同态只是**线性映射**的另一个术语。
    *   **同构 (Isomorphism)：** 既是线性映射又是**双射**的映射。同构的向量空间在结构上是相同的。
    *   **自同态 (Endomorphism)：** 从一个向量空间**到其自身**的线性映射（$\Phi: V \to V$）。
    *   **自同构 (Automorphism)：** 既是自同态又是**双射**的映射。它是从一个向量空间到其自身的同构（例如，旋转或反射）。
    *   **恒等映射 (Identity Mapping)：** 由 $\text{id}(\mathbf{x}) = \mathbf{x}$ 定义的映射。

*   **同构 (Isomorphism)**
    *   **同构与维度：** 一个基本定理指出，两个有限维向量空间 $V$ 和 $W$ 是**同构的**（结构上相同），当且仅当它们具有相同的维度。
         $\text{dim}(V) = \text{dim}(W) \iff V \cong W$
    *   **直观理解：** 这意味着任何n维向量空间本质上都是 $\mathbb{R}^n$ 的一个“重新标记”。

*   **通过有序基的矩阵表示**
    通过选择一个**有序基**，抽象的n维空间 $V$ 和具体的空间 $\mathbb{R}^n$ 之间的同构关系变得实用。基向量的顺序对于定义坐标很重要。
    *   **表示法：** 我们用括号表示一个有序基，例如 $B = (\mathbf{b}_1, \ldots, \mathbf{b}_n)$。

*   **坐标和坐标向量**
    *   **定义：** 给定 $V$ 的一个有序基 $B = (\mathbf{b}_1, \ldots, \mathbf{b}_n)$，每个向量 $\mathbf{x} \in V$ 都可以唯一地写成：
        $$ \mathbf{x} = \alpha_1\mathbf{b}_1 + \cdots + \alpha_n\mathbf{b}_n $$
        标量 $\alpha_1, \ldots, \alpha_n$ 被称为 $\mathbf{x}$ 相对于基 $B$ 的**坐标**。
    *   **坐标向量：** 我们将这些坐标收集成一个列向量，它在标准空间 $\mathbb{R}^n$ 中表示 $\mathbf{x}$：
        $$ [\mathbf{x}]_B = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix} \in \mathbb{R}^n $$

*   **坐标系和基变换**
    *   **概念：** 一个基为向量空间定义了一个坐标系。$\mathbb{R}^2$ 中熟悉的笛卡尔坐标只是相对于标准基 $(\mathbf{e}_1, \mathbf{e}_2)$ 的坐标。任何其他基都定义了一个不同但同样有效的坐标系。

*   **对线性映射的重要性**
    一旦我们为输入和输出空间固定了有序基，我们就可以将任何线性映射表示为一个具体的**矩阵**。这个矩阵表示完全依赖于所选的基。

## 第三部分：基变换和变换矩阵

本节涵盖了将抽象线性映射表示为矩阵以及改变坐标系的机制。

### 1. 线性映射的变换矩阵

变换矩阵为抽象的线性映射提供了一个具体的计算表示，这是相对于所选基而言的。

*   **定义和背景：**
    我们给定一个线性映射 $\Phi: V \to W$，向量空间 $V$ 的一个有序基 $B = (\mathbf{b}_1, \ldots, \mathbf{b}_n)$，以及向量空间 $W$ 的一个有序基 $C = (\mathbf{c}_1, \ldots, \mathbf{c}_m)$。

*   **变换矩阵 ($A_\Phi$) 的构造：**
    变换矩阵 $A_\Phi$ 是一个 $m \times n$ 矩阵，其列描述了输入基向量如何被 $\Phi$ 变换。构造如下：
    1.  对每个输入基向量 $\mathbf{b}_j$（从 $j=1$ 到 $n$）：
    2.  应用线性映射得到其像：$\Phi(\mathbf{b}_j) \in W$。
    3.  将这个像表示为输出基向量 $C$ 的唯一线性组合：
        $$ \Phi(\mathbf{b}_j) = \alpha_{1j}\mathbf{c}_1 + \alpha_{2j}\mathbf{c}_2 + \cdots + \alpha_{mj}\mathbf{c}_m $$
    4.  这个组合的系数构成了矩阵 $A_\Phi$ 的第 $j$ 列。这一列是 $\Phi(\mathbf{b}_j)$ 相对于基 $C$ 的坐标向量：
        $$ A_\Phi \text{ 的第 } j \text{ 列} = [\Phi(\mathbf{b}_j)]_C = \begin{pmatrix} \alpha_{1j} \\ \alpha_{2j} \\ \vdots \\ \alpha_{mj} \end{pmatrix} $$

*   **用法和解释：**
    这个矩阵将任何 $\mathbf{v} \in V$ 的坐标向量（相对于基 $B$）映射到其像 $\Phi(\mathbf{v}) \in W$ 的坐标向量（相对于基 $C$）。核心运算公式是：
    $$ [\Phi(\mathbf{v})]_C = A_\Phi [\mathbf{v}]_B $$
    这个公式将抽象的函数应用转化为具体的矩阵-向量乘法。矩阵 $A_\Phi$ 是映射 $\Phi$ *相对于所选基*的表示；改变任何一个基都会导致同一个线性映射有不同的变换矩阵。

*   **可逆性：**
    线性映射 $\Phi$ 是一个可逆的同构，当且仅当其变换矩阵 $A_\Phi$ 是方阵 ($m=n$) 且可逆。[[Notion/Class/Proof/Non-Invertible Transformations and Information Loss\|Non-Invertible Transformations and Information Loss]]

### 2. 基变换矩阵

这是变换矩阵的一个特殊应用，用于在*同一个*向量空间内将一个向量的坐标从一个基转换到另一个基。这个过程等价于为**恒等映射**（$\text{id}: V \to V$，其中 $\text{id}(\mathbf{x}) = \mathbf{x}$）寻找变换矩阵。

*   **基变换矩阵 ($P_{B \leftarrow B'}$):**
    这个矩阵将坐标从一个新基 $B'$ 转换到一个旧基 $B$。它的列是新基向量在旧基下的坐标向量。
    $$ P_{B \leftarrow B'} = \Big[ \ [\mathbf{b}'_1]_B \ \ \ [\mathbf{b}'_2]_B \ \ \cdots \ \ [\mathbf{b}'_n]_B \ \Big] $$
    *注意：如果“旧”基 $B$ 是 $\mathbb{R}^n$ 中的标准基，则这个矩阵的列就是新基 $B'$ 的向量本身。*

*   **使用公式：**
    *   将坐标**从新基 ($B'$) 转换到旧基 ($B$)**：
        $$ [\mathbf{x}]_B = P_{B \leftarrow B'} \ [\mathbf{x}]_{B'} $$
    *   将坐标**从旧基 ($B$) 转换到新基 ($B'$)**：
        $$ [\mathbf{x}]_{B'} = (P_{B \leftarrow B'})^{-1} \ [\mathbf{x}]_B $$

### 3. 线性映射的基变换定理

[[Notion/Class/Proof/Change-of-Basis Theorem\|Change-of-Basis Theorem]]
该定理提供了一个公式，用于在改变线性映射的定义域和陪域的基（坐标系）时，计算新的变换矩阵。

*   **定理陈述：**
    给定一个线性映射 $\Phi : V \to W$，其中：
    *   一个“旧”输入基 $B$ 和一个“新”输入基 $B̃$，都用于**定义域** $V$。
    *   一个“旧”输出基 $C$ 和一个“新”输出基 $C̃$，都用于**陪域** $W$。
    *   原始的变换矩阵 $A_\Phi$（相对于旧基 $B$ 和 $C$）。

    新的变换矩阵 $Ã_\Phi$（相对于新基 $B̃$ 和 $C̃$）由下式给出：
    $$ Ã_\Phi = T^{-1}A_\Phi S $$
    其中基变换矩阵定义为：
    *   $S$：**定义域 $V$ 内**的基变换矩阵。它将坐标从**新输入基 $B̃$** 转换到**旧输入基 $B$**。
    *   $T$：**陪域 $W$ 内**的基变换矩阵。它将坐标从**新输出基 $C̃$** 转换到**旧输出基 $C$**。

*   **公式解释（变换的“路径”）：**
    该公式表示对一个坐标向量进行的一系列三个操作。坐标的路径是 $B̃ \to B \to C \to C̃$。
    1.  **第1步：`S` (从 $B̃ \to B$ 在定义域中)**: 我们从一个向量在新输入基下的坐标 $[\mathbf{v}]_{B̃}$ 开始。我们应用 $S$ 将这些坐标转换为旧输入基：$S[\mathbf{v}]_{B̃}=[\mathbf{v}]_B$。
    2.  **第2步：`AΦ` (从基 $B$ 到基 $C$)**: 我们将原始变换矩阵 $A_\Phi$ 应用到现已用旧输入基 $B$ 表示的坐标上。这得到了像在旧输出基 $C$ 下的坐标：$A_\Phi([\mathbf{v}]_B) = [\Phi(\mathbf{v})]_C$。
    3.  **第3步：`T⁻¹` (从 $C \to C̃$ 在陪域中)**: 结果是用旧输出基 $C$ 表示的。为了用新输出基 $C̃$ 表示它，我们必须应用 $T$ 的逆。因为 $T$ 从 $C̃$ 转换到 $C$，所以必须使用 $T^{-1}$ 从 $C$ 转换到 $C̃$：$T^{-1}[\Phi(\mathbf{v})]_C = [\Phi(\mathbf{v})]_{C̃}$。

### 4. 矩阵等价与相似

这些概念形式化了这样一种思想：不同的矩阵可以表示相同的底层线性映射，只是使用了不同的坐标系。

*   **矩阵等价：**
    *   **定义：** 如果存在可逆矩阵 $S$（在定义域中）和 $T$（在陪域中）使得 $Ã = T^{-1}AS$，则两个 $m \times n$ 矩阵 $A$ 和 $Ã$ 是**等价的**。
    *   **解释：** 等价矩阵表示**完全相同的线性变换** $\Phi: V \to W$。它们仅仅是由于在定义域 $V$ 和陪域 $W$ 中选择了不同的基（坐标系）而对 $\Phi$ 的不同数值表示。

*   **矩阵相似：**
    *   **定义：** 如果存在一个单一的可逆矩阵 $S$ 使得 $Ã = S^{-1}AS$，则两个**方阵** $n \times n$ 的 $A$ 和 $Ã$ 是**相似的**。
    *   **解释：** 相似性是等价性在**自同态**（$\Phi: V \to V$）上的一个特例，其中同一个空间既作为定义域又作为陪域，因此相同的基变换（即 $T=S$）被应用于输入和输出坐标。

### 5. 线性映射的复合

*   **定理：** 如果 $\Phi : V \to W$ 和 $\Psi : W \to X$ 是线性映射，它们的复合 $(\Psi \circ \Phi) : V \to X$ 也是一个线性映射。
*   **矩阵表示：** 复合映射的变换矩阵是各个变换矩阵的乘积，顺序与应用顺序相反：
    $$ A_{\Psi \circ \Phi} = A_\Psi A_\Phi $$

## 第四部分：仿射空间与仿射子空间

虽然向量空间和子空间是基础，但它们受一个关键要求的限制：必须包含原点。仿射空间将这一思想推广到描述那些不一定穿过原点的几何对象，如直线和平面。

*   **核心直观：向量空间 vs. 仿射空间**
    *   **向量子空间**是一条**必须穿过原点**的直线或平面（或更高维的等价物）。
    *   **仿射子空间**是一条被**平移**过的直线或平面（或更高维的等价物），因此不再需要穿过原点。它是向量空间中的一个“平面”曲面。

*   **仿射子空间的正式定义**

    向量空间 $V$ 的一个**仿射子空间** $L$ 是一个可以表示为一个特定向量（一个点）和一个向量子空间之和的子集。

    $$ L = \mathbf{p} + U = \{ \mathbf{p} + \mathbf{u} \mid \mathbf{u} \in U \} $$

    其中：
    *   $\mathbf{p} \in V$ 是一个特定的向量，通常称为**平移向量**或**支撑点**。它起到平移空间的作用。
    *   $U$ 是 $V$ 的一个**向量子空间**，通常称为**方向空间**或相关的向量子空间。它定义了仿射子空间的方向和“形状”（直线、平面等）。

    仿射子空间 $L$ 的**维度**被定义为其方向空间 $U$ 的维度。

*   **几何示例：**
    *   **$\mathbb{R}^3$中的一条直线：** 穿过点 $\mathbf{p}$ 且方向向量为 $\mathbf{d}$ 的直线是一个仿射子空间。
        $$ L = \mathbf{p} + t\mathbf{d} \quad (t \in \mathbb{R}) $$
        这里，支撑点是 $\mathbf{p}$，方向空间是1维向量子空间 $U = \text{Span}\{\mathbf{d}\}$。
    *   **$\mathbb{R}^3$中的一个平面：** 包含点 $\mathbf{p}$ 且与向量 $\mathbf{u}$ 和 $\mathbf{v}$（线性无关）平行的平面是一个仿射子空间。
        $$ L = \mathbf{p} + s\mathbf{u} + t\mathbf{v} \quad (s, t \in \mathbb{R}) $$
        这里，支撑点是 $\mathbf{p}$，方向空间是2维向量子空间 $U = \text{Span}\{\mathbf{u}, \mathbf{v}\}$。

*   **与线性系统解的联系（关键应用）**

    仿射子空间为线性系统的解集提供了完美的几何描述。

    *   **齐次系统 `Ax = 0`：** 齐次系统的所有解的集合是 $A$ 的**零空间**，记为 $N(A)$。零空间永远是一个**向量子空间**。

    *   **非齐次系统 `Ax = b`：** 非齐次系统（其中 $\mathbf{b} \ne \mathbf{0}$）的所有解的集合是一个**仿射子空间**。
        回想通解公式：
        $$ \mathbf{x} = \mathbf{x}_p + \mathbf{x}_h $$
        让我们将其映射到仿射子空间 $L = \mathbf{p} + U$ 的定义中：
        *   **特解** $\mathbf{x}_p$ 作为**平移向量** $\mathbf{p}$。
        *   所有**齐次解** $\mathbf{x}_h$ 的集合是**方向空间** $U$。这正是零空间 $N(A)$。

        因此，`Ax = b` 的完整解集是仿射子空间：
        $$ L = \mathbf{x}_p + N(A) $$
        这意味着解集是零空间 $N(A)$ 被一个特解向量 $\mathbf{x}_p$ 平移后的结果。

*   **关键差异总结**

| 特征 | **向量子空间** (`U`) | **仿射子空间** (`L = p + U`) |
| :--- | :--- | :--- |
| **必须包含原点?** | **是** (`0 ∈ U`) | **否**，除非 `p ∈ U`。 |
| **加法封闭?** | **是**。如果 `u₁, u₂ ∈ U`，则 `u₁ + u₂ ∈ U`。 | **否**。通常，`l₁ + l₂ ∉ L`。 |
| **标量乘法封闭?**| **是**。如果 `u ∈ U`，则 `cu ∈ U`。 | **否**。通常，`cl₁ ∉ L`。 |
| **几何示例** | 穿过原点的直线/平面。 | 任何被平移的直线/平面。 |
| **线性系统示例**| `Ax = 0` 的解集。 | `Ax = b` 的解集。 |

*   **仿射组合**
    *   一个相关的概念是**仿射组合**。它是一种系数之和为1的线性组合。
        $$ \mathbf{y} = \alpha_1\mathbf{x}_1 + \alpha_2\mathbf{x}_2 + \cdots + \alpha_k\mathbf{x}_k \quad \text{其中} \quad \sum_{i=1}^k \alpha_i = 1 $$
    *   仿射子空间在仿射组合下是封闭的。一组点的所有仿射组合的集合构成了包含它们的最小仿射子空间（它们的“仿射包”）。
[[如何用仿射子空间 (Affine Subspace) 的结构来理解线性方程组 `Aλ = b` 的通解]]

## 第五部分：超平面 (Hyperplanes)

超平面是将在2D空间中的线和3D空间中的平面的概念推广到任意维度向量空间的结果。它是一种极其重要且常见的特殊仿射子空间。

### 1. 核心直观

-   在**2D**空间 ($\mathbb{R}^2$)中，超平面是一条**线**（1维）。
-   在**3D**空间 ($\mathbb{R}^3$)中，超平面是一个**平面**（2维）。
-   在**n维**空间 ($\mathbb{R}^n$)中，超平面是一个**(n-1)维**的“平坦”子空间。

它的关键功能是将整个空间“切片”成两个半空间，使其成为分类问题中理想的**决策边界**。

### 2. 超平面的两个等价定义

超平面可以用两种等价的方式定义：一种是代数的，一种是几何的。

##### **定义1：代数定义（通过单个线性方程）**

$\mathbb{R}^n$中的一个**超平面** $H$ 是满足单个线性方程的所有点 $\mathbf{x}$ 的集合：
$$ a_1x_1 + a_2x_2 + \cdots + a_nx_n = d $$
其中 $a_1, \dots, a_n$ 是不全为零的系数，`d` 是一个常数。

使用向量表示法，这个方程变得更加紧凑：
$$ \mathbf{a}^T \mathbf{x} = d $$
-   **法向量 $\mathbf{a}$**: 向量 $\mathbf{a} = (a_1, \dots, a_n)^T$ 被称为超平面的**法向量**。几何上，它与超平面本身**垂直**。
-   **偏移量 `d`**: 常数 `d` 决定了超平面距离原点的偏移量。
    -   如果 `d = 0`，超平面 `aᵀx = 0` 穿过原点，并且本身是一个**(n-1)维的向量子空间**。
    -   如果 `d ≠ 0`，超平面不穿过原点，是一个真正的**仿射子空间**。

##### **定义2：几何定义（通过仿射子空间）**

n维向量空间 $V$ 中的一个**超平面** $H$ 是一个维度为**n-1**的**仿射子空间**。
$$ H = \mathbf{p} + U $$
其中：
-   $\mathbf{p}$ 是超平面上的任意一个特定点（支撑点）。
-   $U$ 是一个维度为**n-1**的向量子空间（方向空间）。

### 3. 定义之间的联系

这两个定义是完全等价的。

-   **从代数到几何 (`aᵀx = d` → `p + U`)**:
    1.  **方向空间 `U`**: 方向空间 `U` 是与原超平面平行且穿过原点的超平面。它是满足 `aᵀu = 0` 的所有向量 `u` 的集合。这个集合是法向量 `a` 的**正交补**，维度为 n-1。
    2.  **支撑点 `p`**: 我们可以通过找到方程 `aᵀx = d` 的任意一个**特解**来找到一个支撑点 `p`。

### 4. 机器学习中的超平面

超平面是许多机器学习算法的核心，最著名的是**支持向量机 (SVM)**。

-   **作为决策边界**: 在二元分类问题中，目标是找到一个能够最好地分隔属于两个不同类别的数据点的超平面。
-   **SVM 超平面**: SVM 寻求找到一个由以下方程定义的最优超平面：
    $$ \mathbf{w}^T\mathbf{x} - b = 0 $$
    -   $\mathbf{w}$ 是权重向量，等价于**法向量** `a`。
    -   `b` 是偏置项，与**偏移量** `d` 相关。
-   **分类规则**:
    -   如果一个新的数据点 $\mathbf{x}_{\text{new}}$ 满足 $\mathbf{w}^T\mathbf{x}_{\text{new}} - b > 0$，它被分到一类（例如，正类）。
    -   如果它满足 $\mathbf{w}^T\mathbf{x}_{\text{new}} - b < 0$，它被分到另一类（例如，负类）。
    -   这意味着一个点的分类取决于它位于超平面的哪一侧。SVM的目标是找到使这个分隔“间隔”尽可能宽的 `w` 和 `b`。

## 第六部分：仿射映射 (Affine Mappings)

我们已经确定，形式为 `φ(x) = Ax` 的线性映射总是保持原点不变（即 `φ(0) = 0`）。然而，许多实际应用，特别是计算机图形学，需要包括**平移**的变换，这会移动原点。这种更一般的变换类别被称为仿射映射。

### 1. 核心思想：一个线性映射后跟一个平移

一个**仿射映射**本质上是一个**线性映射**和一个**平移**的复合。

-   **线性部分：** 处理旋转、缩放、剪切和其他保持原点固定的变换。
-   **平移部分：** 将整个结果移动到空间中的一个新位置。

### 2. 正式定义

从向量空间 `V` 到向量空间 `W` 的映射 `f: V → W` 被称为**仿射映射**，如果它可以写成以下形式：
$$ f(\mathbf{x}) = A\mathbf{x} + \mathbf{b} $$
其中：
-   `A` 是一个 $m \times n$ 矩阵，表示变换的**线性部分**。
-   `b` 是一个 $m \times 1$ 向量，表示**平移部分**。

**与线性映射的区别：**
-   如果平移向量 `b = 0`，仿射映射退化为纯粹的线性映射。
-   如果 `b ≠ 0`，则 `f(0) = A(0) + b = b`，这意味着原点不再映射到原点，而是被移动到由 `b` 定义的位置。

### 3. 仿射映射的关键性质

虽然仿射映射通常不是线性的（因为 `f(x+y) ≠ f(x) + f(y)`），但它们保留了几个关键的几何性质。

1.  **直线映射为直线：** 仿射映射将一条直线变换为另一条直线（或者在退化情况下，如果直线的方向在 `A` 的零空间中，则变换为单个点）。

2.  **平行性被保留：** 如果两条线是平行的，它们在仿射映射下的像也将是平行的。

3.  **长度比率被保留：** 如果点 `P` 是线段 `QR` 的中点，那么它的像 `f(P)` 将是像线段 `f(Q)f(R)` 的中点。

4.  **仿射组合被保留：** 这是仿射映射最基本的代数性质。如果一个点 `y` 是一组点 `xᵢ` 的仿射组合（即 `y = Σαᵢxᵢ` 且 `Σαᵢ = 1`），那么它的像 `f(y)` 是像点 `f(xᵢ)` 的**相同仿射组合**：
    $$ f\left(\sum \alpha_i \mathbf{x}_i\right) = \sum \alpha_i f(\mathbf{x}_i), \quad \text{前提是} \quad \sum \alpha_i = 1 $$

### 4. 齐次坐标：统一变换的技巧

在计算机图形学等领域，用**单一的矩阵乘法**来表示所有变换（包括平移）是非常理想的。标准形式 `Ax + b` 需要乘法和加法，这对于复合多个变换很不方便。

**齐次坐标**通过增加一个额外的维度，巧妙地解决了这个问题，有效地将一个仿射映射变成了更高维空间中的一个线性映射。

*   **工作原理：**
    1.  一个n维向量 `x = (x₁, ..., xₙ)ᵀ` 被表示为一个(n+1)维的齐次向量：
        $$ \mathbf{x}_{\text{hom}} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \\ 1 \end{bmatrix} $$
    2.  一个仿射映射 `f(x) = Ax + b` 由一个 `(n+1) × (n+1)` 的**增广变换矩阵**表示：
        $$ T_f = \begin{bmatrix}
           & A & & \mathbf{b} \\
           \hline
           0 & \cdots & 0 & 1
           \end{bmatrix}
        $$
        这里，`A` 是 $n \times n$ 的线性部分，`b` 是 $n \times 1$ 的平移向量。底行由零和一个一组成。

*   **统一的操作：**
    仿射变换现在可以用一次矩阵乘法完成：
    $$
    T_f \mathbf{x}_{\text{hom}} =
    \begin{bmatrix}
    A & \mathbf{b} \\
    \mathbf{0}^T & 1
    \end{bmatrix}
    \begin{bmatrix}
    \mathbf{x} \\
    1
    \end{bmatrix}
    =
    \begin{bmatrix}
    A\mathbf{x} + \mathbf{b} \\
    1
    \end{bmatrix}
    $$
    结果向量的前 `n` 个分量正是所需的 `Ax + b`，最后一个分量保持为 `1`。

*   **优势：**
    这种技术允许通过首先乘以它们各自的增广矩阵来复合一系列变换（例如，旋转、缩放、再平移）。得到的单一矩阵可以应用于所有点，极大地简化了复杂变换的计算和管理。

### 5. 总结

| 概念 | **线性映射 (`Ax`)** | **仿射映射 (`Ax + b`)** |
| :--- | :--- | :--- |
| **本质** | 旋转 / 缩放 / 剪切 | 线性变换 + 平移 |
| **保留原点?** | **是**, `f(0) = 0` | **否**, 通常 `f(0) = b` |
| **保留线性组合?** | **是** | **否** |
| **保留什么?** | 直线、平行性、**线性组合** | 直线、平行性、**仿射组合** |
| **表示法** | 矩阵 `A` | 矩阵 `A` 和向量 `b` |
| **齐次坐标形式** | $\begin{bsmallmatrix} A & \mathbf{0} \\ \mathbf{0}^T & 1 \end{bsmallmatrix}$ | $\begin{bsmallmatrix} A & \mathbf{b} \\ \mathbf{0}^T & 1 \end{bsmallmatrix}$ |

***

# 第三讲：解析几何：范数、内积、长度与距离、角度与正交性

### 第一部分：向量空间上的几何结构

在前面部分，我们建立了向量空间和线性映射的代数框架。现在，我们将为这些空间赋予**几何结构**，使我们能够形式化向量的**长度**、向量间的**距离**以及它们之间的**角度**等直观概念。这些概念由范数和内积来捕捉。

#### 1. 范数 (Norms)

范数是向量“长度”或“大小”这一直观概念的形式化推广。

*   **几何直观：** 向量的范数是其长度，即从原点到该向量所代表的点的距离。

*   **范数的正式定义：**
    向量空间 $V$ 上的一个**范数**是一个函数 $\|\cdot\| : V \to \mathbb{R}$，它为每个向量 $\mathbf{x} \in V$ 赋予一个非负实值 $\|\mathbf{x}\|$。该函数必须满足以下三个公理：

    1.  **正定性：** 长度是正的，除非是零向量。
        *   $\|\mathbf{x}\| \ge 0$
        *   $\|\mathbf{x}\| = 0 \iff \mathbf{x} = \mathbf{0}$

    2.  **绝对齐次性：** 缩放一个向量会以相同的因子缩放其长度。
        *   $\|\lambda\mathbf{x}\| = |\lambda|\|\mathbf{x}\|$

    3.  **三角不等式：** 三角形的一边长度不大于另外两边长度之和。
        *   $\|\mathbf{x} + \mathbf{y}\| \le \|\mathbf{x}\| + \|\mathbf{y}\|$

    配备了范数的向量空间称为**赋范向量空间**。

*   **$\mathbb{R}^n$上的范数示例：**
    对于向量 $\mathbf{x} = (x_1, \dots, x_n)^T$：

    *   **$L_1$-范数 (曼哈顿范数):** 测量“城市街区”距离。
        $$ \|\mathbf{x}\|_1 := \sum_{i=1}^n |x_i| $$

    *   **$L_2$-范数 (欧几里得范数):** 标准的“直线”距离。
        $$ \|\mathbf{x}\|_2 := \sqrt{\sum_{i=1}^n x_i^2} = \sqrt{\mathbf{x}^T\mathbf{x}} $$
        **这是默认范数。**当我们写 $\|\mathbf{x}\|$ 而不带下标时，几乎总是指欧几里得范数。

    *   **$L_\infty$-范数 (最大范数):** 长度由向量的最大分量决定。
        $$ \|\mathbf{x}\|_\infty := \max_{i=1}^n |x_i| $$

*   **由范数导出的距离：**
    任何范数都自然地定义了两个向量之间的**距离** $d(\mathbf{x}, \mathbf{y})$，即它们差向量的范数：
    $$ d(\mathbf{x}, \mathbf{y}) := \|\mathbf{x} - \mathbf{y}\| $$

#### 2. 内积 (Inner Products)

内积是一个比范数更基本的概念。它是一个函数，不仅允许我们定义欧几里得范数，还允许我们定义向量间的角度和正交性（垂直性）的概念。

*   **动机：** 内积是$\mathbb{R}^n$中熟悉的**点积**的推广，点积定义为：
    $$ \langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x}^T\mathbf{y} = \sum_{i=1}^n x_i y_i $$

*   **内积的正式定义：**
    实向量空间 $V$ 上的一个**内积**是一个函数 $\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}$，它接收两个向量并返回一个标量。该函数必须是一个**对称、正定的双线性映射**，满足以下公理：

    1.  **双线性：** 函数在每个参数中都是线性的。
    2.  **对称性：** 参数的顺序不影响结果。
        $$ \langle \mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{y}, \mathbf{x} \rangle $$
    3.  **正定性：** 向量与自身的内积是非负的，并且仅当向量为零向量时才为零。
        *   $\langle \mathbf{x}, \mathbf{x} \rangle \ge 0$
        *   $\langle \mathbf{x}, \mathbf{x} \rangle = 0 \iff \mathbf{x} = \mathbf{0}$

    配备了内积的向量空间称为**内积空间**。

#### 3. 桥梁：从内积到几何

内积是向量空间内欧几里得几何的基础。所有关键的几何概念都可以从中导出。

*   **诱导范数：**
    **每个内积都自然地定义（或诱导）一个范数**，由下式给出：
    $$ \|\mathbf{x}\| := \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle} $$
    标准的欧几里得范数 $\|\mathbf{x}\|_2$ 正是由标准点积诱导的范数。

*   **柯西-施瓦茨不等式：**
    它将两个向量的内积与它们的诱导范数联系起来，并为定义角度提供了基础。
    $$ |\langle \mathbf{x}, \mathbf{y} \rangle| \le \|\mathbf{x}\| \|\mathbf{y}\| $$

*   **由内积定义的几何概念：**
    *   **长度：** $\|\mathbf{x}\| = \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle}$
    *   **距离：** $d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\| = \sqrt{\langle \mathbf{x}-\mathbf{y}, \mathbf{x}-\mathbf{y} \rangle}$
    *   **角度：** 两个非零向量 $\mathbf{x}$ 和 $\mathbf{y}$ 之间的角度 $\theta$ 通过以下方式定义：
        $$ \cos\theta = \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{x}\| \|\mathbf{y}\|} $$
    *   **正交性 (垂直性):**
        *   **定义：** 如果两个向量 $\mathbf{x}$ 和 $\mathbf{y}$ 的内积为零，则它们是**正交的**。我们记为 $\mathbf{x} \perp \mathbf{y}$。
            $$ \mathbf{x} \perp \mathbf{y} \iff \langle \mathbf{x}, \mathbf{y} \rangle = 0 $$
        *   **勾股定理：** 如果 $\mathbf{x} \perp \mathbf{y}$，则熟悉的勾股定理成立：
            $$ \|\mathbf{x} + \mathbf{y}\|^2 = \|\mathbf{x}\|^2 + \|\mathbf{y}\|^2 $$

### 第二部分：向量空间上的几何结构

#### 1. 对称正定 (SPD) 矩阵与内积

在像$\mathbb{R}^n$这样的有限维向量空间中，内积的抽象概念可以通过一类特殊的矩阵——**对称正定 (SPD) 矩阵**——来具体表示和计算。

*   **对称正定矩阵的定义：**
    一个方阵 $A \in \mathbb{R}^{n \times n}$ 被称为**对称正定** (SPD)，如果它满足两个条件：

    1.  **对称性：** $A = A^T$
    2.  **正定性：** 对于每个非零向量 $x \in \mathbb{R}^n$，二次型 $x^T A x$ 严格为正。[[Notion/Class/Proof/Quadratic Form\|Quadratic Form]]
        $$ \mathbf{x}^T A \mathbf{x} > 0 \quad \text{对于所有 } \mathbf{x} \in \mathbb{R}^n, \mathbf{x} \ne \mathbf{0} $$

*   **中心定理：内积的矩阵表示**
    **定理：**
    $$ \langle \mathbf{x}, \mathbf{y} \rangle := \hat{\mathbf{x}}^T A \hat{\mathbf{y}} $$
    这个函数是一个有效的**内积**，当且仅当矩阵 $A$ 是**对称且正定的**。

    **解释：**
    *   该定理为有限维空间上所有可能的内积提供了通用配方。
    *   $\mathbb{R}^n$中的标准**点积**是该定理最简单的情况，其中矩阵 $A$ 是单位矩阵 $I$：
        $$ \langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x}^T I \mathbf{y} = \mathbf{x}^T \mathbf{y} $$
    *   *任何* SPD 矩阵 $A$ 都可以用来定义一个新的、完全有效的内积 $\langle \cdot, \cdot \rangle_A$。

*   **SPD 矩阵的性质**
    1.  **可逆性 (零空间平凡):** SPD 矩阵总是可逆的。它的零空间只包含零向量。
    2.  **正对角元素：** SPD 矩阵的所有对角元素都严格为正。

*   **回顾：内积 vs. 点积**
    *   **内积 $\langle \mathbf{x}, \mathbf{y} \rangle$：** **一般概念**。
    *   **点积 $\mathbf{x}^T \mathbf{y}$：** **具体示例**，是由单位矩阵定义的内积。
    *   **欧几里得范数 $\|\mathbf{x}\|_2$：** 由**点积**诱导的范数。

### 第三部分：角度、正交性与正交矩阵

#### 1. 角度与正交性

*   **定义角度：**
    $$ \cos\omega = \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{x}\| \|\mathbf{y}\|} $$
*   **正交性：**
    *   **定义：** 两个向量 $\mathbf{x}$ 和 $\mathbf{y}$ 是**正交的**，如果它们的内积为零。记为 $\mathbf{x} \perp \mathbf{y}$。
        $$ \mathbf{x} \perp \mathbf{y} \iff \langle \mathbf{x}, \mathbf{y} \rangle = 0 $$
    *   **几何意义：** 角度 $\omega = \pi/2$ (90°)。
*   **标准正交性 (Orthonormality):**
    *   两个向量 $\mathbf{x}$ 和 $\mathbf{y}$ 是**标准正交的**，如果它们既**正交**（$\langle \mathbf{x}, \mathbf{y} \rangle = 0$）又是**单位向量**（$\|\mathbf{x}\| = 1$, $\|\mathbf{y}\| = 1$）。

*   **关键点：正交性取决于内积**
    *   两个向量是否正交完全取决于所选的内积。

#### 2. 正交矩阵

*   **定义：**
    一个方阵 $A \in \mathbb{R}^{n \times n}$ 被称为**正交矩阵**，当且仅当其列构成一个**标准正交集**。

*   **等价性质：**
    1.  $A$ 的列是标准正交的。
    2.  $A^T A = I$
    3.  $A^{-1} = A^T$
    *   **核心思想：** 正交矩阵的逆矩阵就是其转置。

*   **几何性质：保持长度和角度**
    由正交矩阵 $A$ 定义的线性变换 $T(\mathbf{x}) = A\mathbf{x}$ 是一种**刚体变换**。
    1.  **保持长度：** $\|A\mathbf{x}\| = \|\mathbf{x}\|$
    2.  **保持内积和角度：** $\langle A\mathbf{x}, A\mathbf{y} \rangle = \langle \mathbf{x}, \mathbf{y} \rangle$

*   **实际意义与应用：**
    *   **几何模型：** 正交矩阵完美地模拟了空间中的**旋转**和**反射**。
    *   **数值稳定性：** 涉及正交矩阵的算法（如 QR 分解）通常非常数值稳定。
    *   **坐标系变换：** 从一个标准正交基到另一个的变换由一个正交矩阵描述。

## 第四部分：度量空间与距离的正式定义

度量 (metric) 形式化了任何集合元素之间“距离”的直观概念。

### 1. 度量函数 (Metric Function)

*   **正式定义：**
    集合 `V` 上的一个**度量**是一个函数 `d : V × V → ℝ`，它将一对元素 `(x, y)` 映射到一个实数 `d(x, y)`，并满足以下三个公理：
    1.  **正定性:**
        *   `d(x, y) ≥ 0`
        *   `d(x, y) = 0` 当且仅当 `x = y`
    2.  **对称性:**
        *   `d(x, y) = d(y, x)`
    3.  **三角不等式:**
        *   `d(x, z) ≤ d(x, y) + d(y, z)`

*   **度量空间 (Metric Space):**
    配备了度量 `d` 的集合 `V` 称为**度量空间**，记为 `(V, d)`。

### 2. 联系：从内积到度量

*   **定理：** 由内积诱导的距离函数 `d(x, y) = ||x - y||` 是一个度量。

### 3. 为什么度量的概念有用？

它允许我们在远超标准欧几里得几何的背景下测量“距离”。
*   **推广：** 适用于任何集合，包括：
    *   **字符串：** **编辑距离**是一个度量。
    *   **图：** 图中两个节点之间的最短路径距离是一个度量。
    *   **函数：** 我们可以定义度量来测量两个函数相距多“远”。

### 4. 空间层次总结

`内积空间` → `赋范空间` → `度量空间` → `拓扑空间`

## 第五部分：正交投影

正交投影是一种基本操作，用于在给定子空间中找到与给定向量“最接近”的向量。

### 1. 正交投影的概念

令 $U$ 是内积空间 $V$ 的一个子空间，$\mathbf{x} \in V$ 是一个向量。$\mathbf{x}$ 到子空间 $U$ 上的**正交投影** $\pi_U(\mathbf{x})$ 是 $U$ 中与 $\mathbf{x}$ “最接近”的唯一向量。

这个投影 $\mathbf{p} = \pi_U(\mathbf{x})$ 由两个基本性质定义：
1.  **隶属属性：** $\mathbf{p} \in U$
2.  **正交属性：** $(\mathbf{x} - \mathbf{p}) \perp U$

### 2. 推导投影公式（法方程）

**第1步：用基表示隶属属性**
令 $B$ 是一个以 $U$ 的基向量为列的矩阵。那么存在唯一的系数向量 $\boldsymbol{\lambda}$ 使得：
$$ \mathbf{p} = B\boldsymbol{\lambda} $$

**第2步：将正交属性表示为方程**
$(\mathbf{x} - \mathbf{p})$ 与 $U$ 的每个基向量的点积都必须为零，这可以紧凑地写成：
$$ B^T(\mathbf{x} - \mathbf{p}) = \mathbf{0} $$

**第3步：组合并求解 λ**
将第一个方程代入第二个方程：
$$ B^T(\mathbf{x} - B\boldsymbol{\lambda}) = \mathbf{0} $$
得到**法方程 (Normal Equation)**：
$$ (B^T B)\boldsymbol{\lambda} = B^T\mathbf{x} $$

### 3. 正交投影算法

1.  **找到基：** 找到子空间 $U$ 的一个基。
2.  **构成基矩阵 `B`**。
3.  **建立法方程：** 计算 `BᵀB` 和 `Bᵀx`。
4.  **求解 `λ`**。
5.  **计算投影 `p`：** 使用公式 `p = Bλ`。

### 4. 特殊情况：标准正交基

如果 $U$ 的基是**标准正交的**，那么 $B$ 的列是标准正交的。此时：
*   $B^T B = I$
*   法方程变为：$\boldsymbol{\lambda} = B^T\mathbf{x}$
*   投影公式为：$\mathbf{p} = B(B^T\mathbf{x}) = (BB^T)\mathbf{x}$
*   矩阵 $P = BB^T$ 称为**投影矩阵**。

***

# 第四讲：解析几何：标准正交基、正交补、函数内积、正交投影、旋转

## 第一部分：标准正交基与正交补

### 1. 标准正交基

*   **定义：** **标准正交基**是一种特殊的基，其中所有基向量相互正交，并且每个基向量都是单位向量。
    *   **形式化：** 对于基 $\{\mathbf{b}_1, \dots, \mathbf{b}_n\}$：
        *   **正交性：** $\langle \mathbf{b}_i, \mathbf{b}_j \rangle = 0$ 对于所有 $i \neq j$。
        *   **归一化：** $\|\mathbf{b}_i\|^2 = 1$ 对于所有 $i$。
*   **正交基：** 如果只满足正交性条件，则称为**正交基**。

### 2. 格拉姆-施密特过程：构造标准正交基

格拉姆-施密特过程是一个将任何线性无关向量集（一个基）转换为相同子空间的标准正交基的算法。

*   **目标：** 给定一个基 $\{\mathbf{a}_1, \dots, \mathbf{a}_n\}$，生成一个标准正交基 $\{\mathbf{q}_1, \dots, \mathbf{q}_n\}$。
*   **算法步骤：**
    1.  **初始化：** 归一化第一个向量 $\mathbf{a}_1$ 得到 $\mathbf{q}_1$。
        $$ \mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|} $$
    2.  **迭代与正交化：** 对于后续的每个向量 $\mathbf{a}_k$：
        a.  **投影并相减：** 从 $\mathbf{a}_k$ 中减去其在已找到的标准正交向量 $\{\mathbf{q}_1, \dots, \mathbf{q}_{k-1}\}$ 所张成的子空间上的投影，得到一个正交向量 $\mathbf{v}_k$。
            $$ \mathbf{v}_k = \mathbf{a}_k - \sum_{j=1}^{k-1} \langle \mathbf{a}_k, \mathbf{q}_j \rangle \mathbf{q}_j $$
        b.  **归一化：** 归一化 $\mathbf{v}_k$ 得到 $\mathbf{q}_k$。
            $$ \mathbf{q}_k = \frac{\mathbf{v}_k}{\|\mathbf{v}_k\|} $$



### 3. 正的概念从单个向量推广到整个子空间。

*   **定义 (正交补):** 令 $U$ 是向量空间 $V$ 的一个子空间。$U$ 的**正交补**，记为 $U^\perp$，是 $V$ 中所有与 $U$ 中*每个*向量都正交的向量的集合。
    $$ U^\perp = \{ \mathbf{v} \in V \mid \langle \mathbf{v}, \mathbf{u} \rangle = 0 \text{ 对于所有 } \mathbf{u} \in U \} $$

*   **空间分解 (直和):** 整个空间 $V$ 可以被唯一地分解为子空间 $U$ 与其正交补 $U^\perp$ 的**直和**。这意味着 $U$ 和 $U^\perp$ 的交集只有零向量，并且它们的维度之和等于 $V$ 的维度。
    $$ V = U \oplus U^\perp \quad \text{且} \quad \dim(U) + \dim(U^\perp) = \dim(V) $$

*   **向量分解 (正交分解):** 基于空间的直和分解，$V$ 中的**任何一个向量 $\mathbf{x}$** 都可以被**唯一地**分解为一个在 $U$ 中的分量和一个在 $U^\perp$ 中的分量之和。
    *   **概念层面:**
        $$ \mathbf{x} = \mathbf{x}_U + \mathbf{x}_{U^\perp} \quad (\text{其中 } \mathbf{x}_U \in U, \mathbf{x}_{U^\perp} \in U^\perp) $$
    *   **基层面表示:** 在计算上，这个分解通过找到 $\mathbf{x}$ 关于 $U$ 的基和 $U^\perp$ 的基的唯一坐标来实现。
        $$ \mathbf{x} = \sum_{m=1}^{M} \lambda_m \mathbf{b}_m + \sum_{j=1}^{D-M} \psi_j \mathbf{b}_j^\perp $$
        其中，$\{\mathbf{b}_1, \ldots, \mathbf{b}_M\}$ 是 $U$ 的一组基，$\{\mathbf{b}_1^\perp, \ldots, \mathbf{b}_{D-M}^\perp\}$ 是 $U^\perp$ 的一组基，而 $\lambda_m, \psi_j$ 是唯一的标量坐标。
    *   分量 $\mathbf{x}_U = \sum \lambda_m \mathbf{b}_m$ 正是 $\mathbf{x}$ 在子空间 $U$ 上的**正交投影**。

## 第二部分：函数内积、正交投影与旋转

### 1. 函数的内积

*   **定义：** 对于在区间 $[a, b]$ 上的连续函数空间，两个函数 $f(x)$ 和 $g(x)$ 之间的标准内积定义为积分：
    $$ \langle f, g \rangle := \int_{a}^{b} f(x)g(x) \,dx $$
*   **诱导的几何性质：**
    *   **长度 (范数):** $\|f\| = \sqrt{\int_{a}^{b} f(x)^2 \,dx}$
    *   **正交性：** 如果 $\langle f, g \rangle = 0$，则两个函数正交。
*   **意义：** 这种推广是**傅里叶级数**的数学基础。[[Notion/Class/Concept/Fourier Series\|Fourier Series]]

### 2. 正交投影

正交投影是解决最小二乘问题的几何基础。

*   **概念：** 向量 $\mathbf{x}$ 在子空间 $U$ 上的正交投影是 $U$ 中唯一的向量 $\mathbf{p}$，使得误差向量 $(\mathbf{x} - \mathbf{p})$ 与整个子空间 $U$ 正交。
*   **投影公式 (回顾):**
    1.  **构成基矩阵 $B$**。
    2.  **求解法方程** $(B^T B)\boldsymbol{\lambda} = B^T\mathbf{x}$ 得到 $\boldsymbol{\lambda}$。
    3.  **计算投影** $\mathbf{p} = B\boldsymbol{\lambda}$。
*   **[[Notion/Class/Proof/Projection Matrix\|Projection Matrix]]:** $P = B(B^T B)^{-1}B^T$。

### 3. 旋转

*   **定义：** 旋转是一种保持物体形状和大小的线性变换。
*   **矩阵性质：** 一个矩阵 $R$ 表示纯旋转，如果它满足两个条件：
    1.  **正交性：** $R^T R = I$ (保持长度和角度)。
    2.  **保持方向：** $\det(R) = 1$ (行列式为-1的表示反射)。
*   **2D 旋转：** 在2D平面中逆时针旋转角度 $\theta$ 的矩阵是：
    $$ R_\theta = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} $$
*   **群结构：** 所有 $n \times n$ 旋转矩阵的集合构成一个数学群，称为**特殊正交群**，记为 $SO(n)$。

## 第三部分：正交投影详解

投影是一类至关重要的线性变换，广泛应用于图形学、编码理论、统计学和机器学习中。

### 1. 正交投影的重要性与概念

*   **在机器学习中的动机：** 在机器学习中，我们经常处理难以分析或可视化的多维数据。一个关键的洞见是，大部分相关信息通常包含在一个维度低得多的子空间内。
*   **目标（维度约减）：** 通过将多维数据投影到一个精心选择的低维“特征空间”，我们可以简化问题、降低计算成本并提取有意义的模式。其目标是在执行此投影的同时，**最小化信息损失**。
*   **什么是正交投影？**
    *   它是一种线性变换，将一个向量从高维空间“投射”到低维子空间上。
    *   它之所以是“正交”的，是因为它通过**最小化**原始数据与其投影图像之间的**误差**（距离）来**最大可能地保留信息**。
    *   这一特性使其成为线性回归、分类和数据压缩的核心。

### 2. 投影的正式定义与性质

*   **代数定义（幂等性）：** 一个线性映射 $\pi: V \to U$ 如果应用两次与应用一次的效果相同，则被称为**投影**。这被称为**幂等性**。
    $$ \pi^2 = \pi \quad (\text{或 } \pi(\pi(\mathbf{x})) = \pi(\mathbf{x})) $$
    *   **矩阵形式：** 如果一个方阵 $P$ 满足 $P^2 = P$，那么它就是一个**投影矩阵**。
*   **几何定义（最近点）：** 向量 $\mathbf{x}$ 在子空间 $U$ 上的**正交投影** $\pi_U(\mathbf{x})$ 是 $U$ 中离 $\mathbf{x}$ **最近**的那个唯一的点。
    *   这个“最近点”条件等价于**正交性条件**：差向量 $(\mathbf{x} - \pi_U(\mathbf{x}))$ 必须与子空间 $U$ 中的每一个向量都正交。

### 3. 投影到一维子空间（直线）
![Image/Class/Mathematics-for-AI/5.png](/img/user/Image/Class/Mathematics-for-AI/5.png)
![Image/Class/Mathematics-for-AI/6.png](/img/user/Image/Class/Mathematics-for-AI/6.png)
我们从最简单的情况开始推导投影公式：将一个向量投影到一条直线上。除非另有说明，我们都假设使用标准的点积作为内积。

*   **设定：** 令 $U$ 为由非零基向量 $\mathbf{b}$ 张成的一维子空间（一条过原点的直线）。
*   **推导过程：** 投影 $\pi_U(\mathbf{x})$ 必须是 $\mathbf{b}$ 的一个标量倍，即 $\pi_U(\mathbf{x}) = \lambda\mathbf{b}$。通过正交条件 $\langle \mathbf{x} - \lambda\mathbf{b}, \mathbf{b} \rangle = 0$，我们可以解出坐标 $\lambda$。
*   **一维投影的最终公式：**
    *   **坐标：** $\lambda = \frac{\mathbf{b}^T\mathbf{x}}{\|\mathbf{b}\|^2}$
    *   **投影向量：** $\pi_U(\mathbf{x}) = \left( \frac{\mathbf{b}^T\mathbf{x}}{\|\mathbf{b}\|^2} \right) \mathbf{b}$
    *   **投影矩阵：** $P_\pi = \frac{\mathbf{b}\mathbf{b}^T}{\|\mathbf{b}\|^2}$

### 4. 投影到一般子空间
![Image/Class/Mathematics-for-AI/7.png](/img/user/Image/Class/Mathematics-for-AI/7.png)
用于一维投影的三步法可以推广到任何 m 维子空间 $U \subseteq \mathbb{R}^n$。

*   **设定：** 假设 $U$ 有一个基 $\{\mathbf{b}_1, \dots, \mathbf{b}_m\}$，构建基矩阵 $B = [\mathbf{b}_1, \dots, \mathbf{b}_m]$。
*   **推导过程：** 投影 $\pi_U(\mathbf{x}) = B\boldsymbol{\lambda}$。通过正交条件 $B^T(\mathbf{x} - B\boldsymbol{\lambda}) = \mathbf{0}$，我们得到**正规方程 (Normal Equation)**。
*   **最终公式：**
    *   **正规方程：** $B^T B \boldsymbol{\lambda} = B^T \mathbf{x}$
    *   **坐标：** $\boldsymbol{\lambda} = (B^T B)^{-1} B^T \mathbf{x}$
    *   **投影向量：** $\pi_U(\mathbf{x}) = B(B^T B)^{-1} B^T \mathbf{x}$
    *   **投影矩阵：** $P_\pi = B(B^T B)^{-1} B^T$

### 5. 核心应用 I：Gram-Schmidt正交化

Gram-Schmidt过程是构造一组标准正交基的经典算法，其核心思想就是**反复利用正交投影**。

*   **目标：** 将一组线性无关的向量 $\{\mathbf{b}_1, \dots, \mathbf{b}_n\}$ 转换为一组正交向量 $\{\mathbf{u}_1, \dots, \mathbf{u}_n\}$，并且两组向量张成相同的子空间。
*   **迭代构造法：**
    1.  **第一步：** 选择第一个向量作为新基的起点。
        $$ \mathbf{u}_1 = \mathbf{b}_1 $$
    2.  **后续步骤 (k=2 to n)：** 对于每一个新的向量 $\mathbf{b}_k$，减去它在**已经构造好的正交子空间** $\text{span}\{\mathbf{u}_1, \dots, \mathbf{u}_{k-1}\}$ 上的投影。剩下的分量就必然与该子空间正交。
        $$ \mathbf{u}_k = \mathbf{b}_k - \pi_{\text{span}\{\mathbf{u}_1, \dots, \mathbf{u}_{k-1}\}}(\mathbf{b}_k) $$
        由于 $\{ \mathbf{u}_i \}$ 已经正交，投影可以更简单地写成：
        $$ \mathbf{u}_k = \mathbf{b}_k - \sum_{i=1}^{k-1} \frac{\langle \mathbf{b}_k, \mathbf{u}_i \rangle}{\langle \mathbf{u}_i, \mathbf{u}_i \rangle} \mathbf{u}_i $$
*   **获得标准正交基 (ONB)：** 在每一步得到正交向量 $\mathbf{u}_k$ 后，对其进行**标准化**即可。
    $$ \mathbf{e}_k = \frac{\mathbf{u}_k}{\|\mathbf{u}_k\|} $$

*   **示例：** 对 $\mathbb{R}^2$ 中的基 $\mathbf{b}_1 = [2, 0]^T, \mathbf{b}_2 = [1, 1]^T$ 进行正交化。
    1.  $\mathbf{u}_1 = \mathbf{b}_1 = \begin{bmatrix} 2 \\ 0 \end{bmatrix}$
    2.  $\mathbf{u}_2 = \mathbf{b}_2 - \frac{\langle \mathbf{b}_2, \mathbf{u}_1 \rangle}{\langle \mathbf{u}_1, \mathbf{u}_1 \rangle} \mathbf{u}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix} - \frac{2}{4} \begin{bmatrix} 2 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$
    *   最终得到正交基 $\{ [2, 0]^T, [0, 1]^T \}$。

### 6. 核心应用 II：投影到仿射子空间

到目前为止，我们讨论的都是投影到过原点的子空间。现在我们将其推广到不过原点的**仿射子空间**（例如，不过原点的直线或平面）。

*   **定义：** 一个仿射子空间 $L$ 可以表示为 $L = \mathbf{x}_0 + U$，其中 $\mathbf{x}_0$ 是一个**支持点**（位移向量），$U$ 是一个与 $L$平行的、过原点的**方向子空间**。

*   **求解策略：平移-投影-移回**
    1.  **平移至原点：** 从待投影点 $\mathbf{x}$ 和仿射空间 $L$ 中都减去支持点 $\mathbf{x}_0$。这使得问题转化为将新向量 $(\mathbf{x} - \mathbf{x}_0)$ 投影到我们熟悉的方向子空间 $U$ 上。
    2.  **标准投影：** 计算向量 $(\mathbf{x} - \mathbf{x}_0)$ 在子空间 $U$ 上的正交投影 $\pi_U(\mathbf{x} - \mathbf{x}_0)$。
    3.  **移回原位：** 将投影结果平移回原来的位置，即加上支持点 $\mathbf{x}_0$。

*   **最终公式：**
    $$ \pi_L(\mathbf{x}) = \mathbf{x}_0 + \pi_U(\mathbf{x} - \mathbf{x}_0) $$

*   **数学证明：** 我们要找到点 $\mathbf{y}^* \in L$ 来最小化距离 $\|\mathbf{x} - \mathbf{y}\|^2$。
    *   因为 $\mathbf{y} \in L$，所以它可以写成 $\mathbf{y} = \mathbf{x}_0 + \mathbf{u}$，其中 $\mathbf{u} \in U$。
    *   最小化 $\|\mathbf{x} - (\mathbf{x}_0 + \mathbf{u})\|^2$ 就等价于最小化 $\|(\mathbf{x} - \mathbf{x}_0) - \mathbf{u}\|^2$。
    *   根据定义，使这个距离最小的 $\mathbf{u}^*$ 正是向量 $(\mathbf{x} - \mathbf{x}_0)$ 在子空间 $U$ 上的投影，即 $\mathbf{u}^* = \pi_U(\mathbf{x} - \mathbf{x}_0)$。
    *   因此，最近点 $\mathbf{y}^* = \mathbf{x}_0 + \mathbf{u}^* = \mathbf{x}_0 + \pi_U(\mathbf{x} - \mathbf{x}_0)$。

*   **点到仿射子空间的距离：**
    $$ d(\mathbf{x}, L) = \|\mathbf{x} - \pi_L(\mathbf{x})\| = \|\mathbf{x} - (\mathbf{x}_0 + \pi_U(\mathbf{x} - \mathbf{x}_0))\| = \|(\mathbf{x} - \mathbf{x}_0) - \pi_U(\mathbf{x} - \mathbf{x}_0)\| = d(\mathbf{x}-\mathbf{x}_0, U) $$
    这表明，点到仿射空间的距离，等于平移后的点到其方向子空间的距离。