---
{"dg-publish":true,"permalink":"/notion/class/artificial-intelligence/mathematics-for-ai-chinese/"}
---

# 第一讲：线性代数：线性方程组、矩阵、向量空间、线性无关

## 第一部分：概念

*   **向量 (Vector):**
    可以进行加法和标量乘法（与标量相乘）运算的对象。这些运算必须满足特定的公理（例如，加法交换律，标量乘法对向量加法的分配律）。
    *   **示例：** 几何向量（二维/三维空间中的箭头）、多项式 ($ax^2+bx+c$)、音频信号、$\mathbb{R}^n$ 中的元组（例如 $(x_1, x_2, \ldots, x_n)$）。

*   **封闭性 (Closure):**
    一个集合相对于特定运算（这里指向量加法和标量乘法）的基本性质。它意味着，如果你从集合中任意取出两个元素进行运算，其结果*永远*也是该集合中的一个元素。如果一个非空向量集合在向量加法和标量乘法下满足封闭性（以及其他公理），它就构成一个**向量空间**。
    *   **重要性：** 封闭性确保了代数结构（集合及其运算）是自洽和一致的。它是定义向量空间的基石。
*   **线性方程组的解 (Solution of the linear equation system):**
    一个能够同时满足给定线性方程组中*所有*方程的 *n*-元组 $(x_1,\cdots,x_n)\in\mathbb R^n$。每个分量 $x_i$ 代表对应变量的值。
    *   **与向量的联系：** 每个这样的 *n*-元组本身就是 $\mathbb{R}^n$ 中的一个**向量**。因此，求解线性方程组等价于寻找满足给定条件的特定向量。
*   **一个方程组可以有：**
	*   **无解 (inconsistent)**
	*   **唯一解 (unique)**
	*   **无穷多解 (underdetermined)**
*   **矩阵表示法 (Matrix Notation):** ![Image/Class/Mathematics-for-AI/1.png](/img/user/Image/Class/Mathematics-for-AI/1.png)
    一个线性方程组可以用矩阵乘法紧凑地表示。对于一个包含 $m$ 个方程和 $n$ 个未知数的系统：
    $$\begin{aligned}
    a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \\
    a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2 \\
    \vdots \\
    a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_m
    \end{aligned}
    $$
    *   **可以写作：** $Ax=b$
        其中：
        *   $A = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{bmatrix}$ 是**系数矩阵**。
        *   $x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}$ 是**变量向量**（或未知数向量）。
        *   $b = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{bmatrix}$ 是**常数向量**（或右侧向量）。

*   **增广矩阵 (Augmented Matrix):**
    在使用行变换（如高斯消元法）求解线性方程组 $Ax=b$ 时，将系数矩阵 $A$ 和常数向量 $b$ 组合成一个名为**增广矩阵**的单一矩阵会非常方便。
    *   **表示法：** 通常写作 `[A | b]`，其中竖线将系数矩阵与常数向量分开。
    *   **结构：**
        如果 $A$ 是一个 $m \times n$ 矩阵， $b$ 是一个 $m \times 1$ 的列向量，那么增广矩阵 `[A | b]` 是一个 $m \times (n+1)$ 的矩阵。
    $$[A | b] = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} & \bigm| & b_1 \\
        a_{21} & a_{22} & \cdots & a_{2n} & \bigm| & b_2 \\
        \vdots & \vdots & \ddots & \vdots & \bigm| & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn} & \bigm| & b_m
        \end{bmatrix}
        $$
    *   **目的：**
        这种表示法允许我们对整个系统（包括系数和常数）同时执行初等行变换，从而简化求解过程。增广矩阵的每一行直接对应线性系统中的一个方程。

	* **可以写作：** $Wx=b$

## 第二部分：矩阵运算

*   **矩阵加法:**
	* 对于 $A,B\in \mathbb R^{n\times m}$：$(A+B)_{ij}=a_{ij}+b_{ij}$
*   **矩阵乘法**
	* 对于 $A\in\mathbb R^{m\times n},B\in\mathbb R^{n\times k},C=AB\in\mathbb R^{m\times k}$：
	$$c_{ij}=\sum_{l=1}^na_{il}b_{lj}$$
	* **仅当内部维度匹配时才定义乘法：**  
	$$A_{m\times n}B_{n\times k}$$
	* **逐元素乘法**称为**哈达玛积 (Hadamard product)：**
	$$(A\circ B)_{ij}=a_{ij}b_{ij}$$
	* **单位矩阵 (Identity matrix):**
	$$I_n=\begin{pmatrix}1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1\end{pmatrix}\in \mathbb R^{n\times n}$$
	* **乘法单位元:**
	$$I_nA=AI_n=A\in \mathbb R^{m\times n}$$
	* **代数性质:**
		* 结合律: $(AB)C=A(BC)$
		* 分配律: $(A+B)C=AC+BC,A(C+D)=AC+AD$
*   **矩阵的逆 (Matrix Inverse):** 对于 $A,B\in R^{n\times n}$，如果 $AB=I_n=BA$，则称 $B$ 是 $A$ 的逆矩阵。记作 $A^{-1}$。
	* **可逆性：** 如果 $A^{-1}$ 存在，则称 $A$ 是正则的/可逆的/非奇异的。否则，它是奇异的/不可逆的。
	* **唯一性：** 如果 $A^{-1}$ 存在，则它是唯一的。
	* $A^{-1}$ 存在 $\iff$ $|A|\neq 0:$ 
	$$A^{-1}=\dfrac{1}{|A|}\text{adj}(A)$$
	* **性质：**
		* $AA^{-1}=I=A^{-1}A$
		* $(AB)^{-1}=B^{-1}A^{-1}$
*   **矩阵转置 (Matrix Transpose)**
	* **转置：** 对于 $A\in R^{m\times n}$ , $A^T\in R^{n\times m}$ 定义为 $(A^T)_{ij}=a_{ji}$。
	* **性质：**
		* $(A^T)^T=A$
		* $(AB)^T=B^TA^T$
		* $(A+B)^T=A^T+B^T$
		* 如果 $A$ 可逆，则 $(A^{-1})^T=(A^T)^{-1}$
*   **对称矩阵 (Symmetric Matrix):** 如果 $A=A^T$，则 $A\in \mathbb R^{n\times n}$ 是对称的。
	* **和：** 对称矩阵的和仍然是对称的。
	* **性质：**
		* **合同变换下的对称性:** 
		$$PAP^T=(PAP^T)^T$$
		* **实对称矩阵的可对角化性：**
			* 每个实对称矩阵都是**可正交对角化的**。这意味着存在一个正交矩阵 $Q$ (其中 $Q^TQ=I$) 和一个对角矩阵 $D$，使得 $A=QDQ^T$。$D$ 的对角线元素是 $A$ 的特征值，而 $Q$ 的列是对应的标准正交特征向量。
*   **标量乘法:** 
$$(\lambda A)_{ij}=\lambda(A_{ij})$$
*  **解：**
	*   **考虑系统：**
	$$ \begin{bmatrix}1 & 0 & 8 & -4 \\ 0 & 1 & 2 & 12 \end{bmatrix}
	    \begin{bmatrix}
	    x_1 \\
	    x_2 \\
	    x_3 \\
	    x_4
	    \end{bmatrix}
	    =
	    \begin{bmatrix}
	    42 \\
	    8
	    \end{bmatrix}
	    $$
	*   **两个方程，四个未知数：** 系统是**欠定的 (underdetermined)**，因此我们期望有无穷多解。
	*   前两列构成一个单位矩阵。这意味着 $x_1$ 和 $x_2$ 是**主元变量**（或基本变量），而 $x_3$ 和 $x_4$ 是**自由变量**。
	    *   为了找到一个特解，我们可以将自由变量设为零。
	    *   设 $x_3 = 0$ 和 $x_4 = 0$ 得到：
	        *   从第一行：$1 \cdot x_1 + 0 \cdot x_2 + 8 \cdot 0 - 4 \cdot 0 = 42 \implies x_1 = 42$
	        *   从第二行：$0 \cdot x_1 + 1 \cdot x_2 + 2 \cdot 0 + 12 \cdot 0 = 8 \implies x_2 = 8$
	*   **因此，$[42, 8, 0, 0]^T$ 是一个特解（也称为特殊解）。**
	*   为了找到非齐次系统 `Ax = b` 的**通解**（描述所有无穷多解），我们需要理解相关的**齐次系统** `Ax = 0` 的解。
	*   **考虑齐次系统：**
	$$\begin{bmatrix}
	    1 & 0 & 8 & -4 \\
	    0 & 1 & 2 & 12
	    \end{bmatrix}
	    \begin{bmatrix}
	    x_1 \\
	    x_2 \\
	    x_3 \\
	    x_4
	    \end{bmatrix}
	    =
	    \begin{bmatrix}
	    0 \\
	    0
	    \end{bmatrix}
	    $$
	*   同样，$x_1$ 和 $x_2$ 是主元变量，$x_3$ 和 $x_4$ 是自由变量。我们用自由变量来表示主元变量：
	    *   从第一行：$x_1 + 8x_3 - 4x_4 = 0 \implies x_1 = -8x_3 + 4x_4$
	    *   从第二行：$x_2 + 2x_3 + 12x_4 = 0 \implies x_2 = -2x_3 - 12x_4$
	*   设自由变量为参数：$x_3 = s$ 和 $x_4 = t$，其中 $s, t \in \mathbb{R}$。
	*   **齐次解 ($x_h$)** 可以写成向量形式：
	$$x_h = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix} = \begin{bmatrix} -8s + 4t \\ -2s - 12t \\ s \\ t \end{bmatrix} = s \begin{bmatrix} -8 \\ -2 \\ 1 \\ 0 \end{bmatrix} + t \begin{bmatrix} 4 \\ -12 \\ 0 \\ 1 \end{bmatrix}
	 $$
	    向量 $\begin{bmatrix} -8 \\ -2 \\ 1 \\ 0 \end{bmatrix}$ 和 $\begin{bmatrix} 4 \\ -12 \\ 0 \\ 1 \end{bmatrix}$ 构成了矩阵 $A$ 的**零空间 (null space)** 的一组基，记为 $N(A)$。这些有时也被称为 $Ax=0$ 的**特殊解**。
	
	*   **`Ax = b` 的通解：**
	    一个相容的线性系统 `Ax = b` 的完整解集是任意一个特解 $x_p$ 与整个零空间 $N(A)$ 的和。
	$$\mathbf{x} = x_p + x_h = x_p + N(A)
	    $$
	    使用我们的具体例子：
	$$\mathbf{x} = \begin{bmatrix} 42 \\ 8 \\ 0 \\ 0 \end{bmatrix} + s \begin{bmatrix} -8 \\ -2 \\ 1 \\ 0 \end{bmatrix} + t \begin{bmatrix} 4 \\ -12 \\ 0 \\ 1 \end{bmatrix} \quad \text{对于任意 } s, t \in \mathbb{R}
	    $$
	    这个公式描述了原始系统 `Ax = b` 的所有无穷多解。

*  **秩-零度定理 (Rank-Nullity Theorem)**

	*   **定理陈述：**
	    对于一个 $m \times n$ 矩阵 $A$， $A$ 的**秩**加上 $A$ 的**零度**等于列数 $n$。
	    即：
	    `rank(A) + nullity(A) = n`
	    这也意味着：
	    `nullity(A) = n - rank(A)`
	
	*   **术语解释：**
	    *   **A 的秩 (rank(A)):**
	        *   定义：矩阵 $A$ 的**列空间 (Col(A))** 的维度。它等于 $A$ 的行简化阶梯型 (RREF) 中**主元变量**的数量。
	    *   **A 的零度 (nullity(A)):**
	        *   定义：矩阵 $A$ 的**零空间 (Nul(A))** 的维度。它等于 $A$ 的行简化阶梯型 (RREF) 中**自由变量**的数量。
	    *   **$n$ (列数 / 变量数):**
	        *   定义：矩阵 $A$ 的列数，代表系统中的未知数总数。
	
	*   **直观意义：**
	    该定理从根本上表明，一个系统中的总变量数 ($n$) 被分为两部分：一部分受方程约束，其数量为**秩**；另一部分是解中可以自由选择的变量，其数量为**零度**。即，**（主元变量数）+（自由变量数）=（总变量数）**。
	
	*   **示例（使用 2x4 矩阵）：**
	    *   考虑我们之前讨论的矩阵 $A = \begin{bmatrix} 1 & 0 & 8 & -4 \\ 0 & 1 & 2 & 12 \end{bmatrix}$。
	    *   这里，列数 $n = 4$（因为有四个未知数 $x_1, x_2, x_3, x_4$）。
	    *   该矩阵已经处于行简化阶梯型。
	        *   **主元变量：** $x_1, x_2$（对应于每行中的首个 1）。因此，`rank(A) = 2`。
	        *   **自由变量：** $x_3, x_4$（不对应主元位置的变量）。因此，`nullity(A) = 2`。
	    *   **验证定理：**
	        *   `rank(A) + nullity(A) = 2 + 2 = 4`。这与列数 $n=4$ 相符。
	        *   `nullity(A) = n - rank(A) \implies 2 = 4 - 2`。这也完全成立。
	    *   这个例子完美地说明了秩-零度定理。

* **初等行变换 (Elementary Row Transformations)**
	*   **定义：**
	    初等行变换是可以对矩阵的行执行的一组操作。这些操作至关重要，因为它们将一个矩阵转换为一个等价矩阵（意味着它们保留了相应线性系统的解集，以及矩阵的行空间、列空间维度和零空间）。
	
	*   **初等行变换的类型：**
	    有三种基本的初等行变换：
	
	    1.  **行交换 (Interchange Two Rows):**
	        *   **描述：** 交换两行的位置。
	        *   **表示法：** $R_i \leftrightarrow R_j$ (交换第 $i$ 行和第 $j$ 行)
	
	    2.  **行缩放 (Multiply a Row by a Non-zero Scalar):**
	        *   **描述：** 将一行中的所有元素乘以一个非零常数标量。
	        *   **表示法：** $k R_i \to R_i$ (将第 $i$ 行乘以标量 $k$，其中 $k \neq 0$)
	
	    3.  **行加法 (Add a Multiple of One Row to Another Row):**
	        *   **描述：** 将一行的标量倍数加到另一行上。被加的行被结果替换。
	        *   **表示法：** $R_i + k R_j \to R_i$ (将 $k$ 倍的第 $j$ 行加到第 $i$ 行，并替换第 $i$ 行)
	
	*   **目的和重要性：**
	    *   **求解线性系统：** 初等行变换是**高斯消元法**和**高斯-若尔当消元法**的基础，这些算法通过将增广矩阵转换为行阶梯型或行简化阶梯型来求解线性方程组。
	    *   **求矩阵的逆：** 它们可用于求方阵的逆。
	    *   **确定秩：** 它们有助于找到矩阵的秩（[[Notion/Class/Concept/REF, RREF\|REF, RREF]] 中主元/非零行的数量）。
	    *   **寻找零空间基：** 它们对于将矩阵转换为 RREF 以识别自由变量并确定零空间的基至关重要。
	    *   **等价性：** 如果一个矩阵可以通过一系列初等行变换转换为另一个矩阵，则这两个矩阵是**行等价**的。行等价的矩阵具有相同的行空间、零空间，因此具有相同的秩。
	*   **重要性：**
		* 如果矩阵是：
	$$\begin{bmatrix}
		\mathbf{1} & 0 & 0 & 5 & \bigm| & 10 \\
		0 & \mathbf{1} & 0 & -2 & \bigm| & 7 \\
		0 & 0 & 0 & 0 & \bigm| & a+1
		\end{bmatrix}$$
		* 当且仅当 $a=-1$ 时，系统有解。
		*  **`[0 0 0 0 | 0]` 这一行意味着什么？**
		    *   一行全为零，包括常数项，意味着与此行对应的原始方程是系统中**其他方程的线性组合**。换句话说，这个方程是多余的，没有提供关于变量的新信息。
		    *   关键的是，`0 = 0` 永远是一个真命题。这表明系统是**相容的**（有解）。它并**不**意味着没有解（一个不相容的系统会有一行像 `[0 0 0 0 | c]`，其中 `c ≠ 0`）。

* **行等价矩阵 (Row Equivalent Matrices)**
	*   **定义：**
	    如果一个矩阵可以通过有限次的初等行变换从另一个矩阵得到，则称这两个矩阵是**行等价**的。
	*   **表示法：**
	    如果矩阵 $A$ 与矩阵 $B$ 行等价，我们写作 $A \sim B$，$B$ 也可写作 $\overset{\sim}A$。
	*   **行等价矩阵的关键性质（保持不变的特性）：**
	    初等行变换之所以强大，是因为它们保留了矩阵的几个基本性质，这些性质对于求解线性系统和理解矩阵空间至关重要：
	
	    1.  **线性系统具有相同的解集：** 如果增广矩阵 $[A | b]$ 与另一个增广矩阵 $[A' | b']$ 行等价，那么线性系统 $Ax=b$ 与 $A'x=b'$ 具有完全相同的解集。
	    2.  **相同的行空间：** 行空间（由矩阵行向量张成的向量空间）在初等行变换下保持不变。
	    3.  **相同的零空间：** 零空间（齐次方程 $Ax=0$ 的所有解的集合）保持不变。
	    4.  **相同的秩：** 由于行空间和零空间的维度被保留，矩阵的秩（等于列空间的维度，也等于行空间的维度）也被保留。
	    5.  **相同的行简化阶梯型 (RREF)：** 每个矩阵都行等价于一个唯一的行简化阶梯型 (RREF)。
	
*   **通过增广矩阵计算逆矩阵：**
    行简化阶梯型 (RREF) 对于求矩阵的逆非常有用。这种策略也被称为**高斯-若尔当消元法求逆**。
    *   **要求：** 矩阵 **A 必须是方阵** ($A \in \mathbb{R}^{n \times n}$)。
    *   **核心思想：** 计算 $A^{-1}$ 实质上是求解矩阵方程 $AX = I_n$。
    *   **步骤：**
        1.  **写出增广矩阵 `[A | I_n]`：**
        2.  **执行高斯消元法（行化简）：** 使用初等行变换将左侧的 $A$ 变为单位矩阵 $I_n$。
            $$ [A | I_n] \xrightarrow{\text{高斯消元}} [I_n | A^{-1}] $$
        3.  **读取逆矩阵：** 如果左侧成功变为 $I_n$，那么右侧的矩阵就是 $A^{-1}$。
        4.  **不可逆的情况：** 如果在行化简过程中，左侧无法变为 $I_n$（例如，出现一个全零行），则矩阵 $A$ 是奇异的（不可逆），$A^{-1}$ 不存在。
        5.  **证明：** [[Notion/Class/Proof/Compatibility of Matrix Multiplication with Partitioned Matrices\|Compatibility of Matrix Multiplication with Partitioned Matrices]]
	        $$C[A|I_n]=[CA|CI_n]=[I_n|C]\Rightarrow C=A^{-1}$$
    *   **限制：** 对于非方阵，此方法不适用。

* **求解线性系统 (`Ax = b`) 的算法：直接法**
	*   **1. 直接求逆法：**
	    *   **适用性：** 当系数矩阵 $A$ 是**方阵且可逆**时。
	    *   **公式：** $x = A^{-1}b$
	
	*   **2. 伪逆法 ([[Notion/Class/Proof/Moore Penrose Pseudo inverse\|Moore Penrose Pseudo inverse]]):**
	    *   **适用性：** 当 $A$ **不是方阵但列线性无关**（即满列秩）时。这在超定系统（方程多于未知数, $m > n$）中很常见。
	    *   **公式：** $x = (A^T A)^{-1} A^T b$
	    *   **结果：** 这个公式给出了**最小范数最小二乘解**。它找到使残差的欧几里得范数 $\|Ax - b\|_2^2$ 最小化的向量 $x$。
	
	*   **局限性 (求逆法和伪逆法的共同点):**
	    *   **计算成本高：** 计算矩阵的逆或伪逆通常**计算成本很高**，对于一个 $n \times n$ 矩阵，其计算成本通常为 $O(n^3)$。
	    *   **数值不稳定：** 对于大型或病态系统，这些方法可能**数值不稳定**。
	
	*   **3. 高斯消元法：**
	    *   **机制：** 一种系统性的方法，通过一系列初等行变换将增广矩阵 `[A | b]` 简化为行阶梯型或行简化阶梯型来求解 `Ax = b`。
	    *   **可扩展性：** 高斯消元法对于数千个变量通常是**高效的**。但对于非常大的系统（例如数百万个变量）则**不实用**，因为其计算成本随变量数量呈立方级增长 ($O(n^3)$)。

## 第三部分：向量空间与群

*   **群 (Group):**
	一个**群**是一个集合 $G$ 与一个二元运算 $*$ 的组合，该运算满足以下四个公理：
	
	1.  **封闭性：** 对所有 $a, b \in G$，$a * b$ 的结果也在 $G$ 中。
	2.  **结合律：** 对所有 $a, b, c \in G$，$(a * b) * c = a * (b * c)$。
	3.  **单位元：** 存在一个元素 $e \in G$（称为单位元），使得对每个 $a \in G$，$a * e = e * a = a$。
	4.  **逆元：** 对每个 $a \in G$，存在一个元素 $a^{-1} \in G$（称为 $a$ 的逆元），使得 $a * a^{-1} = a^{-1} * a = e$。
	
	---
	
	**附加术语：**
	*   **阿贝尔群 (Abelian Group)（交换群）：** 如果运算 $*$ 还满足**交换律**（即对所有 $a, b \in G$，$a * b = b * a$），则该群称为阿贝尔群。
	
	![Image/Class/Mathematics-for-AI/2.png](/img/user/Image/Class/Mathematics-for-AI/2.png)
    ![Image/Class/Mathematics-for-AI/3.png](/img/user/Image/Class/Mathematics-for-AI/3.png)
  ![Image/Class/Mathematics-for-AI/4.png](/img/user/Image/Class/Mathematics-for-AI/4.png)

### 笔记续

*   **向量空间 (Vector Space):**
    一个**向量空间**是一个由称为**向量**的对象组成的集合 ($V$)，以及一个**标量**集合（通常是实数 $\mathbb{R}$），配备了**向量加法**和**标量乘法**两种运算。这些运算必须满足十个公理。

    **向量空间的公理：**
    设 $\mathbf{u}, \mathbf{v}, \mathbf{w}$ 是 $V$ 中的向量， $c, d$ 是 $\mathbb{R}$ 中的标量。

    1.  **加法封闭性：** $\mathbf{u} + \mathbf{v}$ 在 $V$ 中。
    2.  **加法交换律：** $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$。
    3.  **加法结合律：** $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$。
    4.  **零向量（加法单位元）：** $V$ 中存在一个向量 $\mathbf{0}$，使得 $\mathbf{u} + \mathbf{0} = \mathbf{u}$。
    5.  **加法逆元：** 对每个向量 $\mathbf{u}$，$V$ 中存在一个向量 $-\mathbf{u}$，使得 $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$。
        *   **与群的联系：** 前五个公理意味着向量集合 $V$ 与加法运算 $(V, +)$ 构成一个**阿贝尔群**。

    6.  **标量乘法封闭性：** $c\mathbf{u}$ 在 $V$ 中。
    7.  **分配律：** $c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}$。
    8.  **分配律：** $(c+d)\mathbf{u} = c\mathbf{u} + d\mathbf{u}$。
    9.  **标量乘法结合律：** $c(d\mathbf{u}) = (cd)\mathbf{u}$。
    10. **标量单位元：** $1\mathbf{u} = \mathbf{u}$。

*   **子空间 (Subspace):**
    向量空间 $V$ 的一个**子空间**是 $V$ 的一个子集 $H$，它本身在 $V$ 上定义的相同加法和标量乘法运算下也是一个向量空间。
    *   **子空间判别法：** 要验证一个子集 $H$ 是否为子空间，只需检查三个条件：
        1.  **包含零向量：** $V$ 的零向量在 $H$ 中（$\mathbf{0} \in H$）。
        2.  **加法封闭性：** 对任意两个向量 $\mathbf{u}, \mathbf{v} \in H$，它们的和 $\mathbf{u} + \mathbf{v}$ 也在 $H$ 中。
        3.  **标量乘法封闭性：** 对任意向量 $\mathbf{u} \in H$ 和任意标量 $c$，向量 $c\mathbf{u}$ 也在 $H$ 中。
    *   **关键示例：**
        *   $\mathbb{R}^3$ 中任何穿过原点的直线或平面都是 $\mathbb{R}^3$ 的子空间。
        *   $m \times n$ 矩阵 $A$ 的**零空间**，记为 $N(A)$，是 $\mathbb{R}^n$ 的一个子空间。
        *   $m \times n$ 矩阵 $A$ 的**列空间**，记为 $Col(A)$，是 $\mathbb{R}^m$ 的一个子空间。

*   **线性组合 (Linear Combination):**
    给定向量空间 $V$ 中的向量 $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p$ 和标量 $c_1, c_2, \ldots, c_p$，由下式定义的向量 $\mathbf{y}$：
    $$\mathbf{y} = c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_p\mathbf{v}_p$$
    被称为 $\mathbf{v}_1, \ldots, \mathbf{v}_p$ 的一个**线性组合**，其权重为 $c_1, \ldots, c_p$。

*   **生成空间 (Span):**
    *   **定义：** 一组向量 $\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$ 的**生成空间**，记为 $\text{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$，是这些向量所有可能的**线性组合**的集合。
    *   **几何解释：**
        *   $\text{Span}\{\mathbf{v}\}$（其中 $\mathbf{v} \neq \mathbf{0}$）是穿过原点和 $\mathbf{v}$ 的直线。
        *   $\text{Span}\{\mathbf{u}, \mathbf{v}\}$（其中 $\mathbf{u}, \mathbf{v}$ 不共线）是包含原点、$\mathbf{u}$ 和 $\mathbf{v}$ 的平面。
    *   **性质：** 任何向量集的生成空间永远是一个**子空间**。

*   **线性无关与线性相关 (Linear Independence and Dependence):**
    *   **线性无关：** 如果向量方程 $c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_p\mathbf{v}_p = \mathbf{0}$ **只有平凡解**（$c_1 = c_2 = \cdots = c_p = 0$），则向量集 $\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$ 是**线性无关的**。
    *   **线性相关：** 如果存在**不全为零**的权重 $c_1, \ldots, c_p$ 使得该方程成立，则该集合是**线性相关的**。
    *   **直观意义：** 一组向量是线性相关的，当且仅当至少有一个向量可以写成其他向量的线性组合。线性无关的向量是无冗余的。

*   **基 (Basis):**
    向量空间 $V$ 的一个**基**是一个向量集合 $B = \{\mathbf{b}_1, \ldots, \mathbf{b}_n\}$，它满足两个条件：
    1.  集合 $B$ 是**线性无关的**。
    2.  集合 $B$ **生成**整个向量空间 $V$（即 $\text{Span}(B) = V$）。
    *   基是构建整个空间所需的“最小”向量集合。

*   **维度 (Dimension):**
    *   **定义：** 非零向量空间 $V$ 的**维度**，记为 $\text{dim}(V)$，是 $V$ 的**任意一个基中的向量数量**。零向量空间 $\{\mathbf{0}\}$ 的维度定义为 0。
    *   **唯一性：** 虽然一个向量空间可以有许多不同的基，但给定向量空间的所有基都具有相同数量的向量。
    *   **与秩-零度定理的联系：**
        *   矩阵 $A$ 的列空间的维度是其秩：$\text{dim}(\text{Col}(A)) = \text{rank}(A)$。
        *   矩阵 $A$ 的零空间的维度是其零度：$\text{dim}(N(A)) = \text{nullity}(A)$。
*   **使用高斯消元法检验线性无关性：**
    要检验一组向量 $\{\mathbf{v}_1, \ldots, \mathbf{v}_k\}$ 是否线性无关：
    1.  将这些向量作为列，构成一个矩阵 $A$。
    2.  对矩阵 $A$ 执行高斯消元，将其化为**行阶梯型**。

    *   与**主元列**对应的原始向量是线性无关的。
    *   与**非主元列**对应的向量可以写成前面主元列的线性组合。

    **示例：** 在以下行阶梯型矩阵中：
    $$
    \begin{pmatrix}
      \mathbf{1} & 3 & 0 \\
      0 & 0 & \mathbf{1}
    \end{pmatrix}
    $$
    第 1 列和第 3 列是**主元列**（它们对应的原始向量是无关的）；第 2 列是**非主元列**（它对应的原始向量是相关的）。

    因此，原始向量集（矩阵 $A$ 的列）**不是线性无关的**，因为至少有一个非主元列。换句话说，该集合是**线性相关的**。
*   **线性组合的线性无关性**
    假设我们有一组 $k$ 个线性无关的向量 $\{\mathbf{b}_1, \ldots, \mathbf{b}_k\}$，可以看作是 $k$ 维空间的一个基。我们可以构造一组新的 $m$ 个向量 $\{\mathbf{x}_1, \ldots, \mathbf{x}_m\}$，其中每个 $\mathbf{x}_j$ 都是基向量的线性组合：
    $$ \mathbf{x}_j = \sum_{i=1}^{k} \lambda_{ij} \mathbf{b}_i $$
    每组权重可以表示为一个**系数向量** $\boldsymbol{\lambda}_j \in \mathbb{R}^k$。

    *   **关键推论：** 新向量集 $\{\mathbf{x}_1, \ldots, \mathbf{x}_m\}$ 是线性无关的，*当且仅当* 它们对应的系数向量集 $\{\boldsymbol{\lambda}_1, \ldots, \boldsymbol{\lambda}_m\}$ 是线性无关的。
    
*   **生成集的维度定理 (一个基本定理)**
     向量空间的维度不能超过其任何生成集中的向量数量。一个直接的推论是，在维度为 $k$ 的向量空间中，任何包含超过 $k$ 个向量的集合必定是线性相关的。

*   **特殊情况：新向量数量多于基向量数量 ($m > k$)**

    **定理：** 如果你用 $k$ 个线性无关的向量来生成 $m$ 个新向量，并且 $m > k$，那么得到的新向量集 $\{\mathbf{x}_1, \ldots, \mathbf{x}_m\}$ **永远是线性相关的**。

    **证明 (使用矩阵的秩)：**

    1.  **关注系数向量：** 如上所述，$\{\mathbf{x}_j\}$ 的线性无关性等价于其系数向量 $\{\boldsymbol{\lambda}_j\}$ 的线性无关性。我们将证明集合 $\{\boldsymbol{\lambda}_1, \ldots, \boldsymbol{\lambda}_m\}$ 必定是线性相关的。

    2.  **构造系数矩阵：** 让我们将这些系数向量排列成一个矩阵 $\Lambda$ 的列：
        $$ \Lambda = [\boldsymbol{\lambda}_1, \boldsymbol{\lambda}_2, \ldots, \boldsymbol{\lambda}_m] $$
        由于每个系数向量 $\boldsymbol{\lambda}_j$ 都在 $\mathbb{R}^k$ 中，矩阵 $\Lambda$ 有 $k$ 行和 $m$ 列（它是一个 $k \times m$ 矩阵）。

    3.  **分析矩阵的秩：** 矩阵的**秩**有一个基本性质：它不能超过其行数或列数。具体来说，我们关心的是 $\text{rank}(\Lambda) \le k$（行数）。
        *   *（通过一个更基本的定理证明：秩是行空间的维度。行空间由 $k$ 个行向量生成。根据[[Notion/Class/Proof/The Dimension Theorem for Spanning Sets\|The Dimension Theorem for Spanning Sets]]，这个空间的维度不能超过 $k$）。*

    4.  **应用条件 $m > k$：** 我们已经确定了关于矩阵 $\Lambda$ 的两个关键事实：
        *   总列数为 $m$。
        *   秩，代表线性无关列的最大数量，最多为 $k$。即 $\text{rank}(\Lambda) \le k$。

    5.  **将秩与线性相关性联系起来：** 我们已知 $m > k$。这导致了一个关键的不等式：
        $$ \text{总列数 } (m) > \text{线性无关列的最大数量 } (\text{rank}(\Lambda)) $$
        这个不等式意味着 $\Lambda$ 的所有 $m$ 个列不可能都是线性无关的。如果你拥有的向量数量（$m$）超过了它们可以生成的空间的维度（秩，最多为 $k$），那么这个向量集必定是线性相关的。

    6.  **得出结论：** 因为 $\Lambda$ 的列（即系数向量 $\{\boldsymbol{\lambda}_j\}$）构成一个线性相关的集合，所以由它们定义的新向量集 $\{\mathbf{x}_j\}$ 也必定是**线性相关的**。证毕。

***

# 第二讲：线性代数：基与秩、线性映射、仿射空间

## 第一部分：基与秩

*   **生成集 (Generating Set or Spanning Set)**
    *   **定义：** 如果 $\text{Span}(S) = V$，则向量集 $S$ 被称为向量空间 $V$ 的一个**生成集**。
    *   **关键思想：** 生成集可能是**冗余的**；它可能包含线性相关的向量。
    *   **示例：** 集合 $S = \{(1, 0), (0, 1), (1, 1)\}$ 是 $\mathbb{R}^2$ 的一个生成集。它是冗余的，因为 $(1, 1)$ 是另外两个向量的线性组合。

*   **生成空间 (Span) 的附加属性**
    *   **与线性系统的联系：** 线性方程组 $A\mathbf{x} = \mathbf{b}$ 有解，当且仅当向量 $\mathbf{b}$ 位于矩阵 $A$ 的列的生成空间中。即 $\mathbf{b} \in \text{Col}(A)$。

*   **基 (Basis) 的附加属性**
    *   **唯一表示定理：** 基的一个关键性质是，空间中的每个向量 $\mathbf{v}$ 都可以用**唯一的方式**表示为基向量的线性组合。这个唯一组合的系数被称为 $\mathbf{v}$ 相对于该基的**坐标**。
    *   **示例（标准基）：** $\mathbb{R}^n$ 最常用的基是**标准基**，它由 $n \times n$ 单位矩阵 $I_n$ 的列组成。对于 $\mathbb{R}^3$，标准基是 $\{\mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3\} = \left\{ \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} \right\}$。
    
*   **基的特征**
    对于向量空间 $V$ 中的一个非空向量集 $B$，以下陈述是等价的（即如果一个为真，则全部为真）：
    1.  $B$ 是 $V$ 的一个**基**。
    2.  $B$ 是一个**最小生成集**（即它生成 $V$，但 $B$ 的任何真子集都不能生成 $V$）。
    3.  $B$ 是一个**最大线性无关集**（即它是线性无关的，但向其中添加任何其他来自 $V$ 的向量都会使该集线性相关）。

*   **维度的进一步性质**

    *   **存在性与规模的唯一性：** 每个非平凡的向量空间都有一个基。虽然一个空间可以有许多不同的基，但它们都将具有相同数量的向量。这使得维度的概念是明确定义的。
    *   **子空间维度：** 如果 $U$ 是向量空间 $V$ 的一个子空间，那么 $\text{dim}(U) \le \text{dim}(V)$。等号成立当且仅当 $U = V$。
    *   **重要澄清：** 空间的维度指的是其**基中向量的数量**，而不是每个向量中的分量数量。例如，由单个向量 $\begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$ 生成的子空间是一维的，尽管该向量存在于 $\mathbb{R}^3$ 中。

*   **如何为子空间找到一个基（基提取法）**

    为由一组向量 $\{\mathbf{v}_1, \ldots, \mathbf{v}_m\}$ 的生成空间定义的子空间 $U$ 找到一个基：
    1.  创建一个矩阵 $A$，其列为向量 $\{\mathbf{v}_1, \ldots, \mathbf{v}_m\}$。
    2.  将矩阵 $A$ 化简为其**行阶梯型**。
    3.  识别包含**主元**的列。
    4.  $U$ 的基由集合 $\{\mathbf{v}_1, \ldots, \mathbf{v}_m\}$ 中与这些主元列对应的**原始向量**组成。
    
*   **矩阵的秩**

    *   **定义：** 矩阵 $A$ 的**秩**，记为 $\text{rk}(A)$，是 $A$ 中线性无关列的数量。线性代数的一个基本定理指出，这个数字总是等于线性无关行的数量。
    *   **关键性质：** 矩阵的秩等于其转置的秩：$\text{rk}(A) = \text{rk}(A^T)$。

*   **秩及其与基本子空间的联系**

    *   **列空间 (像/值域):** $A$ 的秩是其列空间的维度。
        $\text{rk}(A) = \text{dim}(\text{Col}(A))$
    *   **零空间 (核):** 秩通过秩-零度定理确定了零空间的维度。对于一个 $m \times n$ 矩阵 $A$：
        $\text{dim}(\text{Nul}(A)) = n - \text{rk}(A)$

*   **秩的性质和应用**

    *   **方阵的可逆性：** 一个 $n \times n$ 矩阵 $A$ 是可逆的，当且仅当其秩等于其维度，即 $\text{rk}(A) = n$。
    *   **线性系统的可解性：** 系统 $A\mathbf{x} = \mathbf{b}$ 至少有一个解，当且仅当系数矩阵 $A$ 的秩等于增广矩阵 $[A|\mathbf{b}]$ 的秩。
    *   **满秩和秩亏：**
        *   如果一个矩阵的秩是其维度可能的最大值，则该矩阵为**满秩**：$\text{rk}(A) = \min(m, n)$。
        *   如果 $\text{rk}(A) < \min(m, n)$，则矩阵是**秩亏的**，表明其行或列之间存在线性相关性。

*   **为什么秩很重要**

    矩阵的秩是揭示其基本结构的核心概念。它告诉我们：
    *   线性无关行/列的最大数量。
    *   数据的维度（由列生成的子空间的维度）。
    *   线性系统是否相容（有解）。
    *   方阵是否有逆。
    *   它对于识别冗余和简化数据分析、优化和机器学习中的问题至关重要。

*   **总结：将秩、基和主元联系起来**
    1.  你从一组向量开始。
    2.  你将它们作为列放入矩阵 $A$ 中。
    3.  你执行高斯消元法找到**主元**。
    4.  **主元的数量**就是矩阵 $A$ 的**秩**。
    5.  这个秩也是由原始向量生成的子空间的**维度**。
    6.  与**主元列**对应的**原始向量**构成了该子空间的一个**基**。

## 第二部分：线性映射

*   **线性映射 (Linear Mappings / Linear Transformations)**
    *   **定义：** 从向量空间 $V$到向量空间 $W$ 的一个映射（或函数）$\Phi: V \to W$ 被称为**线性的**，如果它保持了两个基本的向量空间运算：
        1.  **可加性：** 对所有 $\mathbf{x}, \mathbf{y} \in V$，$\Phi(\mathbf{x} + \mathbf{y}) = \Phi(\mathbf{x}) + \Phi(\mathbf{y})$。
        2.  **齐次性：** 对任何标量 $\lambda$，$\Phi(\lambda\mathbf{x}) = \lambda\Phi(\mathbf{x})$。
    *   **矩阵表示：** 任何有限维向量空间之间的线性映射都可以通过矩阵乘法来表示：$\Phi(\mathbf{x}) = A\mathbf{x}$，其中 $A$ 是某个矩阵。

*   **映射的性质：单射、满射、双射**
    *   **单射 (Injective / One-to-one)：** 如果不同的输入总是映射到不同的输出，则映射是单射的。
    *   **满射 (Surjective / Onto)：** 如果映射的值域等于其陪域，则映射是满射的。这意味着目标空间 $W$ 中的每个元素都是起始空间 $V$ 中至少一个元素的像。
    *   **双射 (Bijective)：** 如果一个映射既是单射又是满射，则它是双射的。双射映射有一个唯一的逆映射 $\Phi^{-1}$。

*   **特殊类型的线性映射**
    *   **同态 (Homomorphism)：** 对于向量空间，同态只是**线性映射**的另一个术语。
    *   **同构 (Isomorphism)：** 既是线性映射又是**双射**的映射。同构的向量空间在结构上是相同的。
    *   **自同态 (Endomorphism)：** 从一个向量空间**到其自身**的线性映射（$\Phi: V \to V$）。
    *   **自同构 (Automorphism)：** 既是自同态又是**双射**的映射。它是从一个向量空间到其自身的同构（例如，旋转或反射）。
    *   **恒等映射 (Identity Mapping)：** 由 $\text{id}(\mathbf{x}) = \mathbf{x}$ 定义的映射。

*   **同构 (Isomorphism)**
    *   **同构与维度：** 一个基本定理指出，两个有限维向量空间 $V$ 和 $W$ 是**同构的**（结构上相同），当且仅当它们具有相同的维度。
         $\text{dim}(V) = \text{dim}(W) \iff V \cong W$
    *   **直观理解：** 这意味着任何n维向量空间本质上都是 $\mathbb{R}^n$ 的一个“重新标记”。

*   **通过有序基的矩阵表示**
    通过选择一个**有序基**，抽象的n维空间 $V$ 和具体的空间 $\mathbb{R}^n$ 之间的同构关系变得实用。基向量的顺序对于定义坐标很重要。
    *   **表示法：** 我们用括号表示一个有序基，例如 $B = (\mathbf{b}_1, \ldots, \mathbf{b}_n)$。

*   **坐标和坐标向量**
    *   **定义：** 给定 $V$ 的一个有序基 $B = (\mathbf{b}_1, \ldots, \mathbf{b}_n)$，每个向量 $\mathbf{x} \in V$ 都可以唯一地写成：
        $$ \mathbf{x} = \alpha_1\mathbf{b}_1 + \cdots + \alpha_n\mathbf{b}_n $$
        标量 $\alpha_1, \ldots, \alpha_n$ 被称为 $\mathbf{x}$ 相对于基 $B$ 的**坐标**。
    *   **坐标向量：** 我们将这些坐标收集成一个列向量，它在标准空间 $\mathbb{R}^n$ 中表示 $\mathbf{x}$：
        $$ [\mathbf{x}]_B = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix} \in \mathbb{R}^n $$

*   **坐标系和基变换**
    *   **概念：** 一个基为向量空间定义了一个坐标系。$\mathbb{R}^2$ 中熟悉的笛卡尔坐标只是相对于标准基 $(\mathbf{e}_1, \mathbf{e}_2)$ 的坐标。任何其他基都定义了一个不同但同样有效的坐标系。

*   **对线性映射的重要性**
    一旦我们为输入和输出空间固定了有序基，我们就可以将任何线性映射表示为一个具体的**矩阵**。这个矩阵表示完全依赖于所选的基。

## 第三部分：基变换和变换矩阵

本节涵盖了将抽象线性映射表示为矩阵以及改变坐标系的机制。

### 1. 线性映射的变换矩阵

变换矩阵为抽象的线性映射提供了一个具体的计算表示，这是相对于所选基而言的。

*   **定义和背景：**
    我们给定一个线性映射 $\Phi: V \to W$，向量空间 $V$ 的一个有序基 $B = (\mathbf{b}_1, \ldots, \mathbf{b}_n)$，以及向量空间 $W$ 的一个有序基 $C = (\mathbf{c}_1, \ldots, \mathbf{c}_m)$。

*   **变换矩阵 ($A_\Phi$) 的构造：**
    变换矩阵 $A_\Phi$ 是一个 $m \times n$ 矩阵，其列描述了输入基向量如何被 $\Phi$ 变换。构造如下：
    1.  对每个输入基向量 $\mathbf{b}_j$（从 $j=1$ 到 $n$）：
    2.  应用线性映射得到其像：$\Phi(\mathbf{b}_j) \in W$。
    3.  将这个像表示为输出基向量 $C$ 的唯一线性组合：
        $$ \Phi(\mathbf{b}_j) = \alpha_{1j}\mathbf{c}_1 + \alpha_{2j}\mathbf{c}_2 + \cdots + \alpha_{mj}\mathbf{c}_m $$
    4.  这个组合的系数构成了矩阵 $A_\Phi$ 的第 $j$ 列。这一列是 $\Phi(\mathbf{b}_j)$ 相对于基 $C$ 的坐标向量：
        $$ A_\Phi \text{ 的第 } j \text{ 列} = [\Phi(\mathbf{b}_j)]_C = \begin{pmatrix} \alpha_{1j} \\ \alpha_{2j} \\ \vdots \\ \alpha_{mj} \end{pmatrix} $$

*   **用法和解释：**
    这个矩阵将任何 $\mathbf{v} \in V$ 的坐标向量（相对于基 $B$）映射到其像 $\Phi(\mathbf{v}) \in W$ 的坐标向量（相对于基 $C$）。核心运算公式是：
    $$ [\Phi(\mathbf{v})]_C = A_\Phi [\mathbf{v}]_B $$
    这个公式将抽象的函数应用转化为具体的矩阵-向量乘法。矩阵 $A_\Phi$ 是映射 $\Phi$ *相对于所选基*的表示；改变任何一个基都会导致同一个线性映射有不同的变换矩阵。

*   **可逆性：**
    线性映射 $\Phi$ 是一个可逆的同构，当且仅当其变换矩阵 $A_\Phi$ 是方阵 ($m=n$) 且可逆。[[Notion/Class/Proof/Non-Invertible Transformations and Information Loss\|Non-Invertible Transformations and Information Loss]]

### 2. 基变换矩阵

这是变换矩阵的一个特殊应用，用于在*同一个*向量空间内将一个向量的坐标从一个基转换到另一个基。这个过程等价于为**恒等映射**（$\text{id}: V \to V$，其中 $\text{id}(\mathbf{x}) = \mathbf{x}$）寻找变换矩阵。

*   **基变换矩阵 ($P_{B \leftarrow B'}$):**
    这个矩阵将坐标从一个新基 $B'$ 转换到一个旧基 $B$。它的列是新基向量在旧基下的坐标向量。
    $$ P_{B \leftarrow B'} = \Big[ \ [\mathbf{b}'_1]_B \ \ \ [\mathbf{b}'_2]_B \ \ \cdots \ \ [\mathbf{b}'_n]_B \ \Big] $$
    *注意：如果“旧”基 $B$ 是 $\mathbb{R}^n$ 中的标准基，则这个矩阵的列就是新基 $B'$ 的向量本身。*

*   **使用公式：**
    *   将坐标**从新基 ($B'$) 转换到旧基 ($B$)**：
        $$ [\mathbf{x}]_B = P_{B \leftarrow B'} \ [\mathbf{x}]_{B'} $$
    *   将坐标**从旧基 ($B$) 转换到新基 ($B'$)**：
        $$ [\mathbf{x}]_{B'} = (P_{B \leftarrow B'})^{-1} \ [\mathbf{x}]_B $$

### 3. 线性映射的基变换定理

[[Notion/Class/Proof/Change-of-Basis Theorem\|Change-of-Basis Theorem]]
该定理提供了一个公式，用于在改变线性映射的定义域和陪域的基（坐标系）时，计算新的变换矩阵。

*   **定理陈述：**
    给定一个线性映射 $\Phi : V \to W$，其中：
    *   一个“旧”输入基 $B$ 和一个“新”输入基 $B̃$，都用于**定义域** $V$。
    *   一个“旧”输出基 $C$ 和一个“新”输出基 $C̃$，都用于**陪域** $W$。
    *   原始的变换矩阵 $A_\Phi$（相对于旧基 $B$ 和 $C$）。

    新的变换矩阵 $Ã_\Phi$（相对于新基 $B̃$ 和 $C̃$）由下式给出：
    $$ Ã_\Phi = T^{-1}A_\Phi S $$
    其中基变换矩阵定义为：
    *   $S$：**定义域 $V$ 内**的基变换矩阵。它将坐标从**新输入基 $B̃$** 转换到**旧输入基 $B$**。
    *   $T$：**陪域 $W$ 内**的基变换矩阵。它将坐标从**新输出基 $C̃$** 转换到**旧输出基 $C$**。

*   **公式解释（变换的“路径”）：**
    该公式表示对一个坐标向量进行的一系列三个操作。坐标的路径是 $B̃ \to B \to C \to C̃$。
    1.  **第1步：`S` (从 $B̃ \to B$ 在定义域中)**: 我们从一个向量在新输入基下的坐标 $[\mathbf{v}]_{B̃}$ 开始。我们应用 $S$ 将这些坐标转换为旧输入基：$S[\mathbf{v}]_{B̃}=[\mathbf{v}]_B$。
    2.  **第2步：`AΦ` (从基 $B$ 到基 $C$)**: 我们将原始变换矩阵 $A_\Phi$ 应用到现已用旧输入基 $B$ 表示的坐标上。这得到了像在旧输出基 $C$ 下的坐标：$A_\Phi([\mathbf{v}]_B) = [\Phi(\mathbf{v})]_C$。
    3.  **第3步：`T⁻¹` (从 $C \to C̃$ 在陪域中)**: 结果是用旧输出基 $C$ 表示的。为了用新输出基 $C̃$ 表示它，我们必须应用 $T$ 的逆。因为 $T$ 从 $C̃$ 转换到 $C$，所以必须使用 $T^{-1}$ 从 $C$ 转换到 $C̃$：$T^{-1}[\Phi(\mathbf{v})]_C = [\Phi(\mathbf{v})]_{C̃}$。

### 4. 矩阵等价与相似

这些概念形式化了这样一种思想：不同的矩阵可以表示相同的底层线性映射，只是使用了不同的坐标系。

*   **矩阵等价：**
    *   **定义：** 如果存在可逆矩阵 $S$（在定义域中）和 $T$（在陪域中）使得 $Ã = T^{-1}AS$，则两个 $m \times n$ 矩阵 $A$ 和 $Ã$ 是**等价的**。
    *   **解释：** 等价矩阵表示**完全相同的线性变换** $\Phi: V \to W$。它们仅仅是由于在定义域 $V$ 和陪域 $W$ 中选择了不同的基（坐标系）而对 $\Phi$ 的不同数值表示。

*   **矩阵相似：**
    *   **定义：** 如果存在一个单一的可逆矩阵 $S$ 使得 $Ã = S^{-1}AS$，则两个**方阵** $n \times n$ 的 $A$ 和 $Ã$ 是**相似的**。
    *   **解释：** 相似性是等价性在**自同态**（$\Phi: V \to V$）上的一个特例，其中同一个空间既作为定义域又作为陪域，因此相同的基变换（即 $T=S$）被应用于输入和输出坐标。

### 5. 线性映射的复合

*   **定理：** 如果 $\Phi : V \to W$ 和 $\Psi : W \to X$ 是线性映射，它们的复合 $(\Psi \circ \Phi) : V \to X$ 也是一个线性映射。
*   **矩阵表示：** 复合映射的变换矩阵是各个变换矩阵的乘积，顺序与应用顺序相反：
    $$ A_{\Psi \circ \Phi} = A_\Psi A_\Phi $$

## 第四部分：仿射空间与仿射子空间

虽然向量空间和子空间是基础，但它们受一个关键要求的限制：必须包含原点。仿射空间将这一思想推广到描述那些不一定穿过原点的几何对象，如直线和平面。

*   **核心直观：向量空间 vs. 仿射空间**
    *   **向量子空间**是一条**必须穿过原点**的直线或平面（或更高维的等价物）。
    *   **仿射子空间**是一条被**平移**过的直线或平面（或更高维的等价物），因此不再需要穿过原点。它是向量空间中的一个“平面”曲面。

*   **仿射子空间的正式定义**

    向量空间 $V$ 的一个**仿射子空间** $L$ 是一个可以表示为一个特定向量（一个点）和一个向量子空间之和的子集。

    $$ L = \mathbf{p} + U = \{ \mathbf{p} + \mathbf{u} \mid \mathbf{u} \in U \} $$

    其中：
    *   $\mathbf{p} \in V$ 是一个特定的向量，通常称为**平移向量**或**支撑点**。它起到平移空间的作用。
    *   $U$ 是 $V$ 的一个**向量子空间**，通常称为**方向空间**或相关的向量子空间。它定义了仿射子空间的方向和“形状”（直线、平面等）。

    仿射子空间 $L$ 的**维度**被定义为其方向空间 $U$ 的维度。

*   **几何示例：**
    *   **$\mathbb{R}^3$中的一条直线：** 穿过点 $\mathbf{p}$ 且方向向量为 $\mathbf{d}$ 的直线是一个仿射子空间。
        $$ L = \mathbf{p} + t\mathbf{d} \quad (t \in \mathbb{R}) $$
        这里，支撑点是 $\mathbf{p}$，方向空间是1维向量子空间 $U = \text{Span}\{\mathbf{d}\}$。
    *   **$\mathbb{R}^3$中的一个平面：** 包含点 $\mathbf{p}$ 且与向量 $\mathbf{u}$ 和 $\mathbf{v}$（线性无关）平行的平面是一个仿射子空间。
        $$ L = \mathbf{p} + s\mathbf{u} + t\mathbf{v} \quad (s, t \in \mathbb{R}) $$
        这里，支撑点是 $\mathbf{p}$，方向空间是2维向量子空间 $U = \text{Span}\{\mathbf{u}, \mathbf{v}\}$。

*   **与线性系统解的联系（关键应用）**

    仿射子空间为线性系统的解集提供了完美的几何描述。

    *   **齐次系统 `Ax = 0`：** 齐次系统的所有解的集合是 $A$ 的**零空间**，记为 $N(A)$。零空间永远是一个**向量子空间**。

    *   **非齐次系统 `Ax = b`：** 非齐次系统（其中 $\mathbf{b} \ne \mathbf{0}$）的所有解的集合是一个**仿射子空间**。
        回想通解公式：
        $$ \mathbf{x} = \mathbf{x}_p + \mathbf{x}_h $$
        让我们将其映射到仿射子空间 $L = \mathbf{p} + U$ 的定义中：
        *   **特解** $\mathbf{x}_p$ 作为**平移向量** $\mathbf{p}$。
        *   所有**齐次解** $\mathbf{x}_h$ 的集合是**方向空间** $U$。这正是零空间 $N(A)$。

        因此，`Ax = b` 的完整解集是仿射子空间：
        $$ L = \mathbf{x}_p + N(A) $$
        这意味着解集是零空间 $N(A)$ 被一个特解向量 $\mathbf{x}_p$ 平移后的结果。

*   **关键差异总结**

| 特征          | **向量子空间** (`U`)                        | **仿射子空间** (`L = p + U`) |
| :---------- | :------------------------------------- | :---------------------- |
| **必须包含原点?** | **是** (`0 ∈ U`)                        | **否**，除非 `p ∈ U`。       |
| **加法封闭?**   | **是**。如果 `u₁, u₂ ∈ U`，则 `u₁ + u₂ ∈ U`。 | **否**。通常，`l₁ + l₂ ∉ L`。 |
| **标量乘法封闭?** | **是**。如果 `u ∈ U`，则 `cu ∈ U`。           | **否**。通常，`cl₁ ∉ L`。     |
| **几何示例**    | 穿过原点的直线/平面。                            | 任何被平移的直线/平面。            |
| **线性系统示例**  | `Ax = 0` 的解集。                          | `Ax = b` 的解集。           |

*   **仿射组合**
    *   一个相关的概念是**仿射组合**。它是一种系数之和为1的线性组合。
        $$ \mathbf{y} = \alpha_1\mathbf{x}_1 + \alpha_2\mathbf{x}_2 + \cdots + \alpha_k\mathbf{x}_k \quad \text{其中} \quad \sum_{i=1}^k \alpha_i = 1 $$
    *   仿射子空间在仿射组合下是封闭的。一组点的所有仿射组合的集合构成了包含它们的最小仿射子空间（它们的“仿射包”）。
[[如何用仿射子空间 (Affine Subspace) 的结构来理解线性方程组 `Aλ = b` 的通解]]

## 第五部分：超平面 (Hyperplanes)

超平面是将在2D空间中的线和3D空间中的平面的概念推广到任意维度向量空间的结果。它是一种极其重要且常见的特殊仿射子空间。

### 1. 核心直观

-   在**2D**空间 ($\mathbb{R}^2$)中，超平面是一条**线**（1维）。
-   在**3D**空间 ($\mathbb{R}^3$)中，超平面是一个**平面**（2维）。
-   在**n维**空间 ($\mathbb{R}^n$)中，超平面是一个**(n-1)维**的“平坦”子空间。

它的关键功能是将整个空间“切片”成两个半空间，使其成为分类问题中理想的**决策边界**。

### 2. 超平面的两个等价定义

超平面可以用两种等价的方式定义：一种是代数的，一种是几何的。

##### **定义1：代数定义（通过单个线性方程）**

$\mathbb{R}^n$中的一个**超平面** $H$ 是满足单个线性方程的所有点 $\mathbf{x}$ 的集合：
$$ a_1x_1 + a_2x_2 + \cdots + a_nx_n = d $$
其中 $a_1, \dots, a_n$ 是不全为零的系数，`d` 是一个常数。

使用向量表示法，这个方程变得更加紧凑：
$$ \mathbf{a}^T \mathbf{x} = d $$
-   **法向量 $\mathbf{a}$**: 向量 $\mathbf{a} = (a_1, \dots, a_n)^T$ 被称为超平面的**法向量**。几何上，它与超平面本身**垂直**。
-   **偏移量 `d`**: 常数 `d` 决定了超平面距离原点的偏移量。
    -   如果 `d = 0`，超平面 `aᵀx = 0` 穿过原点，并且本身是一个**(n-1)维的向量子空间**。
    -   如果 `d ≠ 0`，超平面不穿过原点，是一个真正的**仿射子空间**。

##### **定义2：几何定义（通过仿射子空间）**

n维向量空间 $V$ 中的一个**超平面** $H$ 是一个维度为**n-1**的**仿射子空间**。
$$ H = \mathbf{p} + U $$
其中：
-   $\mathbf{p}$ 是超平面上的任意一个特定点（支撑点）。
-   $U$ 是一个维度为**n-1**的向量子空间（方向空间）。

### 3. 定义之间的联系

这两个定义是完全等价的。

-   **从代数到几何 (`aᵀx = d` → `p + U`)**:
    1.  **方向空间 `U`**: 方向空间 `U` 是与原超平面平行且穿过原点的超平面。它是满足 `aᵀu = 0` 的所有向量 `u` 的集合。这个集合是法向量 `a` 的**正交补**，维度为 n-1。
    2.  **支撑点 `p`**: 我们可以通过找到方程 `aᵀx = d` 的任意一个**特解**来找到一个支撑点 `p`。

### 4. 机器学习中的超平面

超平面是许多机器学习算法的核心，最著名的是**支持向量机 (SVM)**。

-   **作为决策边界**: 在二元分类问题中，目标是找到一个能够最好地分隔属于两个不同类别的数据点的超平面。
-   **SVM 超平面**: SVM 寻求找到一个由以下方程定义的最优超平面：
    $$ \mathbf{w}^T\mathbf{x} - b = 0 $$
    -   $\mathbf{w}$ 是权重向量，等价于**法向量** `a`。
    -   `b` 是偏置项，与**偏移量** `d` 相关。
-   **分类规则**:
    -   如果一个新的数据点 $\mathbf{x}_{\text{new}}$ 满足 $\mathbf{w}^T\mathbf{x}_{\text{new}} - b > 0$，它被分到一类（例如，正类）。
    -   如果它满足 $\mathbf{w}^T\mathbf{x}_{\text{new}} - b < 0$，它被分到另一类（例如，负类）。
    -   这意味着一个点的分类取决于它位于超平面的哪一侧。SVM的目标是找到使这个分隔“间隔”尽可能宽的 `w` 和 `b`。

## 第六部分：仿射映射 (Affine Mappings)

我们已经确定，形式为 `φ(x) = Ax` 的线性映射总是保持原点不变（即 `φ(0) = 0`）。然而，许多实际应用，特别是计算机图形学，需要包括**平移**的变换，这会移动原点。这种更一般的变换类别被称为仿射映射。

### 1. 核心思想：一个线性映射后跟一个平移

一个**仿射映射**本质上是一个**线性映射**和一个**平移**的复合。

-   **线性部分：** 处理旋转、缩放、剪切和其他保持原点固定的变换。
-   **平移部分：** 将整个结果移动到空间中的一个新位置。

### 2. 正式定义

从向量空间 `V` 到向量空间 `W` 的映射 `f: V → W` 被称为**仿射映射**，如果它可以写成以下形式：
$$ f(\mathbf{x}) = A\mathbf{x} + \mathbf{b} $$
其中：
-   `A` 是一个 $m \times n$ 矩阵，表示变换的**线性部分**。
-   `b` 是一个 $m \times 1$ 向量，表示**平移部分**。

**与线性映射的区别：**
-   如果平移向量 `b = 0`，仿射映射退化为纯粹的线性映射。
-   如果 `b ≠ 0`，则 `f(0) = A(0) + b = b`，这意味着原点不再映射到原点，而是被移动到由 `b` 定义的位置。

### 3. 仿射映射的关键性质

虽然仿射映射通常不是线性的（因为 `f(x+y) ≠ f(x) + f(y)`），但它们保留了几个关键的几何性质。

1.  **直线映射为直线：** 仿射映射将一条直线变换为另一条直线（或者在退化情况下，如果直线的方向在 `A` 的零空间中，则变换为单个点）。

2.  **平行性被保留：** 如果两条线是平行的，它们在仿射映射下的像也将是平行的。

3.  **长度比率被保留：** 如果点 `P` 是线段 `QR` 的中点，那么它的像 `f(P)` 将是像线段 `f(Q)f(R)` 的中点。

4.  **仿射组合被保留：** 这是仿射映射最基本的代数性质。如果一个点 `y` 是一组点 `xᵢ` 的仿射组合（即 `y = Σαᵢxᵢ` 且 `Σαᵢ = 1`），那么它的像 `f(y)` 是像点 `f(xᵢ)` 的**相同仿射组合**：
    $$ f\left(\sum \alpha_i \mathbf{x}_i\right) = \sum \alpha_i f(\mathbf{x}_i), \quad \text{前提是} \quad \sum \alpha_i = 1 $$

### 4. 齐次坐标：统一变换的技巧

在计算机图形学等领域，用**单一的矩阵乘法**来表示所有变换（包括平移）是非常理想的。标准形式 `Ax + b` 需要乘法和加法，这对于复合多个变换很不方便。

**齐次坐标**通过增加一个额外的维度，巧妙地解决了这个问题，有效地将一个仿射映射变成了更高维空间中的一个线性映射。

*   **工作原理：**
    1.  一个n维向量 `x = (x₁, ..., xₙ)ᵀ` 被表示为一个(n+1)维的齐次向量：
        $$ \mathbf{x}_{\text{hom}} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \\ 1 \end{bmatrix} $$
    2.  一个仿射映射 `f(x) = Ax + b` 由一个 `(n+1) × (n+1)` 的**增广变换矩阵**表示：
        $$ T_f = \begin{bmatrix}
           & A & & \mathbf{b} \\
           \hline
           0 & \cdots & 0 & 1
           \end{bmatrix}
        $$
        这里，`A` 是 $n \times n$ 的线性部分，`b` 是 $n \times 1$ 的平移向量。底行由零和一个一组成。

*   **统一的操作：**
    仿射变换现在可以用一次矩阵乘法完成：
    $$
    T_f \mathbf{x}_{\text{hom}} =
    \begin{bmatrix}
    A & \mathbf{b} \\
    \mathbf{0}^T & 1
    \end{bmatrix}
    \begin{bmatrix}
    \mathbf{x} \\
    1
    \end{bmatrix}
    =
    \begin{bmatrix}
    A\mathbf{x} + \mathbf{b} \\
    1
    \end{bmatrix}
    $$
    结果向量的前 `n` 个分量正是所需的 `Ax + b`，最后一个分量保持为 `1`。

*   **优势：**
    这种技术允许通过首先乘以它们各自的增广矩阵来复合一系列变换（例如，旋转、缩放、再平移）。得到的单一矩阵可以应用于所有点，极大地简化了复杂变换的计算和管理。

### 5. 总结

| 概念 | **线性映射 (`Ax`)** | **仿射映射 (`Ax + b`)** |
| :--- | :--- | :--- |
| **本质** | 旋转 / 缩放 / 剪切 | 线性变换 + 平移 |
| **保留原点?** | **是**, `f(0) = 0` | **否**, 通常 `f(0) = b` |
| **保留线性组合?** | **是** | **否** |
| **保留什么?** | 直线、平行性、**线性组合** | 直线、平行性、**仿射组合** |
| **表示法** | 矩阵 `A` | 矩阵 `A` 和向量 `b` |
| **齐次坐标形式** | $\begin{bsmallmatrix} A & \mathbf{0} \\ \mathbf{0}^T & 1 \end{bsmallmatrix}$ | $\begin{bsmallmatrix} A & \mathbf{b} \\ \mathbf{0}^T & 1 \end{bsmallmatrix}$ |

***

# 第三讲：解析几何：范数、内积、长度与距离、角度与正交性

### 第一部分：向量空间上的几何结构

在前面部分，我们建立了向量空间和线性映射的代数框架。现在，我们将为这些空间赋予**几何结构**，使我们能够形式化向量的**长度**、向量间的**距离**以及它们之间的**角度**等直观概念。这些概念由范数和内积来捕捉。

#### 1. 范数 (Norms)

范数是向量“长度”或“大小”这一直观概念的形式化推广。

*   **几何直观：** 向量的范数是其长度，即从原点到该向量所代表的点的距离。

*   **范数的正式定义：**
    向量空间 $V$ 上的一个**范数**是一个函数 $\|\cdot\| : V \to \mathbb{R}$，它为每个向量 $\mathbf{x} \in V$ 赋予一个非负实值 $\|\mathbf{x}\|$。该函数必须满足以下三个公理：

    1.  **正定性：** 长度是正的，除非是零向量。
        *   $\|\mathbf{x}\| \ge 0$
        *   $\|\mathbf{x}\| = 0 \iff \mathbf{x} = \mathbf{0}$

    2.  **绝对齐次性：** 缩放一个向量会以相同的因子缩放其长度。
        *   $\|\lambda\mathbf{x}\| = |\lambda|\|\mathbf{x}\|$

    3.  **三角不等式：** 三角形的一边长度不大于另外两边长度之和。
        *   $\|\mathbf{x} + \mathbf{y}\| \le \|\mathbf{x}\| + \|\mathbf{y}\|$

    配备了范数的向量空间称为**赋范向量空间**。

*   **$\mathbb{R}^n$上的范数示例：**
    对于向量 $\mathbf{x} = (x_1, \dots, x_n)^T$：

    *   **$L_1$-范数 (曼哈顿范数):** 测量“城市街区”距离。
        $$ \|\mathbf{x}\|_1 := \sum_{i=1}^n |x_i| $$

    *   **$L_2$-范数 (欧几里得范数):** 标准的“直线”距离。
        $$ \|\mathbf{x}\|_2 := \sqrt{\sum_{i=1}^n x_i^2} = \sqrt{\mathbf{x}^T\mathbf{x}} $$
        **这是默认范数。**当我们写 $\|\mathbf{x}\|$ 而不带下标时，几乎总是指欧几里得范数。

    *   **$L_\infty$-范数 (最大范数):** 长度由向量的最大分量决定。
        $$ \|\mathbf{x}\|_\infty := \max_{i=1}^n |x_i| $$

*   **由范数导出的距离：**
    任何范数都自然地定义了两个向量之间的**距离** $d(\mathbf{x}, \mathbf{y})$，即它们差向量的范数：
    $$ d(\mathbf{x}, \mathbf{y}) := \|\mathbf{x} - \mathbf{y}\| $$

#### 2. 内积 (Inner Products)

内积是一个比范数更基本的概念。它是一个函数，不仅允许我们定义欧几里得范数，还允许我们定义向量间的角度和正交性（垂直性）的概念。

*   **动机：** 内积是$\mathbb{R}^n$中熟悉的**点积**的推广，点积定义为：
    $$ \langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x}^T\mathbf{y} = \sum_{i=1}^n x_i y_i $$

*   **内积的正式定义：**
    实向量空间 $V$ 上的一个**内积**是一个函数 $\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}$，它接收两个向量并返回一个标量。该函数必须是一个**对称、正定的双线性映射**，满足以下公理：

    1.  **双线性：** 函数在每个参数中都是线性的。
    2.  **对称性：** 参数的顺序不影响结果。
        $$ \langle \mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{y}, \mathbf{x} \rangle $$
    3.  **正定性：** 向量与自身的内积是非负的，并且仅当向量为零向量时才为零。
        *   $\langle \mathbf{x}, \mathbf{x} \rangle \ge 0$
        *   $\langle \mathbf{x}, \mathbf{x} \rangle = 0 \iff \mathbf{x} = \mathbf{0}$

    配备了内积的向量空间称为**内积空间**。

#### 3. 桥梁：从内积到几何

内积是向量空间内欧几里得几何的基础。所有关键的几何概念都可以从中导出。

*   **诱导范数：**
    **每个内积都自然地定义（或诱导）一个范数**，由下式给出：
    $$ \|\mathbf{x}\| := \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle} $$
    标准的欧几里得范数 $\|\mathbf{x}\|_2$ 正是由标准点积诱导的范数。

*   **柯西-施瓦茨不等式：**
    它将两个向量的内积与它们的诱导范数联系起来，并为定义角度提供了基础。
    $$ |\langle \mathbf{x}, \mathbf{y} \rangle| \le \|\mathbf{x}\| \|\mathbf{y}\| $$

*   **由内积定义的几何概念：**
    *   **长度：** $\|\mathbf{x}\| = \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle}$
    *   **距离：** $d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\| = \sqrt{\langle \mathbf{x}-\mathbf{y}, \mathbf{x}-\mathbf{y} \rangle}$
    *   **角度：** 两个非零向量 $\mathbf{x}$ 和 $\mathbf{y}$ 之间的角度 $\theta$ 通过以下方式定义：
        $$ \cos\theta = \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{x}\| \|\mathbf{y}\|} $$
    *   **正交性 (垂直性):**
        *   **定义：** 如果两个向量 $\mathbf{x}$ 和 $\mathbf{y}$ 的内积为零，则它们是**正交的**。我们记为 $\mathbf{x} \perp \mathbf{y}$。
            $$ \mathbf{x} \perp \mathbf{y} \iff \langle \mathbf{x}, \mathbf{y} \rangle = 0 $$
        *   **勾股定理：** 如果 $\mathbf{x} \perp \mathbf{y}$，则熟悉的勾股定理成立：
            $$ \|\mathbf{x} + \mathbf{y}\|^2 = \|\mathbf{x}\|^2 + \|\mathbf{y}\|^2 $$

### 第二部分：向量空间上的几何结构

#### 1. 对称正定 (SPD) 矩阵与内积

在像$\mathbb{R}^n$这样的有限维向量空间中，内积的抽象概念可以通过一类特殊的矩阵——**对称正定 (SPD) 矩阵**——来具体表示和计算。

*   **对称正定矩阵的定义：**
    一个方阵 $A \in \mathbb{R}^{n \times n}$ 被称为**对称正定** (SPD)，如果它满足两个条件：

    1.  **对称性：** $A = A^T$
    2.  **正定性：** 对于每个非零向量 $x \in \mathbb{R}^n$，二次型 $x^T A x$ 严格为正。[[Notion/Class/Proof/Quadratic Form\|Quadratic Form]]
        $$ \mathbf{x}^T A \mathbf{x} > 0 \quad \text{对于所有 } \mathbf{x} \in \mathbb{R}^n, \mathbf{x} \ne \mathbf{0} $$

*   **中心定理：内积的矩阵表示**
    **定理：**
    $$ \langle \mathbf{x}, \mathbf{y} \rangle := \hat{\mathbf{x}}^T A \hat{\mathbf{y}} $$
    这个函数是一个有效的**内积**，当且仅当矩阵 $A$ 是**对称且正定的**。

    **解释：**
    *   该定理为有限维空间上所有可能的内积提供了通用配方。
    *   $\mathbb{R}^n$中的标准**点积**是该定理最简单的情况，其中矩阵 $A$ 是单位矩阵 $I$：
        $$ \langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x}^T I \mathbf{y} = \mathbf{x}^T \mathbf{y} $$
    *   *任何* SPD 矩阵 $A$ 都可以用来定义一个新的、完全有效的内积 $\langle \cdot, \cdot \rangle_A$。

*   **SPD 矩阵的性质**
    1.  **可逆性 (零空间平凡):** SPD 矩阵总是可逆的。它的零空间只包含零向量。
    2.  **正对角元素：** SPD 矩阵的所有对角元素都严格为正。

*   **回顾：内积 vs. 点积**
    *   **内积 $\langle \mathbf{x}, \mathbf{y} \rangle$：** **一般概念**。
    *   **点积 $\mathbf{x}^T \mathbf{y}$：** **具体示例**，是由单位矩阵定义的内积。
    *   **欧几里得范数 $\|\mathbf{x}\|_2$：** 由**点积**诱导的范数。

### 第三部分：角度、正交性与正交矩阵

#### 1. 角度与正交性

*   **定义角度：**
    $$ \cos\omega = \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{x}\| \|\mathbf{y}\|} $$
*   **正交性：**
    *   **定义：** 两个向量 $\mathbf{x}$ 和 $\mathbf{y}$ 是**正交的**，如果它们的内积为零。记为 $\mathbf{x} \perp \mathbf{y}$。
        $$ \mathbf{x} \perp \mathbf{y} \iff \langle \mathbf{x}, \mathbf{y} \rangle = 0 $$
    *   **几何意义：** 角度 $\omega = \pi/2$ (90°)。
*   **标准正交性 (Orthonormality):**
    *   两个向量 $\mathbf{x}$ 和 $\mathbf{y}$ 是**标准正交的**，如果它们既**正交**（$\langle \mathbf{x}, \mathbf{y} \rangle = 0$）又是**单位向量**（$\|\mathbf{x}\| = 1$, $\|\mathbf{y}\| = 1$）。

*   **关键点：正交性取决于内积**
    *   两个向量是否正交完全取决于所选的内积。

#### 2. 正交矩阵

*   **定义：**
    一个方阵 $A \in \mathbb{R}^{n \times n}$ 被称为**正交矩阵**，当且仅当其列构成一个**标准正交集**。

*   **等价性质：**
    1.  $A$ 的列是标准正交的。
    2.  $A^T A = I$
    3.  $A^{-1} = A^T$
    *   **核心思想：** 正交矩阵的逆矩阵就是其转置。

*   **几何性质：保持长度和角度**
    由正交矩阵 $A$ 定义的线性变换 $T(\mathbf{x}) = A\mathbf{x}$ 是一种**刚体变换**。
    1.  **保持长度：** $\|A\mathbf{x}\| = \|\mathbf{x}\|$
    2.  **保持内积和角度：** $\langle A\mathbf{x}, A\mathbf{y} \rangle = \langle \mathbf{x}, \mathbf{y} \rangle$

*   **实际意义与应用：**
    *   **几何模型：** 正交矩阵完美地模拟了空间中的**旋转**和**反射**。
    *   **数值稳定性：** 涉及正交矩阵的算法（如 QR 分解）通常非常数值稳定。
    *   **坐标系变换：** 从一个标准正交基到另一个的变换由一个正交矩阵描述。

## 第四部分：度量空间与距离的正式定义

度量 (metric) 形式化了任何集合元素之间“距离”的直观概念。

### 1. 度量函数 (Metric Function)

*   **正式定义：**
    集合 `V` 上的一个**度量**是一个函数 `d : V × V → ℝ`，它将一对元素 `(x, y)` 映射到一个实数 `d(x, y)`，并满足以下三个公理：
    1.  **正定性:**
        *   `d(x, y) ≥ 0`
        *   `d(x, y) = 0` 当且仅当 `x = y`
    2.  **对称性:**
        *   `d(x, y) = d(y, x)`
    3.  **三角不等式:**
        *   `d(x, z) ≤ d(x, y) + d(y, z)`

*   **度量空间 (Metric Space):**
    配备了度量 `d` 的集合 `V` 称为**度量空间**，记为 `(V, d)`。

### 2. 联系：从内积到度量

*   **定理：** 由内积诱导的距离函数 `d(x, y) = ||x - y||` 是一个度量。

### 3. 为什么度量的概念有用？

它允许我们在远超标准欧几里得几何的背景下测量“距离”。
*   **推广：** 适用于任何集合，包括：
    *   **字符串：** **编辑距离**是一个度量。
    *   **图：** 图中两个节点之间的最短路径距离是一个度量。
    *   **函数：** 我们可以定义度量来测量两个函数相距多“远”。

### 4. 空间层次总结

`内积空间` → `赋范空间` → `度量空间` → `拓扑空间`

## 第五部分：正交投影

正交投影是一种基本操作，用于在给定子空间中找到与给定向量“最接近”的向量。

### 1. 正交投影的概念

令 $U$ 是内积空间 $V$ 的一个子空间，$\mathbf{x} \in V$ 是一个向量。$\mathbf{x}$ 到子空间 $U$ 上的**正交投影** $\pi_U(\mathbf{x})$ 是 $U$ 中与 $\mathbf{x}$ “最接近”的唯一向量。

这个投影 $\mathbf{p} = \pi_U(\mathbf{x})$ 由两个基本性质定义：
1.  **隶属属性：** $\mathbf{p} \in U$
2.  **正交属性：** $(\mathbf{x} - \mathbf{p}) \perp U$

### 2. 推导投影公式（法方程）

**第1步：用基表示隶属属性**
令 $B$ 是一个以 $U$ 的基向量为列的矩阵。那么存在唯一的系数向量 $\boldsymbol{\lambda}$ 使得：
$$ \mathbf{p} = B\boldsymbol{\lambda} $$

**第2步：将正交属性表示为方程**
$(\mathbf{x} - \mathbf{p})$ 与 $U$ 的每个基向量的点积都必须为零，这可以紧凑地写成：
$$ B^T(\mathbf{x} - \mathbf{p}) = \mathbf{0} $$

**第3步：组合并求解 λ**
将第一个方程代入第二个方程：
$$ B^T(\mathbf{x} - B\boldsymbol{\lambda}) = \mathbf{0} $$
得到**法方程 (Normal Equation)**：
$$ (B^T B)\boldsymbol{\lambda} = B^T\mathbf{x} $$

### 3. 正交投影算法

1.  **找到基：** 找到子空间 $U$ 的一个基。
2.  **构成基矩阵 `B`**。
3.  **建立法方程：** 计算 `BᵀB` 和 `Bᵀx`。
4.  **求解 `λ`**。
5.  **计算投影 `p`：** 使用公式 `p = Bλ`。

### 4. 特殊情况：标准正交基

如果 $U$ 的基是**标准正交的**，那么 $B$ 的列是标准正交的。此时：
*   $B^T B = I$
*   法方程变为：$\boldsymbol{\lambda} = B^T\mathbf{x}$
*   投影公式为：$\mathbf{p} = B(B^T\mathbf{x}) = (BB^T)\mathbf{x}$
*   矩阵 $P = BB^T$ 称为**投影矩阵**。

***

# 第四讲：解析几何：标准正交基、正交补、函数内积、正交投影、旋转

## 第一部分：标准正交基与正交补

### 1. 标准正交基

*   **定义：** **标准正交基**是一种特殊的基，其中所有基向量相互正交，并且每个基向量都是单位向量。
    *   **形式化：** 对于基 $\{\mathbf{b}_1, \dots, \mathbf{b}_n\}$：
        *   **正交性：** $\langle \mathbf{b}_i, \mathbf{b}_j \rangle = 0$ 对于所有 $i \neq j$。
        *   **归一化：** $\|\mathbf{b}_i\|^2 = 1$ 对于所有 $i$。
*   **正交基：** 如果只满足正交性条件，则称为**正交基**。

### 2. 格拉姆-施密特过程：构造标准正交基

格拉姆-施密特过程是一个将任何线性无关向量集（一个基）转换为相同子空间的标准正交基的算法。

*   **目标：** 给定一个基 $\{\mathbf{a}_1, \dots, \mathbf{a}_n\}$，生成一个标准正交基 $\{\mathbf{q}_1, \dots, \mathbf{q}_n\}$。
*   **算法步骤：**
    1.  **初始化：** 归一化第一个向量 $\mathbf{a}_1$ 得到 $\mathbf{q}_1$。
        $$ \mathbf{q}_1 = \frac{\mathbf{a}_1}{\|\mathbf{a}_1\|} $$
    2.  **迭代与正交化：** 对于后续的每个向量 $\mathbf{a}_k$：
        a.  **投影并相减：** 从 $\mathbf{a}_k$ 中减去其在已找到的标准正交向量 $\{\mathbf{q}_1, \dots, \mathbf{q}_{k-1}\}$ 所张成的子空间上的投影，得到一个正交向量 $\mathbf{v}_k$。
            $$ \mathbf{v}_k = \mathbf{a}_k - \sum_{j=1}^{k-1} \langle \mathbf{a}_k, \mathbf{q}_j \rangle \mathbf{q}_j $$
        b.  **归一化：** 归一化 $\mathbf{v}_k$ 得到 $\mathbf{q}_k$。
            $$ \mathbf{q}_k = \frac{\mathbf{v}_k}{\|\mathbf{v}_k\|} $$



### 3. 正的概念从单个向量推广到整个子空间。

*   **定义 (正交补):** 令 $U$ 是向量空间 $V$ 的一个子空间。$U$ 的**正交补**，记为 $U^\perp$，是 $V$ 中所有与 $U$ 中*每个*向量都正交的向量的集合。
    $$ U^\perp = \{ \mathbf{v} \in V \mid \langle \mathbf{v}, \mathbf{u} \rangle = 0 \text{ 对于所有 } \mathbf{u} \in U \} $$

*   **空间分解 (直和):** 整个空间 $V$ 可以被唯一地分解为子空间 $U$ 与其正交补 $U^\perp$ 的**直和**。这意味着 $U$ 和 $U^\perp$ 的交集只有零向量，并且它们的维度之和等于 $V$ 的维度。
    $$ V = U \oplus U^\perp \quad \text{且} \quad \dim(U) + \dim(U^\perp) = \dim(V) $$

*   **向量分解 (正交分解):** 基于空间的直和分解，$V$ 中的**任何一个向量 $\mathbf{x}$** 都可以被**唯一地**分解为一个在 $U$ 中的分量和一个在 $U^\perp$ 中的分量之和。
    *   **概念层面:**
        $$ \mathbf{x} = \mathbf{x}_U + \mathbf{x}_{U^\perp} \quad (\text{其中 } \mathbf{x}_U \in U, \mathbf{x}_{U^\perp} \in U^\perp) $$
    *   **基层面表示:** 在计算上，这个分解通过找到 $\mathbf{x}$ 关于 $U$ 的基和 $U^\perp$ 的基的唯一坐标来实现。
        $$ \mathbf{x} = \sum_{m=1}^{M} \lambda_m \mathbf{b}_m + \sum_{j=1}^{D-M} \psi_j \mathbf{b}_j^\perp $$
        其中，$\{\mathbf{b}_1, \ldots, \mathbf{b}_M\}$ 是 $U$ 的一组基，$\{\mathbf{b}_1^\perp, \ldots, \mathbf{b}_{D-M}^\perp\}$ 是 $U^\perp$ 的一组基，而 $\lambda_m, \psi_j$ 是唯一的标量坐标。
    *   分量 $\mathbf{x}_U = \sum \lambda_m \mathbf{b}_m$ 正是 $\mathbf{x}$ 在子空间 $U$ 上的**正交投影**。

## 第二部分：函数内积、正交投影与旋转

### 1. 函数的内积

*   **定义：** 对于在区间 $[a, b]$ 上的连续函数空间，两个函数 $f(x)$ 和 $g(x)$ 之间的标准内积定义为积分：
    $$ \langle f, g \rangle := \int_{a}^{b} f(x)g(x) \,dx $$
*   **诱导的几何性质：**
    *   **长度 (范数):** $\|f\| = \sqrt{\int_{a}^{b} f(x)^2 \,dx}$
    *   **正交性：** 如果 $\langle f, g \rangle = 0$，则两个函数正交。
*   **意义：** 这种推广是**傅里叶级数**的数学基础。[[Notion/Class/Concept/Fourier Series\|Fourier Series]]

### 2. 正交投影

正交投影是解决最小二乘问题的几何基础。

*   **概念：** 向量 $\mathbf{x}$ 在子空间 $U$ 上的正交投影是 $U$ 中唯一的向量 $\mathbf{p}$，使得误差向量 $(\mathbf{x} - \mathbf{p})$ 与整个子空间 $U$ 正交。
*   **投影公式 (回顾):**
    1.  **构成基矩阵 $B$**。
    2.  **求解法方程** $(B^T B)\boldsymbol{\lambda} = B^T\mathbf{x}$ 得到 $\boldsymbol{\lambda}$。
    3.  **计算投影** $\mathbf{p} = B\boldsymbol{\lambda}$。
*   **[[Notion/Class/Proof/Projection Matrix\|Projection Matrix]]:** $P = B(B^T B)^{-1}B^T$。

### 3. 旋转

*   **定义：** 旋转是一种保持物体形状和大小的线性变换。
*   **矩阵性质：** 一个矩阵 $R$ 表示纯旋转，如果它满足两个条件：
    1.  **正交性：** $R^T R = I$ (保持长度和角度)。
    2.  **保持方向：** $\det(R) = 1$ (行列式为-1的表示反射)。
*   **2D 旋转：** 在2D平面中逆时针旋转角度 $\theta$ 的矩阵是：
    $$ R_\theta = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} $$
*   **群结构：** 所有 $n \times n$ 旋转矩阵的集合构成一个数学群，称为**特殊正交群**，记为 $SO(n)$。

## 第三部分：正交投影详解

投影是一类至关重要的线性变换，广泛应用于图形学、编码理论、统计学和机器学习中。

### 1. 正交投影的重要性与概念

*   **在机器学习中的动机：** 在机器学习中，我们经常处理难以分析或可视化的多维数据。一个关键的洞见是，大部分相关信息通常包含在一个维度低得多的子空间内。
*   **目标（维度约减）：** 通过将多维数据投影到一个精心选择的低维“特征空间”，我们可以简化问题、降低计算成本并提取有意义的模式。其目标是在执行此投影的同时，**最小化信息损失**。
*   **什么是正交投影？**
    *   它是一种线性变换，将一个向量从高维空间“投射”到低维子空间上。
    *   它之所以是“正交”的，是因为它通过**最小化**原始数据与其投影图像之间的**误差**（距离）来**最大可能地保留信息**。
    *   这一特性使其成为线性回归、分类和数据压缩的核心。

### 2. 投影的正式定义与性质

*   **代数定义（幂等性）：** 一个线性映射 $\pi: V \to U$ 如果应用两次与应用一次的效果相同，则被称为**投影**。这被称为**幂等性**。
    $$ \pi^2 = \pi \quad (\text{或 } \pi(\pi(\mathbf{x})) = \pi(\mathbf{x})) $$
    *   **矩阵形式：** 如果一个方阵 $P$ 满足 $P^2 = P$，那么它就是一个**投影矩阵**。
*   **几何定义（最近点）：** 向量 $\mathbf{x}$ 在子空间 $U$ 上的**正交投影** $\pi_U(\mathbf{x})$ 是 $U$ 中离 $\mathbf{x}$ **最近**的那个唯一的点。
    *   这个“最近点”条件等价于**正交性条件**：差向量 $(\mathbf{x} - \pi_U(\mathbf{x}))$ 必须与子空间 $U$ 中的每一个向量都正交。

### 3. 投影到一维子空间（直线）
![Image/Class/Mathematics-for-AI/5.png](/img/user/Image/Class/Mathematics-for-AI/5.png)
![Image/Class/Mathematics-for-AI/6.png](/img/user/Image/Class/Mathematics-for-AI/6.png)
我们从最简单的情况开始推导投影公式：将一个向量投影到一条直线上。除非另有说明，我们都假设使用标准的点积作为内积。

*   **设定：** 令 $U$ 为由非零基向量 $\mathbf{b}$ 张成的一维子空间（一条过原点的直线）。
*   **推导过程：** 投影 $\pi_U(\mathbf{x})$ 必须是 $\mathbf{b}$ 的一个标量倍，即 $\pi_U(\mathbf{x}) = \lambda\mathbf{b}$。通过正交条件 $\langle \mathbf{x} - \lambda\mathbf{b}, \mathbf{b} \rangle = 0$，我们可以解出坐标 $\lambda$。
*   **一维投影的最终公式：**
    *   **坐标：** $\lambda = \frac{\mathbf{b}^T\mathbf{x}}{\|\mathbf{b}\|^2}$
    *   **投影向量：** $\pi_U(\mathbf{x}) = \left( \frac{\mathbf{b}^T\mathbf{x}}{\|\mathbf{b}\|^2} \right) \mathbf{b}$
    *   **投影矩阵：** $P_\pi = \frac{\mathbf{b}\mathbf{b}^T}{\|\mathbf{b}\|^2}$

### 4. 投影到一般子空间
![Image/Class/Mathematics-for-AI/7.png](/img/user/Image/Class/Mathematics-for-AI/7.png)
用于一维投影的三步法可以推广到任何 m 维子空间 $U \subseteq \mathbb{R}^n$。

*   **设定：** 假设 $U$ 有一个基 $\{\mathbf{b}_1, \dots, \mathbf{b}_m\}$，构建基矩阵 $B = [\mathbf{b}_1, \dots, \mathbf{b}_m]$。
*   **推导过程：** 投影 $\pi_U(\mathbf{x}) = B\boldsymbol{\lambda}$。通过正交条件 $B^T(\mathbf{x} - B\boldsymbol{\lambda}) = \mathbf{0}$，我们得到**正规方程 (Normal Equation)**。
*   **最终公式：**
    *   **正规方程：** $B^T B \boldsymbol{\lambda} = B^T \mathbf{x}$
    *   **坐标：** $\boldsymbol{\lambda} = (B^T B)^{-1} B^T \mathbf{x}$
    *   **投影向量：** $\pi_U(\mathbf{x}) = B(B^T B)^{-1} B^T \mathbf{x}$
    *   **投影矩阵：** $P_\pi = B(B^T B)^{-1} B^T$

**拓展：子空间之间的投影[[Notion/Class/Concept/Projections between Subspaces\|Projections between Subspaces]]**
### 5. 核心应用 I：Gram-Schmidt正交化

Gram-Schmidt过程是构造一组标准正交基的经典算法，其核心思想就是**反复利用正交投影**。
![Image/Class/Mathematics-for-AI/8.png](/img/user/Image/Class/Mathematics-for-AI/8.png)
![Image/Class/Mathematics-for-AI/9.png](/img/user/Image/Class/Mathematics-for-AI/9.png)
![Image/Class/Mathematics-for-AI/10.png](/img/user/Image/Class/Mathematics-for-AI/10.png)
*   **目标：** 将一组线性无关的向量 $\{\mathbf{b}_1, \dots, \mathbf{b}_n\}$ 转换为一组正交向量 $\{\mathbf{u}_1, \dots, \mathbf{u}_n\}$，并且两组向量张成相同的子空间。
*   **迭代构造法：**
    1.  **第一步：** 选择第一个向量作为新基的起点。
        $$ \mathbf{u}_1 = \mathbf{b}_1 $$
    2.  **后续步骤 (k=2 to n)：** 对于每一个新的向量 $\mathbf{b}_k$，减去它在**已经构造好的正交子空间** $\text{span}\{\mathbf{u}_1, \dots, \mathbf{u}_{k-1}\}$ 上的投影。剩下的分量就必然与该子空间正交。
        $$ \mathbf{u}_k = \mathbf{b}_k - \pi_{\text{span}\{\mathbf{u}_1, \dots, \mathbf{u}_{k-1}\}}(\mathbf{b}_k) $$
        由于 $\{ \mathbf{u}_i \}$ 已经正交，投影可以更简单地写成：
        $$ \mathbf{u}_k = \mathbf{b}_k - \sum_{i=1}^{k-1} \frac{\langle \mathbf{b}_k, \mathbf{u}_i \rangle}{\langle \mathbf{u}_i, \mathbf{u}_i \rangle} \mathbf{u}_i $$
*   **获得标准正交基 (ONB)：** 在每一步得到正交向量 $\mathbf{u}_k$ 后，对其进行**标准化**即可。
    $$ \mathbf{e}_k = \frac{\mathbf{u}_k}{\|\mathbf{u}_k\|} $$

*   **示例：** 对 $\mathbb{R}^2$ 中的基 $\mathbf{b}_1 = [2, 0]^T, \mathbf{b}_2 = [1, 1]^T$ 进行正交化。
    1.  $\mathbf{u}_1 = \mathbf{b}_1 = \begin{bmatrix} 2 \\ 0 \end{bmatrix}$
    2.  $\mathbf{u}_2 = \mathbf{b}_2 - \frac{\langle \mathbf{b}_2, \mathbf{u}_1 \rangle}{\langle \mathbf{u}_1, \mathbf{u}_1 \rangle} \mathbf{u}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix} - \frac{2}{4} \begin{bmatrix} 2 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix} - \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$
    *   最终得到正交基 $\{ [2, 0]^T, [0, 1]^T \}$。

**拓展：[[Notion/Class/Concept/Cholesky分解\|Cholesky分解]]**
### 6. 核心应用 II：投影到仿射子空间

到目前为止，我们讨论的都是投影到过原点的子空间。现在我们将其推广到不过原点的**仿射子空间**（例如，不过原点的直线或平面）。

*   **定义：** 一个仿射子空间 $L$ 可以表示为 $L = \mathbf{x}_0 + U$，其中 $\mathbf{x}_0$ 是一个**支持点**（位移向量），$U$ 是一个与 $L$平行的、过原点的**方向子空间**。

*   **求解策略：平移-投影-移回**
    1.  **平移至原点：** 从待投影点 $\mathbf{x}$ 和仿射空间 $L$ 中都减去支持点 $\mathbf{x}_0$。这使得问题转化为将新向量 $(\mathbf{x} - \mathbf{x}_0)$ 投影到我们熟悉的方向子空间 $U$ 上。
    2.  **标准投影：** 计算向量 $(\mathbf{x} - \mathbf{x}_0)$ 在子空间 $U$ 上的正交投影 $\pi_U(\mathbf{x} - \mathbf{x}_0)$。
    3.  **移回原位：** 将投影结果平移回原来的位置，即加上支持点 $\mathbf{x}_0$。

*   **最终公式：**
    $$ \pi_L(\mathbf{x}) = \mathbf{x}_0 + \pi_U(\mathbf{x} - \mathbf{x}_0) $$

*   **数学证明：** 我们要找到点 $\mathbf{y}^* \in L$ 来最小化距离 $\|\mathbf{x} - \mathbf{y}\|^2$。
    *   因为 $\mathbf{y} \in L$，所以它可以写成 $\mathbf{y} = \mathbf{x}_0 + \mathbf{u}$，其中 $\mathbf{u} \in U$。
    *   最小化 $\|\mathbf{x} - (\mathbf{x}_0 + \mathbf{u})\|^2$ 就等价于最小化 $\|(\mathbf{x} - \mathbf{x}_0) - \mathbf{u}\|^2$。
    *   根据定义，使这个距离最小的 $\mathbf{u}^*$ 正是向量 $(\mathbf{x} - \mathbf{x}_0)$ 在子空间 $U$ 上的投影，即 $\mathbf{u}^* = \pi_U(\mathbf{x} - \mathbf{x}_0)$。
    *   因此，最近点 $\mathbf{y}^* = \mathbf{x}_0 + \mathbf{u}^* = \mathbf{x}_0 + \pi_U(\mathbf{x} - \mathbf{x}_0)$。
     ![Image/Class/Mathematics-for-AI/11.png](/img/user/Image/Class/Mathematics-for-AI/11.png)![Image/Class/Mathematics-for-AI/12.png](/img/user/Image/Class/Mathematics-for-AI/12.png)![Image/Class/Mathematics-for-AI/13.png](/img/user/Image/Class/Mathematics-for-AI/13.png)![Image/Class/Mathematics-for-AI/14.png](/img/user/Image/Class/Mathematics-for-AI/14.png)
*   **点到仿射子空间的距离：**
    $$ d(\mathbf{x}, L) = \|\mathbf{x} - \pi_L(\mathbf{x})\| = \|\mathbf{x} - (\mathbf{x}_0 + \pi_U(\mathbf{x} - \mathbf{x}_0))\| = \|(\mathbf{x} - \mathbf{x}_0) - \pi_U(\mathbf{x} - \mathbf{x}_0)\| = d(\mathbf{x}-\mathbf{x}_0, U) $$
    这表明，点到仿射空间的距离，等于平移后的点到其方向子空间的距离。
### 7. 核心应用 III：投影与最小二乘解

[[Notion/Class/Proof/Moore Penrose Pseudo inverse\|Moore Penrose Pseudo inverse]]
正交投影为求解无解的线性方程组 `Ax=b` 提供了一个强大的几何框架，这构成了**最小二乘法 (Least Squares Method)** 的基础。

*   **问题的根源：** 当方程 `Ax=b` 无解时，从几何上看，这意味着向量 `b` 不在矩阵 `A` 的列空间 `Col(A)` 内。
*   **解决思路：** 既然无法在 `Col(A)` 中找到一个点**等于** `b`，我们就退而求其次，寻找一个 `Col(A)` 中离 `b` **最近**的点。
*   **投影是答案：** 根据定义，这个“最近点”正是 `b` 在 `Col(A)` 上的**正交投影**，我们记为 $\hat{\mathbf{b}} = \pi_{Col(A)}(\mathbf{b})$。
*   **求解新方程：** 我们现在求解一个新的、**必有解**的方程：
    $$ A\hat{\mathbf{x}} = \hat{\mathbf{b}} $$
    这个方程的解 $\hat{\mathbf{x}}$ 就是原始问题的**最小二乘解**。
*   **为什么叫“最小二乘”？** 因为这个解 $\hat{\mathbf{x}}$ 能够使**误差向量**的长度平方 $\|\mathbf{b} - A\mathbf{x}\|^2$ 达到**最小**。这个长度的平方就是各项误差的**平方和 (sum of squares)**，因此得名。

## 第四部分：旋转详解 (Rotations)

旋转是继投影之后的另一类重要的线性变换，它在几何学、计算机图形学和机器人学中扮演着核心角色。

### 1. 旋转的基本概念

*   **与正交变换的关系：** 旋转是**正交变换**的一个特例。正交变换的核心特性是它们在变换过程中**保持向量的长度和向量间的夹角**不变。旋转完美地符合这一特性。
*   **定义：** 一个旋转是一个线性函数，它将一个平面（或空间）围绕一个固定的原点旋转一个特定的角度 $\theta$。
*   **方向约定：** 按照惯例，一个**正角度 $\theta > 0$** 对应于**逆时针 (counter-clockwise)** 旋转。
*   **核心性质：** 旋转只改变向量的方向，不改变其到原点的距离。

### 2. R² 中的旋转

#### 2.1 R² 旋转矩阵的推导：两种视角

##### 视角一：基变换 (The "Columns are Transformed Basis Vectors" View)

这是从线性变换本质出发的标准推导方法。

*   **目标：** 找到一个矩阵 $R(\theta)$，当它乘以一个向量 $\mathbf{x}$ 时，能够将该向量围绕原点逆时针旋转 $\theta$ 角。
*   **关键思想：** 一个线性变换矩阵的**列向量**，正是**标准基向量**经过该变换后的新坐标。
*   **推导步骤：**
    1.  **确定标准基向量：** 在 $\mathbb{R}^2$ 中，标准基是 $\mathbf{e}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ 和 $\mathbf{e}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$。
    2.  **旋转基向量：**
        *   将 $\mathbf{e}_1$ 旋转 $\theta$ 角，新坐标是 $\Phi(\mathbf{e}_1) = \begin{bmatrix} \cos\theta \\ \sin\theta \end{bmatrix}$。
        *   将 $\mathbf{e}_2$ 旋转 $\theta$ 角，新坐标是 $\Phi(\mathbf{e}_2) = \begin{bmatrix} -\sin\theta \\ \cos\theta \end{bmatrix}$。
    3.  **构建旋转矩阵：** 将变换后的基向量作为列，构成旋转矩阵。
        $$ R(\theta) = [\Phi(\mathbf{e}_1) \quad \Phi(\mathbf{e}_2)] = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix} $$

##### 视角二：极坐标与三角恒等式 (The "Direct Geometric" View)

这是一个更直接的几何证明，不依赖于基变换的思想。

1.  **表示向量：** 将任意向量用极坐标表示：$x = r\cos\phi, y = r\sin\phi$。
2.  **表示旋转：** 将该向量旋转角度 $\theta$，其角度变为 $\phi+\theta$。新坐标 $(x', y')$ 为：
    $$ x' = r\cos(\phi+\theta), \quad y' = r\sin(\phi+\theta) $$
3.  **应用和角公式：**
    *   $x' = r(\cos\phi\cos\theta - \sin\phi\sin\theta) = x\cos\theta - y\sin\theta$
    *   $y' = r(\sin\phi\cos\theta + \cos\phi\sin\theta) = x\sin\theta + y\cos\theta$
4.  **写成矩阵形式：**
    $$ \begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} $$

#### 2.2 R² 旋转的应用与注意事项

*   **坐标系依赖性：** 上述推导的 $R(\theta)$ **只直接适用于在标准笛卡尔坐标系下表示的向量**。
*   **在非标准坐标系中旋转向量：** 如果你的向量坐标 $\mathbf{x}_F$ 是在某个非标准基 $F = [\mathbf{f}_1, \mathbf{f}_2]$ 下给出的，正确的做法是：
    1.  **基变换到笛卡尔坐标：** $\mathbf{x}_{\text{cartesian}} = F\mathbf{x}_F$
    2.  **用标准矩阵旋转：** $\mathbf{x}'_{\text{rotated}} = R(\theta) \mathbf{x}_{\text{cartesian}}$
    3.  **（可选）变换回原基：** $\mathbf{y}_F = F^{-1}\mathbf{x}'_{\text{rotated}}$
    4.  $\Rightarrow M=FR(\theta)F^{-1}$
*   **旋转一个向量空间：** 要旋转整个向量空间，只需旋转其所有基向量即可。如果一个空间的基由矩阵 $B=[\mathbf{b}_1, \dots, \mathbf{b}_k]$ 的列给出，那么旋转后的新基矩阵为 $B' = [R(\theta)\mathbf{b}_1, \dots, R(\theta)\mathbf{b}_k] = R(\theta)B$。

### 3. R³ 中的旋转

三维空间中的旋转比二维更复杂，因为它必须围绕一个**旋转轴 (axis of rotation)** 进行。

*   **定义：** 在 $\mathbb{R}^3$ 中，旋转是围绕一条穿过原点的直线（轴）进行的。该轴上的所有点在旋转过程中保持不变。
*   **构建任意3D旋转矩阵：** 一个通用的3D旋转矩阵 $R$ 可以通过确定标准基 $\mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3$ 旋转后的新位置 $R\mathbf{e}_1, R\mathbf{e}_2, R\mathbf{e}_3$ 来构建，这些新向量必须保持标准正交性。
    $$ R = [R\mathbf{e}_1 \quad R\mathbf{e}_2 \quad R\mathbf{e}_3] $$

#### 3.1 3D旋转的方向约定：右手定则

为了定义“逆时针”旋转，我们使用**右手定则 (Right-Hand Rule)**：
*   **规则：** 将你的右手大拇指指向旋转轴的**正方向**。你其余四指弯曲的方向，就是**正角度（逆时针）** 的旋转方向。

#### 3.2 沿坐标轴的基本旋转

任何复杂的3D旋转都可以分解为沿三个主坐标轴（x, y, z）的一系列基本旋转。

1.  **绕 x 轴 ($e_1$) 旋转 $R_x(\theta)$**
    *   **描述：** x 坐标保持不变，旋转发生在 yz 平面。
    *   **矩阵：**
        $$ R_x(\theta) = \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos\theta & -\sin\theta \\ 0 & \sin\theta & \cos\theta \end{bmatrix} $$

2.  **绕 y 轴 ($e_2$) 旋转 $R_y(\theta)$**
    *   **描述：** y 坐标保持不变，旋转发生在 zx 平面。
    *   **矩阵：**
        $$ R_y(\theta) = \begin{bmatrix} \cos\theta & 0 & \sin\theta \\ 0 & 1 & 0 \\ -\sin\theta & 0 & \cos\theta \end{bmatrix} $$

3.  **绕 z 轴 ($e_3$) 旋转 $R_z(\theta)$**
    *   **描述：** z 坐标保持不变，旋转发生在 xy 平面。
    *   **矩阵：**
        $$ R_z(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta & 0 \\ \sin\theta & \cos\theta & 0 \\ 0 & 0 & 1 \end{bmatrix} $$

#### 3.3 3D基本旋转矩阵的三角证明

*   **绕 x 轴:** 在 yz 平面中，令 $y = r\cos\phi, z = r\sin\phi$。旋转 $\theta$ 角后，新坐标为 $y' = r\cos(\phi+\theta)$ 和 $z' = r\sin(\phi+\theta)$。通过和角公式展开，并保持 $x'=x$，即可推导出 $R_x(\theta)$ 矩阵。
*   **绕 z 轴:** 在 xy 平面中，令 $x = r\cos\phi, y = r\sin\phi$。旋转 $\theta$ 角后，新坐标为 $x' = r\cos(\phi+\theta)$ 和 $y' = r\sin(\phi+\theta)$。展开后即可推导出 $R_z(\theta)$ 矩阵。
*   **绕 y 轴 (特殊情况):**
    *   根据右手定则，大拇指指向+y，四指弯曲方向是从 **z 轴到 x 轴**。
    *   因此，从 `x` 到 `z` 的旋转被视为**负角度**。或者说，要实现一个正角度的旋转，新角度应该是 `ϕ-θ` 而不是 `ϕ+θ`。
    *   $x' = r\cos(\phi-\theta) = x\cos\theta + z\sin\theta$
    *   $z' = r\sin(\phi-\theta) = z\cos\theta - x\sin\theta$
    *   这解释了为什么 $R_y(\theta)$ 矩阵中 `sinθ` 的符号与其他两个矩阵不同。

#### 3.4 3D序贯旋转 (Sequential Rotations)

*   **顺序的重要性：** 在三维及更高维度中，旋转操作**不满足交换律 (not commutative)**。即 `RxRy ≠ RyRx`。因此，旋转的顺序至关重要。
*   **矩阵乘法顺序：** 旋转操作是**从右向左**依次施加的。如果要先绕x轴，再绕y轴，最后绕z轴旋转向量 `x`，组合旋转矩阵的计算方式是：
    $$ \mathbf{x}_{\text{rotated}} = (R_z R_y R_x) \mathbf{x} $$

### 4. 高维空间 (Rⁿ) 中的旋转：吉文斯旋转 (Givens Rotation)

*   **核心思想：** 在 n 维空间中的任何旋转，都可以被看作是在一个**二维平面**内的旋转，同时保持其余 $n-2$ 个维度不变。
*   **定义：吉文斯旋转**
    *   一个在 `(i, j)` 平面中进行旋转的 n 维旋转矩阵，记为 $R_{ij}(θ)$。
    * 它的结构是一个被修改过的单位矩阵。具体来说，$R_ij(θ)$ 是一个 `n x n` 矩阵，其形式如下：
        $$ R_{ij}(\theta) :=
        \begin{bmatrix}
        I_{i-1} & 0 & 0 & 0 & 0 \\
        0 & \cos\theta & 0 & -\sin\theta & 0 \\
        0 & 0 & I_{j-i-1} & 0 & 0 \\
        0 & \sin\theta & 0 & \cos\theta & 0 \\
        0 & 0 & 0 & 0 & I_{n-j}
        \end{bmatrix}
        $$
    *   这个矩阵在绝大多数位置上是单位矩阵，只在第 `i` 行 `i` 列、`i` 行 `j` 列、`j` 行 `i` 列、`j` 行 `j` 列这四个位置上，嵌入了一个标准的二维旋转矩阵：
        *   $r_{ii} = \cosθ$
        *   $r_{ij} = -\sinθ$
        *   $r_{ji} = \sinθ$
        *   $r_{jj} = \cosθ$
*   **实践中的应用：**
    *   在实际算法中（如QR分解），我们通常不是预先设定 `θ`，而是根据需求**反向计算**出 `cosθ` 和 `sinθ` 的值，以达到特定目的，例如将一个向量在特定维度上的分量**清零**。

### 5. 旋转的通用性质

*   **正交性：** 旋转矩阵 `R` 都是正交矩阵，满足 `RᵀR = I`，即 `Rᵀ = R⁻¹`。
*   **保距性：** `||Rx - Ry|| = ||x - y||`，旋转不改变点与点之间的距离。
*   **保角性：** 旋转不改变向量之间的夹角。
*   **交换律：**
    *   在 **R²** 中，旋转**满足**交换律：`R(φ)R(θ) = R(θ)R(φ)`。
    *   在 **R³ 及更高维度**中，旋转**不满足**交换律。

# 第五讲：矩阵分解 (Matrix Decompositions)

## 第一部分：行列式与迹 (Determinant and Trace)

在深入研究复杂的矩阵分解之前，我们首先需要掌握两个描述方阵特性的基本标量：**行列式**和**迹**。

## 1. 行列式 (Determinant)

行列式是线性代数中的一个核心概念，它将一个方阵映射到一个唯一的实数，这个实数蕴含了关于该矩阵和其所代表的线性变换的重要信息。

*   **定义与记号**:
    *   行列式**只为方阵** ($A \in \mathbb{R}^{n \times n}$) 定义。
    *   记号为 $\det(A)$ 或 $|A|$（注意不要与绝对值混淆）。

### 1.1 行列式、可逆性与秩

行列式最直接、最重要的应用就是判断一个方阵是否可逆，以及它是否是满秩的。这三者是等价的。

*   **核心定理**: 对于一个方阵 $A \in \mathbb{R}^{n \times n}$，以下三个命题是等价的：
    1.  $A$ 是**可逆的 (invertible)**。
    2.  $A$ 的**行列式不为零 ($\det(A) \neq 0$)**。
    3.  $A$ 是**满秩的 (full rank)**，即 $\text{rk}(A) = n$。

*   **证明思路 `det(A) ≠ 0 ⇔ rk(A) = n`**:
    *   **(⇒)** 如果 $\det(A) \neq 0$，根据之前的定理，我们知道 $A$ 是可逆的。而一个 $n \times n$ 矩阵可逆的充要条件就是它是满秩的，因此 $\text{rk}(A) = n$。
    *   **(⇐)** 如果 $\text{rk}(A) = n$，这意味着 $A$ 的行（或列）向量线性无关。通过**高斯消元法**（一系列不改变行列式非零性质的行操作），我们可以将 $A$ 化为一个**上三角矩阵 `U`**。因为 $A$ 是满秩的，所以 `U` 的对角线上有 `n` 个主元 (pivots)，**所有对角元都非零**。而三角矩阵的行列式是其对角元的乘积，所以 $\det(U) \neq 0$。由于行变换不改变行列式的非零性，因此 $\det(A)$ 也必然不为零。

### 1.2 行列式的几何意义：体积与方向

行列式最深刻的几何意义是它代表了由矩阵的列向量（或行向量）所张成的**平行多面体 (parallelepiped)** 的**有向体积 (signed volume)**。

*   **体积缩放因子**: 行列式的绝对值 $|\det(A)|$ 代表了由矩阵 $A$ 所代表的线性变换对“体积”的缩放比例。一个单位立方体（由标准基向量张成，体积为1）经过 $A$ 变换后，会变成一个由 $A$ 的列向量张成的平行多面体，其体积恰好是 $|\det(A)|$。
    *   **二维**: $|\det(A)|$ 是平行四边形的**面积**。
    *   **三维**: $|\det(A)|$ 是平行六面体的**体积**。
    *   如果 $\det(A)=0$，意味着空间被“压扁”到更低的维度（例如，一个三维物体被拍扁成一个平面或一条线），体积为零，此时矩阵不可逆。

*   **方向性 (Orientation)**: 行列式的符号 $\text{sign}(\det(A))$ 描述了变换是否**保持或翻转**了空间的方向。
    *   **大小**: $|\det(A)|$ 描述了体积的**缩放比例**。
    *   **符号**: $\text{sign}(\det(A))$ 描述了变换是否**保持或翻转**了空间的方向。
        *   **正行列式 (`det(A) > 0`)**: 保持方向。变换后的基向量组与原始标准基向量组具有相同的“手性”（例如，在 $\mathbb{R}^3$ 中都是右手系）。在 $\mathbb{R}^2$ 中，从第一个列向量到第二个列向量是逆时针旋转。
        *   **负行列式 (`det(A) < 0`)**: 翻转方向。变换后的基向量组的手性与原始相反（例如，从右手系变成了左手系）。在 $\mathbb{R}^2$ 中，从第一个列向量到第二个列向量是顺时针旋转，如同照镜子一样。
        *   **列交换与符号**: 每交换矩阵的两列（或两行），行列式的符号就会反转一次。奇数次交换导致方向翻转，偶数次交换保持方向不变。

### 1.3 行列式的计算

计算行列式有多种方法，适用于不同类型和阶数的矩阵。

---

*   **低阶矩阵公式**:
    *   **1x1 矩阵**:
        $$ \det([a]) = a $$
    *   **2x2 矩阵**:
        $$ \det\begin{pmatrix} a & b \\ c & d \end{pmatrix} = ad-bc $$
    *   **3x3 矩阵**: 可以使用 **萨吕法则 (Sarrus' rule)** 进行计算。

*   **三角矩阵**:
    对于任何**上三角**或**下三角**矩阵，其行列式等于**对角线上所有元素的乘积**。
    $$ \det(A) = \prod_{i=1}^{n} a_{ii} $$

*   **拉普拉斯展开 (Laplace Expansion)**:
    这是一个递归计算高阶行列式的通用方法，可将 $n \times n$ 矩阵的行列式降解为多个 $(n-1) \times (n-1)$ 子矩阵的行列式计算。
    *   **沿第 j 列展开**:
        $$ \det(A) = \sum_{k=1}^{n} (-1)^{k+j} a_{kj} \det(A_{k,j}) $$
    其中，$A_{k,j}$ 是从矩阵 $A$ 中移除第 $k$ 行和第 $j$ 列后得到的子矩阵。

*   **分块矩阵 (Block Matrix)**:
    当一个矩阵可以被划分为几个子矩阵块时，可以利用其结构简化行列式的计算。假设我们有一个分块矩阵 $M$：
    $$ M = \begin{bmatrix} A & B \\ C & D \end{bmatrix} $$
    其中 $A \in \mathbb{R}^{m \times m}$ 和 $D \in \mathbb{R}^{n \times n}$ 都是方阵。

    **计算公式**:
    1.  **如果 $A$ 是可逆的**:
        $$ \det(M) = \det(A) \cdot \det(D - CA^{-1}B) $$
        这里的 $D - CA^{-1}B$ 称为 $A$ 在 $M$ 中的 **舒尔补 (Schur Complement)**。

    2.  **如果 $D$ 是可逆的**:
        $$ \det(M) = \det(D) \cdot \det(A - BD^{-1}C) $$

    3.  **特殊情况 (分块三角矩阵)**:
        如果 $B=0$ 或 $C=0$，矩阵呈现分块三角形式：
        $$ M_1 = \begin{bmatrix} A & B \\ \mathbf{0} & D \end{bmatrix} \quad \text{或} \quad M_2 = \begin{bmatrix} A & \mathbf{0} \\ C & D \end{bmatrix} $$
        此时，行列式为左上角和右下角两个对角块行列式的乘积：
        $$ \det(M_1) = \det(M_2) = \det(A) \cdot \det(D) $$

### 1.4 行列式的重要性质

设 $A, B \in \mathbb{R}^{n \times n}$，$\lambda \in \mathbb{R}$。
*   **乘法性质**: $\det(AB) = \det(A)\det(B)$
*   **转置不变性**: $\det(A) = \det(A^T)$
*   **逆矩阵**: 如果 $A$ 可逆，$\det(A^{-1}) = \frac{1}{\det(A)}$
*   **相似矩阵**: 如果 $A$ 和 $B$ 相似（即 $B = S^{-1}AS$），那么 $\det(B) = \det(A)$。
*   **标量乘法**: $\det(\lambda A) = \lambda^n \det(A)$
*   **行/列操作**:
    *   交换两行或两列，行列式变号。
    *   将某一行或某列的倍数加到另一行或另一列上，**行列式不变**。

### 1.5 行列式的理论与实践作用

*   **历史角色**: 历史上，行列式是分析矩阵可逆性和求解线性方程组（如克拉默法则）的核心手动工具。
*   **现代计算**: 在现代数值计算中，对于大规模矩阵，直接计算行列式（特别是通过拉普拉斯展开）的计算成本极高。**高斯消元法**成为了更受青睐的工具。它不仅可以求解方程组和求逆，还可以高效地计算行列式（通过将矩阵化为三角形式）。
*   **理论重要性**: 尽管在数值计算中的直接应用减少，行列式在**理论层面**仍然至关重要。它对于定义**特征多项式**、理解**特征值**和**特征向量**是不可或缺的，这些概念是现代科学与工程的基石。

## 2. 迹 (Trace)

迹是方阵的另一个重要标量，定义比行列式简单得多。

*   **定义**: 方阵 $A \in \mathbb{R}^{n \times n}$ 的迹是其**主对角线上所有元素的和**。
    $$ \text{tr}(A) := \sum_{i=1}^{n} a_{ii} $$

*   **重要性**:
    *   迹等于矩阵所有**特征值之和**，在谱方法和系统稳定性分析中非常有用。
    *   在机器学习和量子力学中，迹用于计算期望值和定义密度矩阵。

### 2.1 迹的性质

设 $A, B$ 为方阵，$\alpha$ 为标量。
*   **线性性**: $\text{tr}(A+B) = \text{tr}(A) + \text{tr}(B)$ 和 $\text{tr}(\alpha A) = \alpha \text{tr}(A)$。
*   **单位矩阵**: $\text{tr}(I_n) = n$
*   **循环不变性 (Cyclic Property)**:
    *   对于 $A \in \mathbb{R}^{n \times k}, B \in \mathbb{R}^{k \times n}$，有 $\text{tr}(AB) = \text{tr}(BA)$。
    *   **推论**: $\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)$。
![15.png](/img/user/Image/Class/Mathematics-for-AI/15.png)
*   **基无关性 (Basis-Independence)**:
    *   迹对于**相似变换**是不变的。如果 $B = S^{-1}AS$，那么 $\text{tr}(B) = \text{tr}(A)$。
    *   这意味着一个线性变换的迹是一个内在属性，不随坐标基的选择而改变。

## 3. 特征多项式 (Characteristic Polynomial)

行列式和迹共同构成了定义**特征多项式**的基础，这是计算矩阵特征值的关键工具。

*   **定义**: 对于 $A \in \mathbb{R}^{n \times n}$，其特征多项式 $p_A(\lambda)$ 是：
    $$ p_A(\lambda) := \det(A - \lambda I) $$
    这是一个关于 $\lambda$ 的 n 次多项式。
*   **与行列式和迹的关系**:
    *   多项式的常数项是 $p_A(0) = \det(A)$。
    *   $\lambda^{n-1}$ 项的系数与迹相关：$(-1)^{n-1}\text{tr}(A)$。
*   **核心应用**:
    *   **特征值**: 特征多项式的根（即方程 $p_A(\lambda) = 0$ 的解）就是矩阵 $A$ 的**特征值**。

## 第二部分：特征值与特征向量 (Eigenvalues and Eigenvectors)

特征值和特征向量（简称“特征分析”）是线性代数中威力最强大的工具之一。它提供了一种全新的视角来理解和刻画一个方阵及其所代表的线性变换，揭示了变换最本质、最核心的特性。

## 1. 特征值与特征向量的定义与几何意义

### 1.1 定义

*   **动机：** 对于一个给定的线性变换（由矩阵 `A` 代表），我们特别关心那些**方向保持不变**的特殊向量。当变换作用于这些向量时，它们只会被拉伸或压缩，而不会发生方向偏转。
*   **特征值方程 (Eigenvalue Equation)**:
    对于一个方阵 $A \in \mathbb{R}^{n \times n}$，如果存在一个**标量 $\lambda$** 和一个**非零向量 $\mathbf{x} \in \mathbb{R}^n \setminus \{0\}$**，满足以下方程：
    $$ A\mathbf{x} = \lambda\mathbf{x} $$
    那么：
    *   $\lambda$ 被称为矩阵 $A$ 的一个**特征值 (Eigenvalue)**。
    *   $\mathbf{x}$ 被称为对应于特征值 $\lambda$ 的一个**特征向量 (Eigenvector)**。

### 1.2 几何意义：变换下的“不变方向”

*   **一般向量 vs. 特征向量**:
    *   当一个线性变换 $A$ 作用于一个**普通向量**时，该向量的大小和方向通常都会改变。
    *   当 $A$ 作用于一个**特征向量 $\mathbf{x}$** 时，它的**方向保持不变**（或完全反向），仅仅是在这个方向上被**拉伸或压缩**了 $\lambda$ 倍。
        *   如果 $\lambda > 1$，向量被拉长。
        *   如果 $0 < \lambda < 1$，向量被缩短。
        *   如果 $\lambda < 0$，向量的方向被完全翻转（180°），并根据$|\lambda|$进行缩放。
        *   如果 $\lambda = 1$，向量保持不变（是变换的“不动点”）。
        *   如果 $\lambda = 0$，向量被“压扁”到零向量。
*   **示例**:
    对于矩阵 $A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$：
    *   **特征向量**: 取 $\mathbf{x}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$。计算 $A\mathbf{x}_1 = \begin{pmatrix} 3 \\ 3 \end{pmatrix} = 3 \begin{pmatrix} 1 \\ 1 \end{pmatrix} = 3\mathbf{x}_1$。向量 $\mathbf{x}_1$ 的方向不变，只是被拉长了3倍。因此，$\lambda=3$ 是一个特征值，$\mathbf{x}_1$ 是其对应的特征向量。
    *   **非特征向量**: 取 $\mathbf{y} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$。计算 $A\mathbf{y} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$。结果向量的方向发生了偏转，所以 $\mathbf{y}$ 不是一个特征向量。

### 1.3 特征向量的非唯一性

*   如果 $\mathbf{x}$ 是一个特征向量，那么任何与它**共线 (collinear)** 的非零向量 $c\mathbf{x}$ (其中 $c \neq 0$) 也是对应于同一个特征值 $\lambda$ 的特征向量。
*   **证明**:
    $$ A(c\mathbf{x}) = c(A\mathbf{x}) = c(\lambda\mathbf{x}) = \lambda(c\mathbf{x}) $$
*   **结论**: 特征向量定义的不是一个单一的向量，而是一个**方向**，即一条穿过原点的直线。这条直线上的所有非零向量都是具有相同特征值的特征向量。

---

## 2. 特征值的计算

### 2.1 计算原理与特征方程

如何系统地找到一个矩阵的特征值？这需要我们将特征值方程转化为一个我们熟悉的问题。

*   **核心思想**: 将特征值方程 $A\mathbf{x} = \lambda\mathbf{x}$ 变形。
    $$ A\mathbf{x} - \lambda\mathbf{x} = \mathbf{0} $$
    $$ A\mathbf{x} - \lambda I \mathbf{x} = \mathbf{0} $$
    $$ (A - \lambda I)\mathbf{x} = \mathbf{0} $$
*   这个方程 $(A - \lambda I)\mathbf{x} = \mathbf{0}$ 是一个标准的**齐次线性方程组**。我们寻找的是它的**非平凡解 (nontrivial solution)**，因为根据定义，特征向量 $\mathbf{x}$ 不能是零向量。

*   **特征值的等价刻画定理**: 对于 $\lambda \in \mathbb{R}$ 和 $A \in \mathbb{R}^{n \times n}$，以下四个命题是完全等价的：
    1.  $\lambda$ 是 $A$ 的一个特征值。
    2.  方程 $(A - \lambda I)\mathbf{x} = \mathbf{0}$ 存在一个非零解 $\mathbf{x} \neq \mathbf{0}$。
    3.  矩阵 $(A - \lambda I)$ 是**奇异的 (singular)**，即它是不可逆的，或者说它的秩小于n: $\text{rk}(A - \lambda I) < n$。
    4.  矩阵 $(A - \lambda I)$ 的**行列式为零**: $\det(A - \lambda I) = 0$。

*   **计算方法**: 第四个命题为我们提供了计算特征值的具体方法。$\det(A - \lambda I)$ 是一个关于 $\lambda$ 的多项式，被称为**特征多项式** $p_A(\lambda)$。因此：
    > **一个标量 $\lambda$ 是矩阵 $A$ 的特征值，当且仅当它是特征多项式 $p_A(\lambda) = 0$ 的一个根。**

### 2.2 计算步骤示例

给定矩阵 $A = \begin{pmatrix} 4 & 2 \\ 1 & 3 \end{pmatrix}$，我们来计算其特征值和特征向量。

**第一步：求解特征多项式**
我们计算特征多项式 $p_A(\lambda)$ 并令其为零：
$$
\begin{aligned}
p_A(\lambda) = \det(A - \lambda I) &= \det\begin{pmatrix} 4-\lambda & 2 \\ 1 & 3-\lambda \end{pmatrix} \\
&= (4-\lambda)(3-\lambda) - 2 \cdot 1 \\
&= 12 - 7\lambda + \lambda^2 - 2 \\
&= \lambda^2 - 7\lambda + 10 \\
&= (\lambda - 2)(\lambda - 5)
\end{aligned}
$$

**第二步：计算特征值**
令 $p_A(\lambda) = 0$，我们得到特征方程的根，即矩阵 $A$ 的特征值：
$$ \lambda_1 = 5, \quad \lambda_2 = 2 $$

**第三步：计算特征向量与特征空间**
对于每一个特征值，我们求解方程 $(A - \lambda I)\boldsymbol{x} = \mathbf{0}$ 来找到对应的特征向量。

*   **对于 $\lambda_1 = 5$**:
    $$ (A - 5I)\boldsymbol{x} = \begin{pmatrix} -1 & 2 \\ 1 & -2 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \mathbf{0} \implies E_5 = \text{span}\left\{\begin{pmatrix} 2 \\ 1 \end{pmatrix}\right\} $$

*   **对于 $\lambda_2 = 2$**:
    $$ (A - 2I)\boldsymbol{x} = \begin{pmatrix} 2 & 2 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \mathbf{0} \implies E_2 = \text{span}\left\{\begin{pmatrix} 1 \\ -1 \end{pmatrix}\right\} $$

---

## 3. 特征分析的核心概念

### 3.1 特征空间 (Eigenspace)

*   对于一个特定的特征值 $\lambda$，所有与之对应的特征向量，再加上**零向量**，共同构成了一个向量子空间。这个子空间被称为对应于 $\lambda$ 的**特征空间**，记为 $E_\lambda$。
*   $E_\lambda = \text{Ker}(A - \lambda I)$。

### 3.2 代数重数与几何重数 (Algebraic and Geometric Multiplicity)

*   **代数重数 (Algebraic Multiplicity)**: 一个特征值 $\lambda$ 作为特征多项式的根**出现的次数**。
*   **几何重数 (Geometric Multiplicity)**: 一个特征值 $\lambda$ 对应**特征空间 $E_\lambda$ 的维度**。

**重要关系**: 对于任何一个特征值 $\lambda$，$1 \le \text{几何重数} \le \text{代数重数}$。

### 3.3 特征向量的线性无关性

**定理 1**: 如果一个 $n \times n$ 矩阵 $A$ 有 $n$ 个**互不相同**的特征值 $\lambda_1, \dots, \lambda_n$，那么它们对应的特征向量 $\boldsymbol{x}_1, \dots, \boldsymbol{x}_n$ 是**线性无关**的。

**更一般的定理 (Theorem 2)**: 如果一个 $n \times n$ 矩阵 $A$ 有 $m$ 个不同的特征值 $\lambda_1, \dots, \lambda_m$，那么从每个特征空间中任取一个特征向量 $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m$，这组向量是**线性无关**的。

*   **推论**: 如果矩阵有 $m$ 个不同的特征值，我们保证可以找到至少 $m$ 个线性无关的特征向量。

### 3.4 谱 (Spectrum)

*   一个矩阵 $A$ 的所有特征值的**集合**被称为 $A$ 的**谱 (spectrum)**，记为 $\sigma(A)$。

---

## 4. 特征分析的几何直观

特征分析不仅是代数计算，它还提供了强大的几何直观，帮助我们理解线性变换的本质。

### 4.1 变换的分解：沿特征向量方向的拉伸

假设一个 $n \times n$ 矩阵 $A$ 有 $n$ 个线性无关的特征向量 $\boldsymbol{v}_1, \dots, \boldsymbol{v}_n$（它们构成 $\mathbb{R}^n$ 的一个基）。

*   **分解**: 空间中任何一个向量 $\boldsymbol{x}$都可以唯一地表示为这些特征向量的线性组合：
    $$ \boldsymbol{x} = c_1\boldsymbol{v}_1 + c_2\boldsymbol{v}_2 + \dots + c_n\boldsymbol{v}_n $$
*   **变换**: 当矩阵 $A$ 作用于 $\boldsymbol{x}$ 时，变换会独立地作用在每一个特征向量分量上：
    $$ A\boldsymbol{x} = A(c_1\boldsymbol{v}_1 + \dots + c_n\boldsymbol{v}_n) = c_1(A\boldsymbol{v}_1) + \dots + c_n(A\boldsymbol{v}_n) = c_1\lambda_1\boldsymbol{v}_1 + \dots + c_n\lambda_n\boldsymbol{v}_n $$
*   **直观解释**: 线性变换 $A$ 的作用，可以被看作是将向量 $\boldsymbol{x}$ 沿着它的各个特征向量方向进行分解，然后在每个**特征方向**上独立地进行**拉伸或压缩**（缩放因子即为对应的特征值 $\lambda_i$）。

### 4.2 示例1：压缩与拉伸 (保积变换)

考虑矩阵 $A_1 = \begin{pmatrix} 1/2 & 0 \\ 0 & 2 \end{pmatrix}$。
*   **特征值与特征向量**:
    *   $\lambda_1 = 1/2$ 对应特征向量 $\boldsymbol{v}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ (水平方向)。
    *   $\lambda_2 = 2$ 对应特征向量 $\boldsymbol{v}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$ (垂直方向)。
*   **几何效应**:
    *   变换将所有向量在**水平方向**上**压缩**为原来的一半。
    *   变换将所有向量在**垂直方向**上**拉伸**为原来的两倍。
*   **行列式**: $\det(A_1) = (1/2) \times 2 = 1$。这意味着该变换是**保面积**的：任何图形经过此变换，其面积保持不变。

### 4.3 示例2：投影 (降维变换)

考虑矩阵 $A_2 = \begin{pmatrix} 1 & -1 \\ -1 & 1 \end{pmatrix}$。
*   **特征值与特征向量**:
    *   $\lambda_1 = 0$ 对应特征向量 $\boldsymbol{v}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$。
    *   $\lambda_2 = 2$ 对应特征向量 $\boldsymbol{v}_2 = \begin{pmatrix} -1 \\ 1 \end{pmatrix}$。
*   **几何效应**:
    *   任何在 $\boldsymbol{v}_1$ 方向上的分量，其缩放因子为0，意味着被**“压扁”或“消除”**。
    *   任何在 $\boldsymbol{v}_2$ 方向上的分量，被**拉伸**为原来的两倍。
    *   最终效果是将整个二维平面**投影**到直线 $\text{span}\{\boldsymbol{v}_2\}$ 上。
*   **行列式**: $\det(A_2) = 1 - 1 = 0$。这意味着该变换将二维图形的面积**压缩为零**（降维到一条线上）。

---

## 5. 亏损矩阵与广义特征向量 (Defective Matrices)

### 5.1 定义

*   一个 $n \times n$ 的方阵 $A$ 如果它**没有 $n$ 个线性无关的特征向量**，则被称为**亏损矩阵 (Defective Matrix)**。
*   **备注**: 亏损的根本原因是至少存在一个特征值，其**几何重数 < 代数重数**。因此，亏损矩阵必然有重复的特征值。

### 5.2 广义特征向量 (课程范围外)

当一个矩阵是亏损的，我们无法找到一组由特征向量构成的基。为了处理这种情况，需要引入**广义特征向量 (Generalized Eigenvectors)** 的概念来构建一个完整的基（称为**若尔当基 Jordan Basis**）。

*   **若尔当链 (Jordan Chain)**: 对于一个代数重数为 $m$ 但几何重数为 $k < m$ 的特征值 $\lambda$，我们可以构建一个链条：
    *   $\boldsymbol{v}_1$ 是一个普通特征向量: $(A - \lambda I)\boldsymbol{v}_1 = \mathbf{0}$
    *   $\boldsymbol{v}_2$ 是一个广义特征向量: $(A - \lambda I)\boldsymbol{v}_2 = \boldsymbol{v}_1$
    *   ...
    *   $\boldsymbol{v}_j$ 是一个广义特征向量: $(A - \lambda I)\boldsymbol{v}_j = \boldsymbol{v}_{j-1}$
*   **变换作用**: 矩阵 $A$ 在这个链条上的作用是**缩放**与**剪切 (shearing)** 的混合：$A\boldsymbol{v}_k = \lambda\boldsymbol{v}_k + \boldsymbol{v}_{k-1}$。

---

## 6. 总结：线性无关特征向量的数量

对于一个 $n \times n$ 的矩阵 $A$：

1.  **如果 $A$ 有 $n$ 个不同的特征值**：
    *   那么它一定有 $n$ 个线性无关的特征向量。
    *   此时 $A$ 不是亏损的。

2.  **如果 $A$ 有重复的特征值** (即不同的特征值数量 $m < n$):
    *   我们保证**至少有 $m$ 个**线性无关的特征向量（每个不同特征值至少贡献一个）。
    *   线性无关特征向量的总数**最多为 $n$**。
    *   总数是否达到 $n$，取决于**每一个重复特征值的几何重数是否等于其代数重数**。
    *   如果**存在任何一个**特征值，其几何重数小于其代数重数，那么矩阵 $A$ 就是**亏损的**，其线性无关特征向量的总数将**严格小于 $n$**。

## 7. 谱定理与对称矩阵

到目前为止，我们讨论了适用于所有方阵的特征分析。然而，当矩阵具有特定结构时，例如**对称性**，特征分析会展现出非常优美和强大的性质。这些性质由**谱定理**所概括。

### 7.1 对称正定/半正定矩阵

在介绍谱定理之前，我们先引入一类非常重要的对称矩阵。

**定理**: 对于任何矩阵 $A \in \mathbb{R}^{m \times n}$，由 $S = A^\top A$ 构造的矩阵 $S \in \mathbb{R}^{n \times n}$ 具有以下性质：
1.  $S$ 是**对称的 (Symmetric)**。
2.  $S$ 是**半正定的 (Positive Semidefinite)**。

**补充说明**: 如果矩阵 $A$ 的列是线性无关的（即 $\text{rk}(A) = n$），那么 $S = A^\top A$ 是**正定的 (Positive Definite)**。

**证明**:
*   **对称性**:
    $$ S^\top = (A^\top A)^\top = A^\top (A^\top)^\top = A^\top A = S $$
*   **半正定性**: 对于任意非零向量 $\boldsymbol{x} \in \mathbb{R}^n$：
    $$ \boldsymbol{x}^\top S \boldsymbol{x} = \boldsymbol{x}^\top (A^\top A) \boldsymbol{x} = (A\boldsymbol{x})^\top (A\boldsymbol{x}) = \|A\boldsymbol{x}\|_2^2 \ge 0 $$
    由于向量的 $L_2$ 范数的平方总是非负的，所以 $S$ 是半正定的。
*   **正定性 (当 $\text{rk}(A)=n$ 时)**:
    如果 $\text{rk}(A)=n$，那么齐次方程 $A\boldsymbol{x} = \mathbf{0}$ 只有唯一解 $\boldsymbol{x} = \mathbf{0}$。因此，对于任何 $\boldsymbol{x} \neq \mathbf{0}$，都有 $A\boldsymbol{x} \neq \mathbf{0}$。
    这意味着 $\|A\boldsymbol{x}\|_2^2 > 0$。所以，$\boldsymbol{x}^\top S \boldsymbol{x} > 0$ 对所有 $\boldsymbol{x} \neq \mathbf{0}$ 成立，故 $S$ 是正定的。

### 7.2 谱定理 (Spectral Theorem)

**谱定理**: 如果一个矩阵 $A \in \mathbb{R}^{n \times n}$ 是**实对称矩阵**，那么它具有以下三个核心性质：
1.  **所有特征值都是实数**。
2.  存在一个由 $A$ 的特征向量组成的**标准正交基 (Orthonormal Basis)** 来张成整个空间 $\mathbb{R}^n$。
3.  $A$ 是**可正交对角化的 (Orthogonally Diagonalizable)**。

**谱定理的证明要点**:
*   **性质1：特征值为实数**
    *   设 $\lambda$ 是一个（可能为复数）的特征值，$\boldsymbol{v}$ 是对应的特征向量。则 $A\boldsymbol{v} = \lambda\boldsymbol{v}$。
    *   两边左乘其共轭转置 $\boldsymbol{v}^*$：$\boldsymbol{v}^* A \boldsymbol{v} = \lambda \boldsymbol{v}^* \boldsymbol{v}$。
    *   因为 $A$ 是实对称矩阵 ($A=A^\top=\overline{A}=A^*$)，所以 $\boldsymbol{v}^* A \boldsymbol{v}$ 是一个实数。
    *   同时，$\boldsymbol{v}^* \boldsymbol{v} = \|\boldsymbol{v}\|^2$ 也是一个正实数。
    *   由于 $\text{(实数)} = \lambda \cdot \text{(正实数)}$，所以 $\lambda$ 必须是实数。

*   **性质2：存在标准正交基**
    *   **不同特征值对应的特征向量正交**: 如果 $A\boldsymbol{v} = \lambda\boldsymbol{v}$ 且 $A\boldsymbol{w} = \mu\boldsymbol{w}$，其中 $\lambda \neq \mu$，那么可以证明 $\boldsymbol{v}^\top \boldsymbol{w} = 0$。
    *   **相同特征值对应的特征空间**: 对于实对称矩阵，任何特征值的**几何重数都等于其代数重数**。这意味着我们总能找到足够多的线性无关的特征向量。对于重根特征值对应的特征空间，我们可以使用 **格拉姆-施密特 (Gram-Schmidt)** 正交化方法来构造一组标准正交基。
    *   将所有特征空间的正交基合并起来，就构成了整个 $\mathbb{R}^n$ 空间的一个标准正交基。

### 7.3 正交对角化 (Orthogonal Diagonalization)

谱定理的第三点通常以矩阵分解的形式呈现，这是其最重要的应用之一。

**谱定理 (矩阵形式)**: 任何一个实对称矩阵 $A \in \mathbb{R}^{n \times n}$ 都可以被分解为：
$$ A = Q \Lambda Q^\top $$
其中：
*   $Q$ 是一个**正交矩阵** ($Q^\top Q = I$，即 $Q^{-1} = Q^\top$)。$Q$ 的列由 $A$ 的**标准正交特征向量**构成。
*   $\Lambda$ (大写Lambda) 是一个**对角矩阵**，其对角线上的元素是与 $Q$ 中特征向量一一对应的**实数特征值**。

### 7.4 谱分解 (Spectral Decomposition)

正交对角化还可以写成一种“求和”形式，称为谱分解。

**谱分解**: 任何一个实对称矩阵 $A$ 都可以表示为其特征值和特征向量外积的加权和：
$$ A = \sum_{i=1}^{n} \lambda_i \boldsymbol{u}_i \boldsymbol{u}_i^\top $$
其中：
*   $\lambda_i$ 是第 $i$ 个特征值。
*   $\boldsymbol{u}_i$ 是对应的单位长度（标准正交）的特征向量。
*   每一项 $\lambda_i \boldsymbol{u}_i \boldsymbol{u}_i^\top$ 都是一个**秩为1的对称矩阵**，可以看作是向 $\boldsymbol{u}_i$ 方向的投影操作，并按 $\lambda_i$ 进行缩放。
*   **直观意义**: 任何对称变换都可以被分解为一系列在相互正交方向上的**投影和拉伸**操作的叠加。
[[Notion/Class/Proof/对称矩阵的几何变换 (Geometric transformation of symmetric matrices)\|对称矩阵的几何变换 (Geometric transformation of symmetric matrices)]]
### 7.5 示例：对称矩阵的正交对角化

考虑对称矩阵 $A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$。

1.  **求特征值与特征向量**:
    *   $\det(A-\lambda I) = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3 = (\lambda-3)(\lambda-1)=0$。
    *   特征值为 $\lambda_1 = 3, \lambda_2 = 1$。
    *   对应的特征向量为 $\boldsymbol{v}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \boldsymbol{v}_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$。（注意它们是正交的）

2.  **单位化特征向量**:
    $$ \boldsymbol{u}_1 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1 \end{pmatrix}, \quad \boldsymbol{u}_2 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -1 \end{pmatrix} $$

3.  **构造 Q 和 Λ**:
    $$ Q = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{pmatrix}, \quad \Lambda = \begin{pmatrix} 3 & 0 \\ 0 & 1 \end{pmatrix} $$

4.  **正交对角化**: $A = Q\Lambda Q^\top$。

5.  **谱分解**:
    $$ A = 3 \cdot \boldsymbol{u}_1\boldsymbol{u}_1^\top + 1 \cdot \boldsymbol{u}_2\boldsymbol{u}_2^\top = 3 \begin{pmatrix} 1/2 & 1/2 \\ 1/2 & 1/2 \end{pmatrix} + 1 \begin{pmatrix} 1/2 & -1/2 \\ -1/2 & 1/2 \end{pmatrix} $$

---

## 8. 特征值与矩阵不变量的关系

特征值与矩阵的两个重要不变量——**行列式 (Determinant)** 和 **迹 (Trace)** 之间存在着深刻的联系。

### 8.1 特征值与行列式

**定理**: 任何一个方阵 $A \in \mathbb{R}^{n \times n}$ 的**行列式**等于其所有特征值（包括复数和重根）的**乘积**。
$$ \det(A) = \prod_{i=1}^{n} \lambda_i $$

**证明思路 (对于可对角化矩阵)**:
1.  如果 $A$ 可对角化，则 $A = P \Lambda P^{-1}$。
2.  $\det(A) = \det(P \Lambda P^{-1}) = \det(P) \det(\Lambda) \det(P^{-1})$。
3.  因为 $\det(P^{-1}) = 1/\det(P)$，所以 $\det(A) = \det(\Lambda)$。
4.  对角矩阵 $\Lambda$ 的行列式就是其对角元素的乘积，即所有特征值的乘积。

**几何直观**: 行列式描述了线性变换对“体积”（或面积）的缩放比例。特征值则描述了在各个特征方向上的缩放比例。变换对总体积的缩放，等于在各个独立方向上缩放比例的乘积。

### 8.2 特征值与迹

**定理**: 任何一个方阵 $A \in \mathbb{R}^{n \times n}$ 的**迹**（主对角线元素之和）等于其所有特征值（包括复数和重根）的**和**。
$$ \text{tr}(A) = \sum_{i=1}^{n} \lambda_i $$

**证明思路 (对于可对角化矩阵)**:
1.  利用迹的循环性质：$\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)$。
2.  $\text{tr}(A) = \text{tr}(P \Lambda P^{-1})$。
3.  令 $B=P, C=\Lambda P^{-1}$，则 $\text{tr}(A) = \text{tr}((\Lambda P^{-1})P) = \text{tr}(\Lambda)$。
4.  对角矩阵 $\Lambda$ 的迹就是其对角元素的和，即所有特征值的和。

---

**非对称矩阵的补充说明**:
尽管非对称矩阵不一定满足谱定理的优美性质（如特征值不一定是实数，特征向量不一定正交），但只要它们是**可对角化**的，上述关于变换分解、行列式、迹的结论依然成立。对称矩阵的特殊之处在于它**保证**了可对角化，并且是用一种**更稳定、更具几何美感**的正交矩阵来实现的。

**拓展：[[Notion/Class/Proof/鞍点的最速逃离方向 (The fastest escape direction for the stationed point)\|鞍点的最速逃离方向 (The fastest escape direction for the stationed point)]]**

## 9. Cholesky 分解 (Cholesky Decomposition)

Cholesky 分解是另一类重要的矩阵分解，它专门针对**对称正定矩阵 (Symmetric Positive Definite, SPD)**，并且在数值计算和机器学习中因其高效性而被广泛使用。

### 9.1 定义与动机

对于一个正实数（例如 9），我们可以通过开方运算将其分解为两个相同的部分（$9 = 3 \cdot 3$）。Cholesky 分解提供了类似的概念，可以看作是对称正定矩阵的“平方根”分解。

它将一个对称正定矩阵 $A$ 分解为一个下三角矩阵 $L$ 和其转置 $L^\top$ 的乘积。这种分解在处理多元高斯分布的协方差矩阵等场景中非常实用。

### 9.2 Cholesky 分解定理

**定理 (Cholesky Decomposition)**: 任何一个对称正定矩阵 $A \in \mathbb{R}^{n \times n}$ 都可以被唯一地分解为：
$$ A = LL^\top $$
其中：
*   $L$ 是一个**下三角矩阵 (lower triangular matrix)**。
*   $L$ 的主对角线元素均为**正数**。
*   $L$ 被称为 $A$ 的 **Cholesky 因子 (Cholesky factor)**。
*   对于每一个 $A$，这个分解是**唯一的**。

**重要前提**: Cholesky 分解仅当矩阵 $A$ 是**对称**且**正定**时才存在。

**分解结构示例**:
$$ \begin{bmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn} \end{bmatrix} = \begin{bmatrix} l_{11} & 0 & \cdots & 0 \\ l_{21} & l_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ l_{n1} & l_{n2} & \cdots & l_{nn} \end{bmatrix} \begin{bmatrix} l_{11} & l_{21} & \cdots & l_{n1} \\ 0 & l_{22} & \cdots & l_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & l_{nn} \end{bmatrix} $$

### 9.3 计算方法与示例

#### 1. 计算推导 (以 3x3 矩阵为例)
我们希望找到 $A = LL^\top$ 中的 $L$。
$$
\begin{bmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{bmatrix} =
\begin{bmatrix}
l_{11} & 0 & 0 \\
l_{21} & l_{22} & 0 \\
l_{31} & l_{32} & l_{33}
\end{bmatrix}
\begin{bmatrix}
l_{11} & l_{21} & l_{31} \\
0 & l_{22} & l_{32} \\
0 & 0 & l_{33}
\end{bmatrix} =
\begin{bmatrix}
l_{11}^2 & l_{21}l_{11} & l_{31}l_{11} \\
l_{21}l_{11} & l_{21}^2 + l_{22}^2 & l_{31}l_{21} + l_{32}l_{22} \\
l_{31}l_{11} & l_{31}l_{21} + l_{32}l_{22} & l_{31}^2 + l_{32}^2 + l_{33}^2
\end{bmatrix}
$$
通过逐个对比矩阵两边的元素，我们可以按顺序求解 $l_{ij}$：
*   $l_{11} = \sqrt{a_{11}}$
*   $l_{21} = \frac{a_{21}}{l_{11}}$
*   $l_{31} = \frac{a_{31}}{l_{11}}$
*   $l_{22} = \sqrt{a_{22} - l_{21}^2}$
*   $l_{32} = \frac{a_{32} - l_{31}l_{21}}{l_{22}}$
*   $l_{33} = \sqrt{a_{33} - (l_{31}^2 + l_{32}^2)}$

这是一个递归过程，计算每个 $l_{ij}$ 时，仅依赖于 $A$ 中的元素和已经计算出的 $l$ 元素。

#### 2. 数值示例
对以下对称正定矩阵 $A$ 进行 Cholesky 分解：
$$ A = \begin{bmatrix} 4 & 2 & 2 \\ 2 & 2 & 0 \\ 2 & 0 & 3 \end{bmatrix} $$
1.  **第一列**:
    *   $l_{11} = \sqrt{a_{11}} = \sqrt{4} = 2$
    *   $l_{21} = \frac{a_{21}}{l_{11}} = \frac{2}{2} = 1$
    *   $l_{31} = \frac{a_{31}}{l_{11}} = \frac{2}{2} = 1$
2.  **第二列**:
    *   $l_{22} = \sqrt{a_{22} - l_{21}^2} = \sqrt{2 - 1^2} = \sqrt{1} = 1$
    *   $l_{32} = \frac{a_{32} - l_{31}l_{21}}{l_{22}} = \frac{0 - (1)(1)}{1} = -1$
3.  **第三列**:
    *   $l_{33} = \sqrt{a_{33} - (l_{31}^2 + l_{32}^2)} = \sqrt{3 - (1^2 + (-1)^2)} = \sqrt{3-2} = 1$

最终得到 Cholesky 因子 $L$:
$$ L = \begin{bmatrix} 2 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & -1 & 1 \end{bmatrix} $$
验证: $LL^\top = \begin{bmatrix} 2 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & -1 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 & 1 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 2 & 2 \\ 2 & 2 & 0 \\ 2 & 0 & 3 \end{bmatrix} = A$

### 9.4 Cholesky 分解算法 (General Algorithm)

对于一个 $n \times n$ 的对称正定矩阵 $A$，其 Cholesky 因子 $L$ 的元素可以通过以下公式按列或按行计算：
*   **对角元素 ($l_{jj}$)**:
    $$ l_{jj} = \sqrt{a_{jj} - \sum_{k=1}^{j-1} l_{jk}^2} $$
*   **非对角元素 ($l_{ij}$ for $i > j$)**:
    $$ l_{ij} = \frac{1}{l_{jj}} \left( a_{ij} - \sum_{k=1}^{j-1} l_{ik}l_{jk} \right) $$

**计算流程**:
按列进行计算，从 `j = 1` 到 `n`:
1.  首先使用对角元素公式计算 $l_{jj}$。
2.  然后，对于该列下方的所有元素 (`i = j + 1` 到 `n`)，使用非对角元素公式计算 $l_{ij}$。

### 9.5 在机器学习中的应用

Cholesky 分解是机器学习中高效数值计算的关键工具，尤其适用于处理协方差矩阵等对称正定矩阵。
*   **从多元高斯分布中采样**: 如果已知协方差矩阵 $\Sigma$ 的 Cholesky 分解 $\Sigma = LL^\top$，我们可以通过 $x = Lz + \mu$ 生成服从 $\mathcal{N}(\mu, \Sigma)$ 的样本，其中 $z \sim \mathcal{N}(0, I)$ 是一个标准正态分布的随机向量。
*   **随机变量的线性变换**: 该分解可以高效地实现随机变量的线性变换，这在深度随机模型（如变分自编码器 VAE）中被大量使用。
*   **高效计算行列式**: 由于 $L$ 是三角矩阵，其行列式是主对角元素的乘积。因此，$\det(A) = \det(LL^\top) = \det(L)\det(L^\top) = (\det(L))^2 = \left(\prod_{i=1}^{n} l_{ii}\right)^2$。这比直接计算行列式要快得多且数值上更稳定。

## 10. 特征分解与对角化 (Eigendecomposition and Diagonalization)

特征分解是将一个矩阵分解为由其特征值和特征向量组成的矩阵的过程。如果一个矩阵可以被特征分解，它就可以被**对角化 (Diagonalize)**，这极大地简化了与该矩阵相关的计算。

### 10.1 可对角化矩阵 (Diagonalizable Matrix)

**对角矩阵 (Diagonal Matrix)** 是所有非主对角线元素都为零的矩阵。它具有非常优良的计算性质：
*   **行列式**: $\det(D) = c_1 c_2 \cdots c_n$
*   **幂运算**: $D^k = \text{diag}(c_1^k, \dots, c_n^k)$
*   **逆矩阵**: $D^{-1} = \text{diag}(1/c_1, \dots, 1/cn)$ (如果所有 $c_i \neq 0$)

**可对角化定义**: 一个方阵 $A \in \mathbb{R}^{n \times n}$ 如果与一个对角矩阵 $D$ **相似 (similar)**，则称 $A$ 是**可对角化的**。也就是说，存在一个**可逆矩阵** $P \in \mathbb{R}^{n \times n}$ 使得：
$$ D = P^{-1}AP $$
对角化 $A$ 的过程，本质上是为与 $A$ 相关的线性变换找到了一个特殊的基——由 $A$ 的特征向量组成的基。在这个基下，线性变换被简化为简单的缩放操作。

### 10.2 特征分解定理 (Eigendecomposition Theorem)

**核心关系**: 矩阵方程 $AP = PD$ 是连接特征值/向量与对角化分解的桥梁。
设 $P = [p_1, p_2, \dots, p_n]$ 是一个列向量为 $p_i$ 的矩阵， $D$ 是一个对角线元素为 $\lambda_i$ 的对角矩阵。
*   $AP = A[p_1, \dots, p_n] = [Ap_1, \dots, Ap_n]$
*   $PD = [p_1, \dots, p_n] \begin{bmatrix} \lambda_1 & & \\ & \ddots & \\ & & \lambda_n \end{bmatrix} = [\lambda_1 p_1, \dots, \lambda_n p_n]$

因此，$AP = PD$ 等价于 $Ap_i = \lambda_i p_i$ 对所有 $i=1, \dots, n$ 成立。这意味着：**$P$ 的列向量必须是 $A$ 的特征向量，$D$ 的对角线元素是与之一一对应的特征值**。

**定理 (特征分解/可对角化定理)**: 一个 $n \times n$ 的方阵 $A$ 可以被分解为
$$ A = PDP^{-1} $$
**当且仅当** $A$ 拥有 $n$ 个**线性无关的特征向量**。
此时，
*   $P$ 是由这 $n$ 个线性无关的特征向量构成的可逆矩阵。
*   $D$ 是由对应的特征值构成的对角矩阵。

**不可对角化的矩阵**: 如果一个 $n \times n$ 矩阵没有 $n$ 个线性无关的特征向量，则称其为**亏损矩阵 (Defective Matrix)**，此时它**不能**被对角化。这种情况通常发生在特征值有重根，且重根对应的线性无关特征向量数量小于其代数重数时。

### 10.3 谱定理 vs. 特征分解定理

| 特性         | 特征分解定理 (一般情况)            | 谱定理 (特殊情况)                                  |
| :--------- | :----------------------- | :------------------------------------------ |
| **适用对象**   | 任何 $n \times n$ 方阵 $A$   | 仅限**实对称矩阵** ($A=A^\top$)                    |
| **分解形式**   | $A = PDP^{-1}$           | $A = Q\Lambda Q^\top$ (或 $Q\Lambda Q^{-1}$) |
| **分解条件**   | **必须**有 $n$ 个线性无关的特征向量   | **保证**存在                                    |
| **特征值**    | 可能是复数                    | **保证**是实数                                   |
| **特征向量矩阵** | $P$ 仅需**可逆** (列向量线性无关)   | $Q$ 是**正交矩阵** (列向量标准正交)                     |
| **关系**     | 谱定理是特征分解定理在一个更强、性质更好的特例。 | 每个实对称矩阵都可对角化，并且是**正交可对角化**。                 |

### 10.4 对称矩阵的可对角化性

**定理**: 任何一个实对称矩阵 $A \in \mathbb{R}^{n \times n}$ **总是**可以被对角化。

这是谱定理的直接推论。因为谱定理保证了：
1.  实对称矩阵的所有特征值都是实数。
2.  实对称矩阵总能找到 $n$ 个线性无关的特征向量，这些特征向量甚至可以被构造成一个**标准正交基**。
3.  因此，总能找到一个正交矩阵 $Q$ (其列是标准正交的特征向量) 和一个对角矩阵 $\Lambda$ (其对角元是特征值)，使得 $A=Q\Lambda Q^\top$。
4.  由于 $Q$ 是正交的，我们有 $Q^{-1} = Q^\top$，所以分解也符合 $A=Q\Lambda Q^{-1}$ 的形式，证明了对称矩阵总是可对角化的。

### 10.5 特征分解的几何解释

特征分解 $A = PDP^{-1}$ 为我们提供了一个理解线性变换 $x \mapsto Ax$ 的深刻几何视角。它将复杂的变换过程分解为三个简单的步骤：

1.  **切换到特征基 (Change of Basis)**: $y = P^{-1}x$
    *   $P^{-1}$ 将输入向量 $x$ 从标准坐标系转换到由 $A$ 的特征向量构成的“特征坐标系” (eigenbasis) 中，得到新坐标 $y$。

2.  **沿轴缩放 (Scaling)**: $z = Dy$
    *   在特征坐标系中，线性变换 $A$ 的作用变得极其简单：它只是将新坐标 $y$ 的每个分量（即沿着每个特征向量方向的分量）乘以对应的特征值 $\lambda_i$。这是一个纯粹的、沿坐标轴的拉伸或压缩。

3.  **切换回标准基 (Change back to Original Basis)**: $Ax = Pz$
    *   $P$ 将经过缩放后的向量 $z$ 从特征坐标系转换回标准坐标系，得到最终的变换结果 $Ax$。

**总结**: 特征分解揭示了任何可对角化的线性变换的本质——它无非是在一个特定（特征向量）的坐标系下的**“切换基 → 缩放 → 切换回来”**三部曲。

### 10.6 非对称矩阵的特征分解示例

我们对一个非对称矩阵 $B$ 进行特征分解，即使它没有对称矩阵的优美性质，但只要它可对角化，分解过程依然适用。

考虑非对称矩阵:
$$ B = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} $$

**第一步：计算特征值**
求解特征方程 $\det(B - \lambda I) = 0$。
$$ \det \begin{pmatrix} 1-\lambda & 2 \\ 3 & 4-\lambda \end{pmatrix} = (1-\lambda)(4-\lambda) - (2)(3) = 4 - 5\lambda + \lambda^2 - 6 = \lambda^2 - 5\lambda - 2 = 0 $$
利用求根公式，得到特征值：
$$ \lambda_{1,2} = \frac{-(-5) \pm \sqrt{(-5)^2 - 4(1)(-2)}}{2} = \frac{5 \pm \sqrt{25 + 8}}{2} = \frac{5 \pm \sqrt{33}}{2} $$
所以，$\lambda_1 = \frac{5 + \sqrt{33}}{2}$，$\lambda_2 = \frac{5 - \sqrt{33}}{2}$。

**第二步：计算特征向量**
对每一个特征值 $\lambda$，求解方程 $(B - \lambda I)v = 0$。

*   **对于 $\lambda_1 = \frac{5 + \sqrt{33}}{2}$**:
    $$ \begin{bmatrix} 1-\lambda_1 & 2 \\ 3 & 4-\lambda_1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} $$
    从第一行得到：$(1-\lambda_1)x + 2y = 0 \implies x = \frac{-2}{1-\lambda_1}y$。
    我们可以令 $y=1$，则对应的特征向量 $v_1 = \begin{bmatrix} \frac{-2}{1-\lambda_1} \\ 1 \end{bmatrix}$。

*   **对于 $\lambda_2 = \frac{5 - \sqrt{33}}{2}$**:
    $$ \begin{bmatrix} 1-\lambda_2 & 2 \\ 3 & 4-\lambda_2 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} $$
    从第一行得到：$(1-\lambda_2)x + 2y = 0 \implies x = \frac{-2}{1-\lambda_2}y$。
    我们可以令 $y=1$，则对应的特征向量 $v_2 = \begin{bmatrix} \frac{-2}{1-\lambda_2} \\ 1 \end{bmatrix}$。

**第三步：构建 P 和 D 矩阵**
将特征向量作为列向量构建矩阵 $P$，将对应的特征值放入对角矩阵 $D$。
$$ P = [v_1, v_2] = \begin{bmatrix} \frac{-2}{1-\lambda_1} & \frac{-2}{1-\lambda_2} \\ 1 & 1 \end{bmatrix}, \quad D = \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix} $$
最终，矩阵 $B$ 的特征分解为：
$$ B = PDP^{-1} $$

### 10.7 谱定理与对角化定理总结

| 特性        | **谱定理 (Spectral Theorem)**        | **对角化定理 (Diagonalization Theorem)**      |
| :-------- | :-------------------------------- | :--------------------------------------- |
| **适用矩阵**  | 实对称矩阵 ($A=A^\top$)                | 任何可对角化的方阵                                |
| **可对角化性** | **总是**可以对角化                       | **不一定**，仅当存在 $n$ 个线性无关的特征向量时             |
| **特征值**   | 保证为**实数**                         | 可能是复数                                    |
| **特征向量**  | 存在一组**标准正交 (orthonormal)** 的特征向量基 | 只需要**线性无关 (linearly independent)**，不要求正交 |
| **分解形式**  | $A = Q\Lambda Q^\top$             | $A = PDP^{-1}$                           |
| **变换矩阵**  | $Q$ 是**正交矩阵** ($Q^{-1}=Q^\top$)   | $P$ 只是一个**可逆矩阵**                         |

**核心区别总结**:
*   **谱定理**（针对对称矩阵）给出了一个**非常强的保证**：不仅总是可以对角化，而且可以用一个结构优美的**正交矩阵**来实现。
*   **对角化定理**（针对一般方阵）的条件则**更为宽泛**：它只要求有足够的线性无关的特征向量，而不保证这些向量是正交的，甚至不保证矩阵一定能被对-角化。

---

## 11. 特征分解的应用

特征分解之所以重要，不仅因为它揭示了矩阵变换的内在几何结构，还因为它为许多复杂的矩阵运算提供了高效的计算途径。

### 11.1 矩阵的幂运算 (Matrix Powers)

直接计算一个矩阵的高次幂 $A^k$ 是非常耗时的，需要进行 $k-1$ 次矩阵乘法。如果矩阵 $A$ 是可对角化的，即 $A = PDP^{-1}$，我们可以极大地简化这个过程。

$$ A^k = (PDP^{-1})^k = (PDP^{-1})(PDP^{-1})\cdots(PDP^{-1}) $$
由于矩阵乘法满足结合律，中间的 $P^{-1}P$ 项会相互抵消（$P^{-1}P=I$）：
$$ A^k = P D (P^{-1}P) D (P^{-1}P) \cdots D P^{-1} = P D I D I \cdots D P^{-1} = PD^kP^{-1} $$
因此，我们得到：
$$ A^k = PD^kP^{-1} $$
这个公式的威力在于，计算对角矩阵的幂 $D^k$ 非常简单，只需将对角线上的每个元素各自进行 $k$ 次方即可：
$$ D^k = \begin{bmatrix} \lambda_1^k & 0 & \cdots & 0 \\ 0 & \lambda_2^k & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_n^k \end{bmatrix} $$
**结论**: 计算 $A^k$ 的过程从多次矩阵乘法转变为一次特征分解、一次对角矩阵幂运算和两次矩阵乘法，当 $k$ 很大时，计算效率显著提升。

### 11.2 行列式计算 (Determinant Calculation)

我们也可以利用特征分解来高效计算行列式，并再次验证行列式与特征值的关系。
假设矩阵 $A$ 可被分解为 $A = PDP^{-1}$，利用行列式的性质 $\det(XY) = \det(X)\det(Y)$ 和 $\det(P^{-1}) = 1/\det(P)$：
$$ \det(A) = \det(PDP^{-1}) = \det(P) \det(D) \det(P^{-1}) $$
$$ \det(A) = \det(P) \det(D) \frac{1}{\det(P)} = \det(D) $$
而对角矩阵 $D$ 的行列式就是其对角元素的乘积，即所有特征值的乘积。
$$ \det(A) = \det(D) = \prod_{i=1}^n \lambda_i $$
这再次证明了**一个矩阵的行列式等于其所有特征值的乘积**。

## 12. 展望：奇异值分解 (Singular Value Decomposition, SVD)

特征分解是一个强大有力的工具，但它有一个根本性的限制：**它只适用于方阵**。

为了将矩阵分解的思想推广到任意形状的**非方阵**（例如 $m \times n$ 矩阵），我们需要一种更通用的分解方法。这就是我们将在下一讲中介绍的**奇异值分解 (SVD)**。SVD 是线性代数中最重要、应用最广泛的分解之一，可以看作是特征分解对任意矩阵的推广。

# 第六讲：矩阵分解 (续)

**讲座主题**: 奇异值分解 (SVD), 矩阵近似

---

## 12. 奇异值分解 (Singular Value Decomposition, SVD)

奇异值分解是线性代数中**最重要、最普适**的矩阵分解方法，有时被称为“线性代数基本定理”。它将特征分解的思想从方阵推广到了任意形状的矩阵。

### 12.1 SVD 定理

对于**任何**一个 $m \times n$ 的实数矩阵 $A$（秩为 $r$），它都可以被分解为三个矩阵的乘积：
$$ A = U\Sigma V^T $$
其中：
*   **$U$ ($m \times m$)**: 一个**正交矩阵**。它的列向量 $\mathbf{u}_i$ 被称为**左奇异向量 (left-singular vectors)**。它们构成了输出空间 $\mathbb{R}^m$ 的一组标准正交基。
*   **$V$ ($n \times n$)**: 一个**正交矩阵**。它的列向量 $\mathbf{v}_j$ 被称为**右奇异向量 (right-singular vectors)**。它们构成了输入空间 $\mathbb{R}^n$ 的一组标准正交基。
*   **$\Sigma$ ($m \times n$)**: 一个**伪对角矩阵 (pseudo-diagonal matrix)**，其形状与 $A$ 相同。
    *   它的“对角线”上（即 $\Sigma_{ii}$ 位置）的元素 $\sigma_i$ 是**奇异值 (singular values)**。
    *   所有奇异值都是**非负的** ($\sigma_i \ge 0$)，并按降序排列：$\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r > 0$，其余奇异值为0。
    *   所有非对角线元素都为零。

### 12.2 SVD 的几何直观解释

SVD 将一个线性变换 $A: \mathbb{R}^n \to \mathbb{R}^m$ 分解为三个基本几何操作：
1.  **输入空间的基变换 (旋转/反射)**: 矩阵 $V^T$ 作用于输入向量。由于 $V$ 是正交矩阵，这对应于一次旋转或反射。它将输入空间的标准基对齐到一组新的、更“合适”的标准正交基（即右奇异向量 `V` 的列）。
2.  **缩放与维度变换**: 伪对角矩阵 $\Sigma$ 沿着新的坐标轴对向量进行缩放（乘以对应的奇异值 $\sigma_i$），并处理维度的变化（如果 $m > n$，会增加零维度；如果 $m < n$，会丢弃一些维度）。
3.  **输出空间的基变换 (旋转/反射)**: 矩阵 $U$ 作用于变换后的向量。这对应于在输出空间中的一次旋转或反射，将经过缩放和维度变换后的向量对齐到最终的位置。

### 12.3 SVD 的构造方法

SVD 与特征分解密切相关。构造 SVD 的关键技巧是利用原矩阵 $A$ 构造出**对称半正定矩阵**，然后利用谱定理。

1.  **构造并分析 $A^TA$ (求解 V 和 Σ)**:
    *   $A^TA$ 是一个 $n \times n$ 的对称半正定矩阵。根据谱定理，它可以被正交对角化：$A^T A = P D P^T$。
    *   另一方面，如果我们假设 SVD 存在 ($A = U\Sigma V^T$)，那么：$A^T A = V(\Sigma^T\Sigma)V^T$。
    *   比较这两个表达式，我们得出结论：
        *   **右奇异向量 `V`** 就是 $A^TA$ 的**特征向量矩阵 `P`**。
        *   **奇异值 `σᵢ`** 的平方等于 $A^TA$ 的**特征值 `λᵢ`**，即 $\sigma_i^2 = \lambda_i$ 或 $\sigma_i = \sqrt{\lambda_i}$。

2.  **构造并分析 $AA^T$ (求解 U)**:
    *   类似地，$AA^T$ 是一个 $m \times m$ 的对称半正定矩阵。
    *   通过与 $AA^T$ 的特征分解 $SDS^T$ 比较，可以得出：
        *   **左奇异向量 `U`** 就是 $AA^T$ 的**特征向量矩阵 `S`**。

3.  **更高效的构造方法与奇异值方程**:
    *   在实践中，我们通常只计算一个（比如 $A^TA$）来得到 $V$ 和 $\sigma_i$。
    *   然后，利用**奇异值方程 (Singular Value Equation)** 来直接计算 $U$ 的列向量：
        $$ A\mathbf{v}_i = \sigma_i \mathbf{u}_i $$
    *   因此，对于所有非零奇异值 $\sigma_i > 0$：
        $$ \mathbf{u}_i = \frac{1}{\sigma_i} A\mathbf{v}_i $$
    *   这个方程也揭示了 SVD 的一个深刻联系：$A$ 将它的一个右奇异向量 $\mathbf{v}_i$ 映射为对应左奇异向量 $\mathbf{u}_i$ 的一个倍数，缩放因子就是奇异值 $\sigma_i$。

**重要注记**: 这种通过构造 $A^TA$ 来计算SVD的方法主要用于理论推导。在实际数值计算中，由于计算 $A^TA$ 可能会导致精度损失，通常会采用更直接、更稳健的算法（如 Golub-Reinsch 算法）。

---

## 13. 矩阵近似与SVD的应用

SVD 最强大的应用之一是**低秩近似 (Low-Rank Approximation)**，它构成了现代数据压缩、降噪和推荐系统的数学基础。

### 13.1 SVD作为秩-1矩阵之和

任何一个秩为 $r$ 的矩阵 $A$ 都可以被精确地表示为 $r$ 个**秩-1矩阵**的加权和：
$$ A = \sum_{i=1}^r \sigma_i \mathbf{u}_i \mathbf{v}_i^T $$
*   **$\mathbf{u}_i \mathbf{v}_i^T$**: 第 $i$ 对左、右奇异向量的**外积 (outer product)**，形成一个秩-1矩阵。
*   **$\sigma_i$**: 对应的奇异值，作为这个秩-1矩阵的**权重**。

**解释**:
SVD 将一个复杂的矩阵分解成了若干个简单的、带有权重的“基本模式”的叠加。奇异值的大小代表了每个“基本模式”的重要性。

### 13.2 最佳低秩近似：Eckart-Young定理

如果我们不加总所有的 $r$ 项，而只取其中最重要的前 $k$ 项（$k < r$），我们就能得到一个对原矩阵 $A$ 的**秩-k近似矩阵** $\hat{A}_{(k)}$：
$$ \hat{A}_{(k)} = \sum_{i=1}^k \sigma_i \mathbf{u}_i \mathbf{v}_i^T $$
这个近似为什么好？**Eckart-Young定理**给出了答案。

*   **谱范数**: 我们需要一种衡量矩阵大小或近似误差的方法。**谱范数 $\|A\|_2$** 定义为矩阵 $A$ 能将一个单位向量拉伸到的最大长度，它等于最大的奇异值 $\sigma_1$。
*   **定理内容**: 在所有秩为 $k$ 的矩阵中，由SVD截断前 $k$ 项得到的近似矩阵 $\hat{A}_{(k)}$ 是与原矩阵 $A$ **最接近**的一个（在谱范数和弗罗贝尼乌斯范数下）。
    $$ \hat{A}_{(k)} = \arg\min_{\text{rank}(B)=k} \| A - B \|_2 $$
*   **近似误差**: 这个最佳近似的误差大小（用谱范数衡量）恰好等于**被丢弃的第一个奇异值** $\sigma_{k+1}$。
    $$ \|A - \hat{A}_{(k)}\|_2 = \sigma_{k+1} $$

**意义**:
1.  **最优性保证**: SVD提供了一种构造最佳低秩近似的“配方”。
2.  **误差可控**: 我们可以通过观察奇异值的大小来预估近似带来的误差。如果 $\sigma_{k+1}$ 很小，说明这个秩-k近似的质量非常高。
3.  **应用**: 这正是SVD被广泛用于**数据压缩**、**数据去噪**和**降维**（如主成分分析PCA）的理论基础。

### 13.3 SVD的潜在结构解释能力

*   SVD不仅仅是一个数学工具，它能够揭示数据中隐藏的模式。在“用户-电影”评分矩阵这类应用中：
    *   **左奇异向量 `U` 的列**: 可被解释为物品的**“主题”或“类型”**。
    *   **右奇异向量 `V` 的列**: 可被解释为用户的**“原型”或“偏好群组”**。
    *   **奇异值 `Σ` 的对角元**: 代表了每个“主题-原型”模式的**强度**。
*   通过保留少数几个最大的奇异值，SVD能够提取出数据中最主要的、潜在的结构，从而使数据变得**可解释**，并能够用于预测和推荐。

# 第七讲：向量微积分 (Vector Calculus)

**讲座主题**: 单变量函数微分，偏微分与梯度，向量值函数的梯度，矩阵的梯度，梯度计算实用恒等式

---

## 第一部分：从单变量到多变量微积分

本讲座将从最基础的微积分概念开始，逐步推广到处理向量和矩阵的多元微积分，这对于理解和实现机器学习中的优化算法至_关重要。

### 1. 核心概念回顾

**1.1 函数 (Functions)**

*   **核心**: 函数是建立输入与输出之间关系的核心数学工具。
*   **定义**: 一个函数 $f$ 将一个**输入 (input)** $x$ 映射到一个**输出 (target)** $f(x)$。
*   **记法**: $f: \mathbb{R}^D \to \mathbb{R}$，其中 $x \mapsto f(x)$。

**1.2 梯度在机器学习中的作用**

*   **重要性**: 梯度指明了函数值**上升最快的方向 (direction of steepest ascent)**。在优化算法中，我们沿着梯度的反方向移动，以找到函数的最小值点。

---

### 2. 单变量函数的微分 (Differentiation of Univariate Functions)

本节回顾了单变量微积分的基础。

*   **差商 (Difference Quotient)**:
    $$ \frac{\delta y}{\delta x} := \frac{f(x + \delta x) - f(x)}{\delta x} $$
    差商计算的是函数图像上两点之间**割线的斜率**，代表了区间的**平均变化率**。

*   **导数 (Derivative)**:
    当 $\delta x$ (或记为 $h$) 趋近于 0 时，割线逼近**切线**。这条切线的斜率就是函数 $f$ 在点 $x$ 的**导数**。
    $$ \frac{df}{dx} := \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} $$
    导数代表了函数在该点的**瞬时变化率**。

---

### 3. 泰勒级数 (Taylor Series)

泰勒级数是微积分中一个极其强大的工具，它允许我们将一个复杂的、可微的函数**用一个无穷多项式来表示**，从而在局部近似和分析函数的行为。

#### 3.1 泰勒多项式 (Taylor Polynomial)

*   **定义**: 函数 $f: \mathbb{R} \to \mathbb{R}$ 在点 $x_0$ 处的 **n 阶泰勒多项式** $T_n(x)$ 是泰勒级数的**有限项截断**，它提供了对原函数的一个**局部近似**。其定义为：
    $$ T_n(x) := \sum_{k=0}^{n} \frac{f^{(k)}(x_0)}{k!} (x - x_0)^k $$
    其中 $f^{(k)}(x_0)$ 是函数 $f$ 在点 $x_0$ 的 **k 阶导数**。
*   **近似的意义**: 阶数 `n` 越高，泰勒多项式在 $x_0$ 附近就越能精确地模拟原函数 $f(x)$ 的行为。

#### 3.2 泰勒级数与麦克劳林级数

*   **泰勒级数**: 当 n 趋近于无穷时，泰勒多项式就变成了**泰勒级数**。这要求函数是**无限次可微的** ($f \in C^\infty$)。
    $$ T_\infty(x) = \sum_{k=0}^{\infty} \frac{f^{(k)}(x_0)}{k!} (x - x_0)^k $$
*   **麦克劳林级数 (Maclaurin Series)** 是泰勒级数的一个重要特例，即在展开点 **$x_0 = 0$** 处展开。

#### 3.3 泰勒级数与幂级数的关系

*   **幂级数 (Power Series)** 是任何形如 $\sum_{k=0}^{\infty} a_k (x-c)^k$ 的无穷级数。
*   **泰勒级数是幂级数的一个特例**，其特殊之处在于，系数 $a_k$ 不是任意的，而是由函数 $f$ 在展开点 $c$ 的各阶导数唯一确定的：
    $$ a_k = \frac{f^{(k)}(c)}{k!} $$

## **第二部分：多元函数的微分**

现在，我们将微积分的思想从单变量函数推广到处理多个变量的函数，这是向量微积分的核心。

### 4. 偏导数与梯度

#### 4.1 偏导数 (Partial Derivative)

*   **核心思想**: 计算多变量函数对其中**一个变量**的变化率，同时**将所有其他变量视为常数**。
*   **定义**:
    $$ \frac{\partial f}{\partial x_i} := \lim_{h \to 0} \frac{f(x_1, \dots, x_i+h, \dots, x_n) - f(\mathbf{x})}{h} $$

#### 4.2 梯度 (Gradient)

*   **定义**: 一个多元**标量值**函数 $f(\mathbf{x})$ 的**梯度**，是将其对**每一个**自变量的偏导数收集到一个**行向量**中。
*   **记法与符号约定**: 在本课程的语境下，以下符号是等价的，都代表梯度：
    $$ \frac{df}{d\mathbf{x}} = \frac{\partial f}{\partial \mathbf{x}} = \nabla_{\mathbf{x}}f = \begin{bmatrix} \frac{\partial f}{\partial x_1} & \frac{\partial f}{\partial x_2} & \cdots & \frac{\partial f}{\partial x_n} \end{bmatrix} $$
*   **维度**: 如果 $\mathbf{x} \in \mathbb{R}^n$ (列向量)，其梯度是 **$1 \times n$ 的行向量**。
*   **几何意义**: 梯度向量指向函数值**上升最快**的方向。

---

### 5. 雅可比矩阵与多元链式法则

#### 5.1 雅可比矩阵 (Jacobian Matrix)

*   **推广**: 当我们的函数是一个**向量值**函数，即 $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ 时，我们需要一个矩阵来包含所有的导数信息。
*   **定义**: 函数 $\mathbf{f}$ 的**雅可比矩阵** $J$ 是一个 $m \times n$ 的矩阵，其第 `i` 行是输出向量的第 `i` 个分量 $f_i$ 的梯度。
    $$ J = \frac{d\mathbf{f}}{d\mathbf{x}} = \begin{bmatrix} \nabla_{\mathbf{x}}f_1 \\ \nabla_{\mathbf{x}}f_2 \\ \vdots \\ \nabla_{\mathbf{x}}f_m \end{bmatrix} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{bmatrix} \quad \text{其中 } J_{ij} = \frac{\partial f_i}{\partial x_j} $$
*   **梯度是雅可比矩阵的特例**: 当函数是标量值时 ($m=1$)，其 $1 \times n$ 的雅可比矩阵就是我们之前定义的梯度（行向量）。

#### 5.2 多元链式法则 (Chain Rule for Multivariate Functions)

*   **核心法则**: 链式法则是计算复合函数导数的基础。它的通用形式是**雅可比矩阵的乘积**。
*   **形式**: 对于复合函数 $\mathbf{h}(\mathbf{x}) = \mathbf{g}(\mathbf{f}(\mathbf{x}))$，其导数为：
    $$ \frac{\partial \mathbf{h}}{\partial \mathbf{x}} = \frac{\partial \mathbf{g}}{\partial \mathbf{f}} \frac{\partial \mathbf{f}}{\partial \mathbf{x}} $$
*   **解读**: 这是一个**矩阵乘法**。如果 $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ 且 $\mathbf{g}: \mathbb{R}^m \to \mathbb{R}^p$，那么最终结果 $\frac{\partial \mathbf{h}}{\partial \mathbf{x}}$ 是一个 $p \times n$ 的雅可比矩阵。
*   **通用性**: 这个法则适用于任意层数的函数复合，例如 $g(f(h(x)))$ 的导数是 $\frac{dg}{df}\frac{df}{dh}\frac{dh}{dx}$。
#### 5.3 核心求导示例

*   **示例1: 线性变换的雅可比**
    *   **问题**: 计算函数 $\mathbf{f}(\mathbf{x}) = A\mathbf{x}$ 对 $\mathbf{x}$ 的导数（雅可比矩阵），其中 $A \in \mathbb{R}^{M \times N}$。
    *   **推导**:
        1.  函数的第 `i` 个分量是 $f_i(\mathbf{x}) = \sum_{j=1}^N A_{ij}x_j$。
        2.  计算其对 $x_k$ 的偏导数：$\frac{\partial f_i}{\partial x_k} = A_{ik}$。
        3.  将这些偏导数 $J_{ik} = A_{ik}$ 填入雅可比矩阵：
            $$ \frac{d\mathbf{f}}{d\mathbf{x}} = \begin{bmatrix}
            \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_N} \\
            \vdots & \ddots & \vdots \\
            \frac{\partial f_M}{\partial x_1} & \cdots & \frac{\partial f_M}{\partial x_N}
            \end{bmatrix} = \begin{bmatrix}
            A_{11} & \cdots & A_{1N} \\
            \vdots & \ddots & \vdots \\
            A_{M1} & \cdots & A_{MN}
            \end{bmatrix}
            $$
    *   **结论**:
        $$ \frac{d(A\mathbf{x})}{d\mathbf{x}} = A $$

*   **示例2：最小二乘损失的梯度**
    *   **问题**: 计算损失函数 $L(\boldsymbol{\theta}) = \|\mathbf{y}_{\text{obs}} - \Phi\boldsymbol{\theta}\|^2$ 对参数 $\boldsymbol{\theta}$ 的梯度 $\frac{\partial L}{\partial \boldsymbol{\theta}}$。
    *   **重要性**: 这是线性回归等模型的核心优化问题。
    *   **步骤 (使用链式法则)**:
        1.  设误差 $\mathbf{e}(\boldsymbol{\theta}) = \mathbf{y}_{\text{obs}} - \Phi\boldsymbol{\theta}$，则 $L(\mathbf{e}) = \mathbf{e}^T\mathbf{e}$。
        2.  应用链式法则：$\frac{\partial L}{\partial \boldsymbol{\theta}} = \frac{\partial L}{\partial \mathbf{e}} \frac{\partial \mathbf{e}}{\partial \boldsymbol{\theta}}$。
        3.  计算各部分：$\frac{\partial L}{\partial \mathbf{e}} = 2\mathbf{e}^T$ (这是一个 $1 \times N$ 的梯度)，$\frac{\partial \mathbf{e}}{\partial \boldsymbol{\theta}} = -\Phi$ (这是一个 $N \times D$ 的雅可比矩阵)。
        4.  组合结果：
            $$ \frac{\partial L}{\partial \boldsymbol{\theta}} = 2\mathbf{e}^T(-\Phi) = -2\mathbf{e}^T\Phi = -2(\mathbf{y}_{\text{obs}} - \Phi\boldsymbol{\theta})^T\Phi $$

---

### 6. 矩阵的梯度 (Gradients of/with respect to Matrices)

#### 6.1 概念与挑战

*   **张量结构**: 当我们对矩阵求导时，例如计算 $m \times n$ 矩阵 $A$ 对 $p \times q$ 矩阵 $B$ 的导数，结果是一个复杂的**四维张量**。其第 `(i,j,k,l)` 个元素是 $\frac{\partial A_{ij}}{\partial B_{kl}}$。
*   **实践中的简化**: 在实践中，直接处理高维张量很不方便。一个常用的技巧是**将矩阵“展平”(flatten)** 为向量，然后问题就退化为我们熟悉的向量对向量求导，其结果是一个大的雅可比矩阵。

#### 6.2 核心示例推导

*   **示例1: 向量函数对矩阵的梯度**
    *   **问题**: 对于 $\mathbf{f} = A\mathbf{x}$，计算 $\frac{d\mathbf{f}}{dA}$。
    *   **推导**:
        1.  输出的第 `i` 个分量 $f_i = \sum_{j=1}^{N} A_{ij} x_j$。
        2.  计算 $f_i$ 对 $A_{mn}$ 的偏导数：$\frac{\partial f_i}{\partial A_{mn}} = \begin{cases} x_n & \text{if } i=m \\ 0 & \text{if } i \neq m \end{cases}$ *(这里幻灯片的 `xk if i=j` 似乎有误，根据推导应该是 `xn if i=m`。我们以推导为准)*。
        3.  这个结果表明，输出 $f_i$ **只依赖于**矩阵 $A$ 的**第 i 行** $A_{i,:}$。
        4.  对该行求导的结果是：
            $$ \frac{\partial f_i}{\partial A_{i,:}} = \mathbf{x}^T $$
    *   **结论 (雅可比矩阵结构)**: 对整个输出向量 $\mathbf{f}$ 求导时，其对 $A$ 的梯度是一个块状结构的雅可比矩阵。例如，对 $f_i$ 的导数部分，其结构是一个在第 `i` 个块位置上为 $\mathbf{x}^T$ 的大行向量：
        $$ \frac{\partial f_i}{\partial A} = \begin{bmatrix} \mathbf{0}^T & \cdots & \mathbf{0}^T & \underbrace{\mathbf{x}^T}_{\text{i-th block}} & \mathbf{0}^T & \cdots & \mathbf{0}^T \end{bmatrix} $$

*   **示例2: 矩阵函数对矩阵的梯度**
    *   **问题**: 对于 $F = AB$，计算 $\frac{dF}{dA}$。
    *   **推导**:
        1.  输出矩阵的元素 $F_{ik} = \sum_{j=1}^{N} A_{ij} B_{jk}$。
        2.  计算其对 $A_{mn}$ 的偏导数：$\frac{\partial F_{ik}}{\partial A_{mn}} = \begin{cases} B_{nk} & \text{if } i=m \\ 0 & \text{if } i \neq m \end{cases}$。
        3.  这个结果表明，输出矩阵 $F$ 的第 `i` 行 $F_{i,:}$ **只依赖于**输入矩阵 $A$ 的**第 i 行** $A_{i,:}$。
    *   **结论 (块状雅可比)**:
        *   对 $A$ 的第 `i` 行求导，我们得到一个矩阵：
            $$ \frac{\partial F_{i,:}}{\partial A_{i,:}} = B^T $$
        *   对整个输出 $F$ 求导时，其雅可比矩阵也呈块状结构。例如，对输出的第 `i` 行 $F_{i,:}$ 求导，其结果为：
            $$ \frac{\partial F_{i,:}}{\partial A} = \begin{bmatrix} 0 & \cdots & 0 & \underbrace{B^T}_{\text{i-th block}} & 0 & \cdots & 0 \end{bmatrix} $$
