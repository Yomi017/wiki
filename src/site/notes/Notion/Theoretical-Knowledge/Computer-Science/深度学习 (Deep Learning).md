---
{"dg-publish":true,"permalink":"/notion/theoretical-knowledge/computer-science/deep-learning/"}
---

### 1. 神经网络 (Neural Network)

![Image/Deep Learning/1.png](/img/user/Image/Deep%20Learning/1.png)
从左往右依次是：**输入层 (Input Layer)**   **隐藏层 (Hidden Layer)**   **输出层 (Output Layer)**

1. **输入层 (Input Layer / 输入层)**
    
    - **作用**: 接收原始数据或特征。网络从这一层获取信息。
        
    - **神经元数量**: 通常等于输入数据的特征数量。例如，如果你的输入是一张28x28像素的灰度图片，展平后输入层就有784个神经元，每个神经元对应一个像素值。
        
    - **计算**: 输入层通常不做任何计算，它只是将输入值传递给下一层（隐藏层）。有时可能会进行一些预处理，如归一化。
        

---

1. **隐藏层 (Hidden Layer / 隐藏层)**
    
    - **作用**: 这是网络的核心，负责从输入数据中学习和提取复杂的特征和模式。隐藏层通过对输入数据进行非线性变换，使得网络能够学习非线性关系。
        
    - **神经元数量**: 隐藏层中神经元的数量是一个超参数，需要根据具体问题和数据进行调整和优化。可以有一个或多个隐藏层（如果多于一个隐藏层，就称为深度神经网络）。在一个“三层神经网络”的经典定义中，通常指一个隐藏层。
        
    - **计算**:
        
        - 每个隐藏层的神经元都会接收来自前一层（输入层）所有神经元的加权输入。
            
        - 计算加权和：$$z = (\omega_1*x_1 + \omega_2*x_2 + ... + \omega_n*x_n) + b$$其中 $x$ 是输入，$\omega$ 是权重，$b$ 是偏置。
         写成矩阵形式则是：$$\begin{pmatrix} 
         a_0^{(j+1)} \\
         a_1^{(j+1)} \\
         \vdots \\
         a_n^{(j+1)}
         \end{pmatrix}
         =
         \begin{pmatrix} 
         \omega_{0,0} & \omega_{0,1} & \cdots & \omega_{0,n} \\
         \omega_{1,0} & \omega_{1,1} & \cdots & \omega_{1,n} \\
         \omega_{2,0} & \omega_{2,1} & \cdots & \omega_{2,n} \\
         \vdots & \vdots & \ddots & \vdots \\
         \omega_{k,0} & \omega_{k,1} & \cdots & \omega_{k,n} \\
         \end{pmatrix}
         \begin{pmatrix} 
         a_0^{(j)} \\
         a_1^{(j)} \\
         \vdots \\
         a_n^{(j)}
         \end{pmatrix}
         $$即$$z^{(j+1)}=Wz^{(j)}+b$$
		 **训练的进行就是要改变这些参数以达到最优解**
            
        - 将加权和通过一个**激活函数 (Activation Function)**（如 Sigmoid, ReLU, Tanh 等）进行非线性转换：$a = \text{activation-function}(z)$。这个激活后的值 a 会作为下一层（输出层）的输入。
         
         **Sigmoid**: $\dfrac{1}{1+e^{-x}}$
         **ReLU**: $\max{(0,x)}$
         **Tanh**: $\tanh x$
			
    - **重要性**: 隐藏层的存在使得神经网络能够学习比线性模型复杂得多的函数。
        

---

1. **输出层 (Output Layer / 输出层)**
    
    - **作用**: 产生网络的最终输出，即预测结果或决策。
        
    - **神经元数量**: 输出层神经元的数量取决于具体的任务类型：
        
        - **二分类问题** (e.g., 是/否, 猫/狗): 通常有1个神经元，使用 Sigmoid 激活函数输出一个0到1之间的概率值。
            
        - **多分类问题** (e.g., 数字0-9识别): 通常有N个神经元，N等于类别数量，使用 Softmax 激活函数输出每个类别的概率分布。
            
        - **回归问题** (e.g., 预测房价): 通常有1个神经元（如果要预测多个值，则有对应数量的神经元），通常不使用激活函数或使用线性激活函数。
            
    - **计算**: 与隐藏层类似，输出层的神经元接收来自前一层（隐藏层）的加权输入，计算加权和，并通过适合任务的激活函数得到最终输出。

**前向传播 Forward Propagation:**

数据从输入层开始，依次通过隐藏层，最后到达输出层，得到最终的预测结果。这个过程称为前向传播。
