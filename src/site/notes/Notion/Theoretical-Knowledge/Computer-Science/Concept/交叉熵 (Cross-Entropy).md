---
{"dg-publish":true,"permalink":"/notion/theoretical-knowledge/computer-science/concept/cross-entropy/"}
---

想象一下，你有两组关于同一事件可能结果的概率分布。交叉熵就是一种衡量这两组概率分布之间差异的方法。在分类问题中：

*   **一组概率分布是真实的 (True Distribution)**：这个我们是知道的。例如，对于一张图片，它**真实**的类别是“猫”。如果我们有3个类别（猫，狗，鸟），那么这个真实分布可以表示为：
    *   P(猫) = 1, P(狗) = 0, P(鸟) = 0  (这就是 one-hot 编码)
*   **另一组概率分布是模型预测的 (Predicted Distribution)**：这是我们的机器学习模型（比如神经网络）经过计算后给出的关于这张图片属于各个类别的概率。例如，模型可能预测：
    *   P_model(猫) = 0.7, P_model(狗) = 0.2, P_model(鸟) = 0.1

我们希望模型预测的分布尽可能地接近真实的分布。交叉熵就提供了一个量化这种“接近程度”的方法，或者更准确地说，是量化用一个分布（模型预测的）去编码另一个分布（真实的）时所需的平均信息量。**交叉熵越小，说明两个分布越接近。**

---

### 1. 从信息论的角度理解 (Understanding from Information Theory)

*   **信息量 (Information Content)**: 一个事件发生的概率越小，它实际发生时所包含的信息量就越大。信息量 $I(x)$ 可以用 $-\log(P(x))$ 来表示，其中 $P(x)$ 是事件 $x$ 发生的概率。对数通常以2为底（单位是比特 bits），也可以是自然对数 $e$（单位是奈特 nats）。
*   **熵 (Entropy)**: 一个概率分布的熵表示了遵循该分布的事件所包含的平均信息量，或者说是编码来自该分布的随机选择的样本所需的平均比特数。对于一个离散随机变量 $X$ 可能取值为 $x_1, x_2, \dots, x_K$，其概率分布为 $P(x_k)$，熵 $H(P)$ 定义为：
    $$H(P) = - \sum_{k=1}^{K} P(x_k) \log(P(x_k))$$
*   **交叉熵 (Cross-Entropy)**: 现在假设我们有两个概率分布 $P$（真实分布）和 $Q$（模型预测分布），它们都是关于同一组事件的。交叉熵 $H(P, Q)$ 衡量的是，当我们使用基于分布 $Q$ 的编码方案来编码来自真实分布 $P$ 的事件时，平均所需的编码长度（或信息量）。它的定义是：
    $$H(P, Q) = - \sum_{k=1}^{K} P(x_k) \log(Q(x_k))$$
    **关键点**：我们用真实分布 $P(x_k)$ 来加权，但取的是模型预测概率 $Q(x_k)$ 的对数。

---

### 2. 在分类任务中的应用 (Application in Classification Tasks)

在分类任务中，我们的目标是让模型的预测概率分布 $Q$ 尽可能地匹配真实的类别概率分布 $P$。

#### A. 二分类交叉熵 (Binary Cross-Entropy)

*   **场景**: 只有两个类别，通常标记为 0 和 1 (例如：垃圾邮件/非垃圾邮件，猫/非猫)。
*   **真实标签 ($y^{(n)}$)**: 对于第 $n$ 个样本，其真实标签 $y^{(n)}$ 是 0 或 1。
*   **模型预测 ($\hat{y}^{(n)}$)**: 模型（通常是神经网络最后一层使用 Sigmoid 激活函数）输出一个概率值 $\hat{y}^{(n)}$，表示该样本属于类别 1 的概率。那么，它属于类别 0 的概率就是 $1 - \hat{y}^{(n)}$。
*   **公式推导**:
    我们可以将二分类问题看作是 $K=2$ 的特殊多分类问题。
    *   类别 1: 真实概率 $P(\text{class}=1) = y^{(n)}$，模型预测概率 $Q(\text{class}=1) = \hat{y}^{(n)}$
    *   类别 0: 真实概率 $P(\text{class}=0) = 1 - y^{(n)}$，模型预测概率 $Q(\text{class}=0) = 1 - \hat{y}^{(n)}$

    根据交叉熵的定义 $H(P, Q) = - \sum_{k=1}^{K} P(x_k) \log(Q(x_k))$:
    单个样本 $n$ 的损失 $e_n$ (即二元交叉熵) 为：
    \begin{align*} e_n &= - [P(\text{class}=1) \log(Q(\text{class}=1)) + P(\text{class}=0) \log(Q(\text{class}=0))] \\ &= - [y^{(n)} \log(\hat{y}^{(n)}) + (1 - y^{(n)}) \log(1 - \hat{y}^{(n)})] \end{align*}
    这正是我们看到的二元交叉熵公式。

*   **直观解释**:
    *   **如果真实标签 $y^{(n)} = 1$**:
        *   公式变为 $e_n = - [1 \cdot \log(\hat{y}^{(n)}) + 0 \cdot \log(1 - \hat{y}^{(n)})] = - \log(\hat{y}^{(n)})$。
        *   这意味着我们只关心模型预测该样本为类别 1 的概率 $\hat{y}^{(n)}$。
        *   如果模型预测 $\hat{y}^{(n)} \approx 1$ (正确且高置信度)，那么 $\log(\hat{y}^{(n)}) \approx 0$，损失 $e_n \approx 0$。
        *   如果模型预测 $\hat{y}^{(n)} \approx 0$ (错误且高置信度)，那么 $\log(\hat{y}^{(n)})$ 是一个很大的负数，损失 $e_n$ 是一个很大的正数，惩罚很大。
    *   **如果真实标签 $y^{(n)} = 0$**:
        *   公式变为 $e_n = - [0 \cdot \log(\hat{y}^{(n)}) + 1 \cdot \log(1 - \hat{y}^{(n)})] = - \log(1 - \hat{y}^{(n)})$。
        *   我们关心模型预测该样本为类别 0 的概率，即 $1 - \hat{y}^{(n)}$。
        *   如果模型预测 $\hat{y}^{(n)} \approx 0$ (正确且高置信度)，那么 $1 - \hat{y}^{(n)} \approx 1$，$\log(1 - \hat{y}^{(n)}) \approx 0$，损失 $e_n \approx 0$。
        *   如果模型预测 $\hat{y}^{(n)} \approx 1$ (错误且高置信度)，那么 $1 - \hat{y}^{(n)} \approx 0$，$\log(1 - \hat{y}^{(n)})$ 是一个很大的负数，损失 $e_n$ 是一个很大的正数。

#### B. 多分类交叉熵 (Categorical Cross-Entropy)

*   **场景**: 有 $K$ 个类别 ($K > 2$) (例如：数字识别 0-9，物体分类 猫/狗/鸟/鱼)。
*   **真实标签 ($y^{(n)}$)**: 对于第 $n$ 个样本，其真实标签通常表示为一个 **one-hot 编码**的向量。这是一个 $K$ 维的向量，如果样本真实属于第 $j$ 类，则向量的第 $j$ 个元素为 1，其余所有元素为 0。
    *   例如，有3个类别 (猫，狗，鸟)。如果一个样本是“狗”，其 one-hot 真实标签是 $y^{(n)} = [0, 1, 0]$。这里 $y_1^{(n)}=0, y_2^{(n)}=1, y_3^{(n)}=0$。
*   **模型预测 ($\hat{y}^{(n)}$)**: 模型（通常是神经网络最后一层使用 Softmax 激活函数）输出一个 $K$ 维的概率向量 $\hat{y}^{(n)} = [\hat{y}_1^{(n)}, \hat{y}_2^{(n)}, \dots, \hat{y}_K^{(n)}]$。其中 $\hat{y}_k^{(n)}$ 是模型预测该样本属于类别 $k$ 的概率，并且所有类别的预测概率之和为 1 ($\sum_{k=1}^{K} \hat{y}_k^{(n)} = 1$)。
*   **公式**:
    单个样本 $n$ 的损失 $e_n$ (即分类交叉熵) 为：
    $$e_n = - \sum_{k=1}^{K} y_k^{(n)} \log(\hat{y}_k^{(n)})$$
*   **直观解释**:
    *   由于 $y_k^{(n)}$ 是 one-hot 编码的，在求和 $\sum_{k=1}^{K}$ 中，只有一项 $y_k^{(n)}$ 的值为 1（对应真实类别），其余项的 $y_k^{(n)}$ 都为 0。
    *   因此，这个求和实际上只关注模型对**真实类别**的预测概率。
    *   假设样本 $n$ 的真实类别是第 $j$ 类 (即 $y_j^{(n)} = 1$，其他 $y_k^{(n)} = 0$ for $k \neq j$)。那么损失公式简化为：
        $$e_n = - [0 \cdot \log(\hat{y}_1^{(n)}) + \dots + 1 \cdot \log(\hat{y}_j^{(n)}) + \dots + 0 \cdot \log(\hat{y}_K^{(n)})] = - \log(\hat{y}_j^{(n)})$$
    *   这意味着，多分类交叉熵只惩罚模型对真实类别的预测概率 $\hat{y}_j^{(n)}$。
        *   如果模型对真实类别 $j$ 的预测概率 $\hat{y}_j^{(n)} \approx 1$ (正确且高置信度)，那么 $\log(\hat{y}_j^{(n)}) \approx 0$，损失 $e_n \approx 0$。
        *   如果模型对真实类别 $j$ 的预测概率 $\hat{y}_j^{(n)} \approx 0$ (错误且高置信度)，那么 $\log(\hat{y}_j^{(n)})$ 是一个很大的负数，损失 $e_n$ 是一个很大的正数。

---

### 3. 为什么在深度学习分类中使用交叉熵？

1.  **衡量概率分布差异**: 它直接衡量了模型预测的概率分布与真实的类别分布之间的差异，这正是分类任务的目标。
2.  **与输出层激活函数良好配合**:
    *   **Sigmoid (二分类)**: Sigmoid 输出 $(0,1)$ 的概率，二元交叉熵损失函数对其求导后形式简洁，有助于优化。
    *   **Softmax (多分类)**: Softmax 输出所有类别的概率分布（和为1），分类交叉熵损失函数与其配合求导后形式也相对简洁。
3.  **梯度特性**:
    *   **避免饱和区域的梯度消失**: 相比于均方误差 (MSE) 用于分类问题，交叉熵在模型输出远离目标时（即 Sigmoid 或 Softmax 输出接近0或1的饱和区，但预测错误）仍然能提供较大的梯度。MSE 在这些区域梯度会非常小，导致学习停滞。
    *   例如，对于 Sigmoid 输出 $\sigma(z)$ 和真实标签 $y$，使用二元交叉熵时，损失对 $z$ (Sigmoid的输入) 的导数是 $\sigma(z) - y$，这是一个非常简单且直观的形式：预测与真实的差异。
4.  **惩罚错误预测**: 它对预测错误且置信度高的模型给予非常大的惩罚（因为 $\log(p)$ 在 $p \to 0$ 时趋向 $-\infty$），从而强烈地驱动模型向正确的方向学习。

---

*   交叉熵是一种衡量两个概率分布之间差异的指标。
*   在分类任务中，我们用它来衡量模型预测的类别概率分布与真实的类别概率分布之间的差异。
*   二元交叉熵用于两个类别的情况，多分类交叉熵用于多个类别的情况（通常配合 one-hot 编码的真实标签）。
*   其核心思想是：如果模型对真实类别的预测概率很高，损失就很小；如果预测概率很低，损失就很大。
*   它因其良好的数学特性（特别是梯度特性）和对错误预测的有效惩罚，成为深度学习分类任务中首选的损失函数。
