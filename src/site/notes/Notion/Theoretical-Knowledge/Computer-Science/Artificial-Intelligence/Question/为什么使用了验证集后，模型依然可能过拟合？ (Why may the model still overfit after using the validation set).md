---
{"dg-publish":true,"permalink":"/notion/theoretical-knowledge/computer-science/artificial-intelligence/question/why-may-the-model-still-overfit-after-using-the-validation-set/"}
---

# **1. 为什么使用了验证集后，模型依然可能过拟合？**

在机器学习流程中，我们通常将数据分为训练集（Training Set）、验证集（Validation Set）和测试集（Test Set）。验证集的核心作用是模拟未见过的真实数据，帮助我们进行模型选择和超参数调优，以期获得更好的泛化能力。

然而，一个常见的误解是：只要在验证集上表现好，模型就一定不会过拟合。**事实并非如此。** 模型完全有可能对**验证集本身**产生过拟合，这种现象被称为**“数据窥探 (Data Snooping)”** 或对验证集的过拟合。

## **1.1 核心原因：验证集被“污染”**

当我们在模型开发周期中**反复使用**同一个验证集来指导我们的决策时，验证集的信息就会逐渐**“泄露 (leak)”**到我们的模型选择和调优过程中。

**这个过程可以被看作一次“元级别的训练 (meta-training)”：**

*   **“训练数据”**: 我们的各种候选模型（由不同架构、不同超参数训练得到）。
*   **“标签”**: 每个模型在验证集上的性能得分（如准确率、损失值）。
*   **“学习者”**: 我们自己（或者自动化调参算法）。
*   **“学习过程”**: 我们根据验证集的反馈，不断调整策略（“这个学习率太高了，调低一点”、“这个网络层数似乎更好”），直到找到一个在验证集上得分最高的模型。

在这个“元级别训练”中，我们不知不觉地把模型调整得越来越适应**这个特定**的验证集，而不是普适的、未知的真实数据分布。

## **1.2 理论解释：泛化理论的应用**

我们可以用之前讨论过的泛化理论来精确地解释这个现象。

![24.png](/img/user/Image/Computer-Science/Machine%20Learning/24.png)

1.  **回顾泛化上界**:
    一个学习过程失败（即在训练数据上好，但在真实数据上差）的概率上界为：
    $$ P(\text{bad data}) \le |\mathcal{H}| \cdot 2\exp(-2N\epsilon^2) $$

2.  **将理论应用于验证过程**:
    *   **“训练集”** $\rightarrow$ **验证集 $\mathcal{D}_{val}$** (大小为 $N_{val}$)
    *   **“假设空间” $\mathcal{H}$** $\rightarrow$ **候选模型集 $\mathcal{H}_{val} = \{h^*_1, h^*_2, h^*_3, \dots\}$**

    于是，我们用验证集进行选择这个过程会失败的概率上界为：
    $$ P(\mathcal{D}_{val} \text{ is bad}) \le |\mathcal{H}_{val}| \cdot 2\exp(-2N_{val}\epsilon^2) $$

    *   **$P(\mathcal{D}_{val} \text{ is bad})$** 指的是：我们选出的在验证集上表现最好的模型，其在真实世界中的表现却很差的概率。
    *   **$|\mathcal{H}_{val}|$** 是我们**尝试过的候选模型的总数量**。

3.  **过拟合如何发生**:
    *   从公式中可以清晰地看到，失败概率与 $|\mathcal{H}_{val}|$ **成正比**。
    *   **如果你只尝试了少数几个模型（比如3-5个），$|\mathcal{H}_{val}|$ 很小**。只要你的验证集 $N_{val}$ 不是太小，失败的概率就极低。这是验证集能正常工作的理想情况。
    *   **但如果你进行了大规模的、自动化的超参数搜索，尝试了成百上千种模型组合**，那么 $|\mathcal{H}_{val}|$ 就会变得非常大。此时，即使 $N_{val}$ 不小，这个概率上界也可能会变得很大。
    *   **直观解释**: 当你用足够多的钥匙去试一把锁时，总有一把钥匙能“碰巧”打开它，即使那不是正确的钥匙。同理，当你用足够多的模型去“拟合”一个固定的验证集时，总会有一个模型能“碰巧”在该验证集上获得高分，但这很可能只是因为该模型的随机特性恰好契合了验证集的随机噪声，而非它学到了真正的规律。

## **1.3 如何避免对验证集过拟合？**

1.  **严格遵守数据划分**:
    *   **训练集 (Training Set)**: 只用于训练模型参数。
    *   **验证集 (Validation Set)**: 只用于模型选择和超参数调优。**不要用它来训练模型参数！**
    *   **测试集 (Test Set)**: **“保险箱里的最终考卷”**。在整个模型开发周期中，它只能被使用**一次**，用于报告模型的最终性能。它的存在就是为了检测是否对验证集发生了过拟合。如果在验证集上得分95%，但在测试集上只有85%，说明很可能发生了过拟合。

2.  **限制调优次数**:
    不要进行无休止的、地毯式的超参数搜索。有节制地选择几个有代表性的候选模型进行比较。

3.  **使用交叉验证 (Cross-Validation)**:
    *   对于数据集较小的情况，可以使用K折交叉验证。它将训练数据分成K份，轮流使用其中一份作为验证集，其余K-1份作为训练集，最后将K次的结果平均。
    *   这相当于用多个不同的验证集来评估模型，使得评估结果更稳定、更鲁棒，降低了对单一固定验证集过拟合的风险。

**结论**: 验证集是一个强大的工具，但不是万能的。它本身也是从真实数据分布中抽取的一个样本，同样存在被“过度利用”而导致信息泄露和过拟合的风险。理解这一点，并严格遵守数据使用纪律，是成为一名优秀机器学习工程师的关键。