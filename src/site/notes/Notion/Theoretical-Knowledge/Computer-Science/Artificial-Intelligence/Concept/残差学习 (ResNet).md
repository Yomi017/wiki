---
{"dg-publish":true,"permalink":"/notion/theoretical-knowledge/computer-science/artificial-intelligence/concept/res-net/"}
---


### 1. 背景：深度网络的“退化”问题 (The Degradation Problem)

**引言 (Introduction):**
在ResNet出现之前，深度学习领域的一个普遍认知是：网络越深，通过更多的层级，模型就能学习到越复杂、越抽象的特征，从而性能会越好。然而，实验发现，当网络深度增加到一定程度后，再继续加深网络，模型的性能反而会下降。这种现象被称为**“网络退化” (Degradation)**。

*   **注意**: 这不是**过拟合 (Overfitting)**。过拟合是指模型在训练集上表现好，但在测试集上表现差。而“退化”是指模型在**训练集和测试集上**的表现都变差了。
*   **理论与实践的矛盾**: 理论上，一个更深的模型至少应该能达到和一个浅层模型相同的性能。具体做法是：让新增加的层学习成为一个**恒等映射 (Identity Mapping)**，即直接把前一层的输出原封不动地传给下一层，这样深层模型就退化成了一个浅层模型。但实践中，让神经网络去拟合一个恒等映射都非常困难。

**ResNet的目标**：解决网络退化问题，使得构建真正有效的、非常深的网络成为可能。

---

### 2. 核心思想：残差学习 (Residual Learning)

ResNet的作者们提出了一个天才般的想法：**与其让网络层直接学习一个理想的底层映射 `H(x)`，不如让它学习这个映射与输入 `x` 之间的“差值”，即残差 (Residual)。**

#### 2.1 残差块 (Residual Block) 的结构

为了实现残差学习，ResNet设计了**残差块 (Residual Block)**。

![[Image/Computer-Science/Deep-Learning/ResNet_Block.png\|Image/Computer-Science/Deep-Learning/ResNet_Block.png]]

*   **主路 (Main Path)**: 输入 `x` 经过一系列的卷积层和激活函数，得到一个特征变换结果。我们称这个结果为 `F(x)`。
*   **快捷连接 (Shortcut Connection / Skip Connection)**: 输入 `x` 同时通过一条“捷径”，不经过任何处理（或只经过一个简单的线性变换以匹配维度），直接到达主路的末端。
*   **汇合点**: 在末端，主路的输出 `F(x)` 与快捷连接的输出 `x` 进行**逐元素相加**。这个块的最终输出被定义为 `H(x) = F(x) + x`。

#### 2.2 为什么学习残差更容易？

假设在某种情况下，最优的映射就是一个恒等映射，即 `H(x) = x`。

*   **对于传统网络**: 它需要费力地调整卷积层中的所有权重，去拟合一个 `H(x) = x` 的函数。
*   **对于残差网络**: 由于最终输出是 `H(x) = F(x) + x`，它只需要让 `F(x) = 0` 即可。让一个网络的权重趋向于0（从而输出为0），远比让它拟合一个恒等函数要容易得多。

**核心优势**：ResNet通过这种结构，为网络提供了一条“什么都不做”的捷径。如果增加的层是有益的，网络就会通过主路学习到有用的特征 `F(x)`；如果增加的层是无益的，网络就可以轻易地让 `F(x)` 趋近于0，从而退化为恒等映射，至少保证性能不会下降。

---

### 3. 深入解答：关于“加上x”的核心疑惑

这里我们来详细剖析“为什么加上x”以及整个学习流程，解决之前的疑惑。

#### 3.1 疑惑一：为什么最后要“画蛇添足”地加上`x`？

**回答：`+x` 不是在学习任务完成后的“画蛇添足”，而是整个学习任务的“前提和定义”。**

让我们对比一下设计者的思路：

*   **传统CNN设计者的思路**：
    1.  我需要一个网络块来学习目标 `H(x)`。
    2.  我把这个网络块的**输出定义为 `H(x)`**。
    3.  所以，这个网络块的**任务就是直接学习 `H(x)`**。

*   **ResNet设计者的思路**：
    1.  我需要一个网络块来学习目标 `H(x)`。
    2.  我把这个网络块的**输出重新定义为 `F(x) + x`**，其中 `F(x)` 是卷积层的输出。
    3.  所以，这个网络块里的卷积层的**任务就变成了学习残差 `F(x) = H(x) - x`**。

**关键洞察**：**`+x` 这个操作是整个残差块结构定义的一部分，它先于学习过程。正是因为结构上规定了最后必须 `+x`，才使得中间卷积层的学习目标从困难的 `H(x)` 变成了简单的 `F(x)`。** 它不是一个后处理步骤，而是设计的基石。

#### 3.2 疑惑二：学习流程到底是怎样的？是先学再减再加吗？

**回答：不是。实际流程由网络结构决定，学习过程（反向传播）会自动适应这个结构。**

让我们想象一下**一个神经元（或一层权重）**在训练时会“思考”什么：

1.  **前向传播 - 我做了什么？**
    *   “我（权重W）接收了输入`x`，经过计算输出了`F(x)`。在我的下游，有人把我的输出`F(x)`和原始的`x`加了起来，得到了`H(x)`。”

2.  **反向传播 - 我收到了什么指令？**
    *   “网络的最终端传来一个误差信号（梯度），这个信号告诉我，`H(x)` 需要进行一些调整才能减小总误差。”
    *   “因为 `H(x) = F(x) + x`，这个调整指令会同时传递给我产生的 `F(x)` 和原始的 `x`。”
    *   “我（权重W）收到的指令是：‘请调整你自己，让你产生的 `F(x)` 发生改变，从而让 `F(x)+x` 更接近期望的目标。’”

3.  **学习与适应 - 我该怎么做？**
    *   “我的任务不是凭空创造出一个完美的结果。我的任务是观察原始的`x`，然后思考我应该输出一个怎样的**修正量 `F(x)`**，才能让 `F(x) + x` 的组合变得更好。”
    *   “哦，所以我天生的使命就是学习那个‘差值’！”

**结论**: 整个过程中，没有“减去x”这个步骤。**“学习残差”是网络在反向传播过程中，为了最小化最终损失而自动采取的最优策略**，而这种策略之所以可行，完全得益于前向传播中 `+x` 这个结构的存在。

#### 3.3 “加上x”的另一大好处：解决梯度消失

在反向传播中，梯度需要通过链式法则逐层传递。对于 `H(x) = F(x) + x`，它的梯度是：

$$ \frac{\partial J}{\partial x} = \frac{\partial J}{\partial H(x)} \cdot \frac{\partial H(x)}{\partial x} = \frac{\partial J}{\partial H(x)} \cdot \left( \frac{\partial F(x)}{\partial x} + 1 \right) $$

*   **`+1`** 这一项至关重要。它相当于为梯度信号开辟了一条**“高速公路”**。
*   即使主路 `F(x)` 的梯度 `∂F(x)/∂x` 因为网络过深而变得非常小（接近0），梯度信号仍然可以通过这条带有 `+1` 的快捷连接无损地向前传播。
*   这从根本上缓解了深度网络中的**梯度消失问题**，保证了即使是靠前的网络层也能接收到有效的训练信号。

---

### 4. 总结

ResNet通过其创新的**残差块**和**快捷连接**设计：
1.  将困难的**直接映射学习**任务，转化为更简单的**残差学习**任务。
2.  通过结构性地引入 `+x`，巧妙地解决了深度网络的**退化问题**。
3.  通过为梯度提供“直连通道”，有效缓解了**梯度消失**问题。

这些革命性的贡献使得构建和训练数百甚至上千层的超深度神经网络成为现实，并极大地推动了深度学习领域的发展。现代几乎所有先进的CNN架构都深受ResNet思想的影响。