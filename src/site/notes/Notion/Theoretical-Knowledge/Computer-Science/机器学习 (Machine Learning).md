---
{"dg-publish":true,"permalink":"/notion/theoretical-knowledge/computer-science/machine-learning/"}
---

# 1. 基本概念 (Basic Concepts)

## 1.1 主要机器学习任务类型 (Major Types of Machine Learning Tasks)

机器学习算法可以根据其学习目标和所处理的数据类型大致分为以下几类：

1.  **监督学习 (Supervised Learning)**
    *   **目标**: 从带标签的训练数据（即每个数据点都有一个已知的“答案”或“目标输出”）中学习一个映射函数，以便对新的、未见过的数据进行预测。
    *   **子类型**:
        *   **回归 (Regression)**:
            *   **目标输出**: 连续的数值 (a continuous scalar value)。
            *   **例子**: 预测房价、股票价格、温度。
        ![Image/Machine Learning/1.png](/img/user/Image/Machine%20Learning/1.png)
        *   **分类 (Classification)**:
            *   **目标输出**: 离散的类别标签 (a discrete class label) from a predefined set.
            *   **例子**: 图像识别（猫/狗）、邮件分类（垃圾/非垃圾）、疾病诊断（有病/无病）。![Image/Machine Learning/2.png](/img/user/Image/Machine%20Learning/2.png)
            ![3.jpg](/img/user/Image/Machine%20Learning/3.jpg)

2.  **无监督学习 (Unsupervised Learning)**
    *   **目标**: 从未带标签的数据中发现隐藏的模式、结构或关系。算法自行探索数据。
    *   **子类型**:
        *   **聚类 (Clustering)**:
            *   **目标**: 将数据点分组成相似的集合（簇），使得同一簇内的数据点相似度高，不同簇之间的数据点相似度低。
            *   **例子**: 客户分群、异常检测。
        *   **降维 (Dimensionality Reduction)**:
            *   **目标**: 减少数据特征的数量，同时保留重要信息，以便于可视化、提高效率或减少噪声。
            *   **例子**: 主成分分析 (PCA)、t-SNE。
        *   **关联规则学习 (Association Rule Learning)**:
            *   **目标**: 发现数据项之间的有趣关系或关联。
            *   **例子**: 购物篮分析（“购买面包的人也倾向于购买牛奶”）。
        *   **(概率)结构学习 (Probabilistic Structure Learning / Graphical Model Learning)
        *   ***(部分属于此类)***:
            *   **目标**: 发现一组随机变量之间的概率依赖关系，并用图结构（如贝叶斯网络、马尔可夫网络）来表示这些关系。
            *   **特性**: 当没有预先指定变量间的关系，而是从数据中推断这些关系时，这通常被视为一种无监督的发现过程。它可以帮助理解数据的内在结构和生成机制。
            *   **例子**: 从基因表达数据中推断基因调控网络，从传感器数据中学习变量间的依赖。

3.  **强化学习 (Reinforcement Learning)**
    *   **目标**: 智能体 (agent) 通过与环境 (environment) 交互来学习如何做出决策，以最大化累积奖励 (cumulative reward)。智能体通过试错来学习最优策略。
    *   **例子**: 训练机器人行走、棋类游戏AI (AlphaGo)、自动驾驶策略。


明白你的意思了！你是希望在描述完 "1.2.1 定义模型" 和 "1.2.2 定义损失" 之后，**统一地**阐述这两者整体与深度学习的关系，而不是分开在各自小节的末尾分别论述。

好的，我们来调整一下结构，将“与深度学习的关系”作为一个总结性的部分，放在这两个小节之后。

---

## 1.2 机器学习的步骤 (Steps in Machine Learning)

### 1.2.1 定义一个带有未知参数的函数/模型 (Define a Function/Model with Unknown Parameters)

机器学习的核心任务之一是从数据中学习一个函数（或模型），该函数能够很好地描述输入和输出之间的关系，或者发现数据中的潜在结构。这个函数通常包含一些**未知参数 (unknown parameters)**，这些参数的值需要从训练数据中学习得到。

以一个简单的线性回归模型为例：
$$y = b + w x_1$$
这里：
*   $y$ 是我们想要预测的目标输出 (target output)。
*   $x_1$ 是一个输入特征 (input feature)。
*   $w$ (权重, weight) 和 $b$ (偏置, bias) 是模型的**未知参数**。我们的目标就是通过学习算法，利用训练数据来找到最优的 $w$ 和 $b$ 的值。

### 1.2.2 定义代价函数/损失函数以评估模型 (Define Cost/Loss Function to Evaluate the Model)

在定义了带有未知参数的模型之后，我们需要一种方法来**衡量模型的预测结果与真实目标值之间的差异**。这个衡量标准就是**代价函数 (Cost Function)** 或 **损失函数 (Loss Function)**。代价函数的值反映了当前模型参数的好坏：代价越小，模型对训练数据的拟合越好。

**代价函数是参数的函数 (Loss is a function of parameters):**
给定训练数据集，对于一组特定的模型参数（例如 $w$ 和 $b$），我们可以计算出模型在整个训练集上的总体表现。因此，代价函数 $L$ 可以看作是这些未知参数的函数。例如，对于参数 $w$ 和 $b$，代价函数可以表示为 $L(w, b)$。

$$L(w, b) = \frac{1}{N} \sum_{n=1}^{N} e_n$$
这里：
*   $N$ 是训练样本的总数。
*   $e_n$ 是模型在第 $n$ 个训练样本上的**误差 (error)** 或 **损失 (loss)**。
*   代价函数 $L(w, b)$ 是所有单个样本损失的平均值（或总和）。

**常见的单个样本损失计算方式 ($e_n$)**:
*   **平均绝对误差 (Mean Absolute Error, MAE)**:
    $$e_n = |\hat{y}^{(n)} - y^{(n)}|$$
*   **均方误差 (Mean Squared Error, MSE)**:
    $$e_n = (\hat{y}^{(n)} - y^{(n)})^2$$
*   **[[Notion/Theoretical-Knowledge/Computer-Science/Concept/交叉熵 (Cross-Entropy)\|交叉熵 (Cross-Entropy)]]**:
    常用于分类问题，衡量预测概率分布与真实类别分布之间的差异。具体形式取决于二分类还是多分类。
    *   **二分类交叉熵 (Binary Cross-Entropy)**:
        $e_n = - [y^{(n)} \log(\hat{y}^{(n)}) + (1 - y^{(n)}) \log(1 - \hat{y}^{(n)})]$
        (其中 $y^{(n)} \in \{0, 1\}$, $\hat{y}^{(n)}$ 是预测为类别1的概率)
    *   **多分类交叉熵 (Categorical Cross-Entropy)**:
        $e_n = - \sum_{k=1}^{K} y_k^{(n)} \log(\hat{y}_k^{(n)})$
        (其中 $y^{(n)}$ 是one-hot编码的真实标签, $\hat{y}^{(n)}$ 是预测的概率分布, $K$ 是类别数)

**损失函数的作用 (The Role of the Loss Function):**
损失函数告诉我们**当前这组参数 $(w, b)$ 的表现有多好 (how good a set of values is)**。我们的目标是找到一组参数，使得这个损失函数的值最小。

**误差平面 (Error Surface):**
我们可以将损失函数 $L(w, b)$ 想象成一个多维空间中的曲面，其中参数（如 $w$ 和 $b$）是坐标轴，损失函数的值是高度。这个曲面被称为**误差平面 (Error Surface)** 或损失平面。
![4.png](/img/user/Image/Machine%20Learning/4.png) *(图示：损失值如何随着参数变化而变化，目标是找到曲面的最低点)*

---

### 1.2.4 定义模型与定义损失函数和深度学习的关系 (Relationship of Defining Model & Loss to Deep Learning)

上述两个核心步骤——**1. 定义一个带有未知参数的模型** 和 **2. 定义一个损失函数来评估模型**——构成了监督式机器学习的基础框架，并且这一框架在**深度学习**中得到了直接的应用和显著的扩展：

1.  **参数化模型的核心思想一致 (Consistent Core Idea of Parameterized Models):**
    *   无论是简单的线性回归 ($y = b + wx_1$) 还是复杂的深度神经网络，其本质都是**参数化的函数/模型**。它们都包含大量需要从数据中学习的未知参数（权重和偏置）。
    *   深度学习模型，如神经网络，可以看作是这种参数化函数的一种高度复杂和灵活的实现。它们通过多层非线性变换构建出表达能力极强的函数，能够拟合非常复杂的数据模式。

2.  **损失函数作为统一的评估和优化目标 (Loss Function as a Unified Goal for Evaluation and Optimization):**
    *   对于任何参数化模型（包括深度神经网络），都需要一个**损失函数**来量化其预测与真实目标之间的差距。这个损失函数是模型参数的函数。
    *   在深度学习中，训练的目标同样是找到一组使损失函数最小化的参数。由于深度学习模型参数众多（可达数百万甚至数十亿），损失函数在高维参数空间中形成的“误差平面”会异常复杂。

3.  **深度学习的扩展与深化 (Extensions and Deepening in Deep Learning):**
    *   **模型复杂度**: 深度学习通过构建**深层网络结构**（多个隐藏层）极大地扩展了模型的复杂度。每一层都包含参数，使得模型能够学习从低级到高级的层次化特征表示。例如，在图像识别中，浅层可能学习边缘和角点，深层则学习物体的部件乃至整个物体。
    *   **非线性能力**: 深度学习模型广泛使用**非线性激活函数**（如 ReLU, Sigmoid, Tanh），这使得它们能够学习输入和输出之间高度非线性的映射关系，远超简单线性模型的能力。
    *   **特定任务的损失函数**: 针对深度学习应用的各种复杂任务（如目标检测、语义分割、机器翻译、语音生成等），研究者们设计了各种精巧的损失函数。例如，在图像生成中可能会使用结合了像素级损失和感知损失 (perceptual loss) 的复杂损失函数，或者在强化学习中优化累积奖励。
    *   **端到端学习 (End-to-End Learning)**: 深度学习常常实现“端到端”学习，即从原始输入直接学习到最终输出，中间的特征提取和转换过程都由网络参数通过最小化损失函数自动学习得到，而不是像传统机器学习那样需要大量手工特征工程。

**总结来说，"定义模型" 和 "定义损失函数" 这两个步骤是机器学习的基本配方。深度学习继承并极大地丰富了这个配方：它使用更深、更复杂的参数化模型（神经网络）来处理更复杂的数据和任务，并依赖于精心设计的损失函数和强大的优化算法（如基于梯度的优化）来在巨大的参数空间中学习这些模型的参数。因此，理解这两个基本步骤对于理解深度学习的原理至关重要。**
