---
{"dg-publish":true,"permalink":"/wiki/notion/theoretical-knowledge/computer-science/paper-intensive-reading/2/"}
---

# 1. 中文翻译：
### BEVFormer v2：通过透视视角监督使现代图像主干网络适配鸟瞰图识别

陈宇杨¹*，陈云韬²*，田皓³*，陶晨新¹，朱西周³，张兆翔²⁴
黄高¹，李弘扬⁵，乔宇⁵，吕乐为³
周杰¹
戴季峰¹⁵B

¹清华大学 ²中国科学院香港创新研究院人工智能与机器人研究中心
³商汤科技研究院 ⁴中国科学院自动化研究所 (CASIA)
⁵上海人工智能实验室

{yangcy19, tcx20}@mails.tsinghua.edu.cn, chenyuntao08@gmail.com, tianhao2@senseauto.com
{zhuwalter, luotto}@sensetime.com, zhaoxiang.zhang@ia.ac.cn
{gaohuang, jzhou, daijifeng}@tsinghua.edu.cn, {lihongyang, qiaoyu}@pjlab.org.cn

**摘要**

我们提出了一种带有透视视角监督的新型鸟瞰图（BEV）检测器，它收敛更快，并且更适合现代图像主干网络。现有的SOTA（state-of-the-art）BEV检测器通常与特定的深度预训练主干网络（如VoVNet）绑定，这阻碍了蓬勃发展的图像主干网络与BEV检测器之间的协同作用。为解决这一局限性，我们通过引入透视视角监督，优先简化BEV检测器的优化过程。为此，我们提出了一个两阶段BEV检测器，其中来自透视头的提案被送入鸟瞰图头以进行最终预测。为了评估我们模型的有效性，我们进行了广泛的消融实验，重点关注监督的形式以及所提出检测器的泛化性。所提出的方法在多种传统和现代图像主干网络上得到了验证，并在大规模nuScenes数据集上取得了新的SOTA结果。代码将很快发布。

---

### 1. 引言

鸟瞰图（BEV）识别模型 [17, 22, 26, 28, 30, 36, 43] 在自动驾驶领域引起了广泛关注，因为它们可以自然地将来自多个传感器的局部原始观测数据整合成一个统一的、全局性的3D输出空间。

一个典型的BEV模型建立在图像主干网络之上，其后跟随一个视图转换模块，该模块将透视视角的图像特征提升（lift）到BEV特征，这些特征再由BEV特征编码器和一些特定任务的头进行处理。尽管在设计视图转换模块 [17, 28, 43] 和将日益增多的下游任务 [9, 28] 整合到新的识别框架中投入了大量精力，但对BEV模型中图像主干网络的研究却远未受到足够关注。作为一个前沿且要求极高的领域，将现代图像主干网络引入自动驾驶是理所当然的。然而令人惊讶的是，研究界选择固守于VoVNet [13]，以享受其大规模深度预训练 [27] 带来的好处。在这项工作中，我们专注于释放现代图像特征提取器在BEV识别中的全部潜力，为未来研究人员在该领域探索更好的图像主干网络设计打开大门。

然而，简单地使用那些未经适当预训练的现代图像主干网络并不能产生令人满意的结果。例如，一个在ImageNet [6]上预训练的ConvNeXt-XL [24] 主干网络在3D物体检测上的表现仅与一个在DDAD-15M上预训练的VoVNet-99 [27] 相当，尽管前者的参数量是后者的3.5倍。我们将现代图像主干网络适配困难的原因归结为以下几个问题：1) 自然图像与自动驾驶场景之间的领域差异。在通用2D识别任务上预训练的主干网络缺乏感知3D场景的能力，尤其是在深度估计方面。2) 当前BEV检测器的复杂结构。以BEVFormer [17]为例，3D边界框和物体类别标签的监督信号与图像主干网络之间被视图编码器和物体解码器隔开，而这两者都由多层Transformer组成。用于适配通用2D图像主干网络以服务于自动驾驶任务的梯度流被堆叠的Transformer层所扭曲。

为了克服上述在将现代图像主干网络适配于BEV识别中遇到的困难，我们在BEVFormer中引入了**透视视角监督**，即来自透视视角任务的额外监督信号，并直接应用于主干网络。它引导主干网络学习2D识别任务中缺失的3D知识，并克服了BEV检测器的复杂性，极大地促进了模型的优化。具体来说，我们在主干网络上构建了一个透视视角3D检测头 [27]，它以图像特征为输入，直接预测目标物体的3D边界框和类别标签。这个透视头的损失，表示为**透视损失**，作为辅助检测损失，被加到源自BEV头的原始损失（**BEV损失**）中。这两个检测头通过它们各自对应的损失项进行联合训练。此外，我们发现将这两个检测头组合成一个两阶段的BEV检测器——**BEVFormer v2**——是很自然的做法。由于透视头功能完备，它可以在透视视角下生成高质量的物体提案，我们将其用作第一阶段的提案。我们将它们编码为物体查询，并与原始BEVFormer中的可学习查询一起，形成**混合物体查询**，然后送入第二阶段的检测头以生成最终预测。

我们进行了广泛的实验，以证实我们提出的透视视角监督的有效性和必要性。透视损失促进了图像主干网络的适配，从而提高了检测性能并加快了模型收敛。而如果没有这种监督，即使使用更长的训练计划，模型也无法达到相当的结果。因此，我们成功地将现代图像主干网络适配到BEV模型中，在nuScenes [2] 测试集上达到了63.4%的NDS。

我们的贡献可以总结如下：
*   我们指出，透视视角监督是将通用2D图像主干网络适配到BEV模型的关键。我们通过在透视视角中增加一个检测损失来显式地添加这种监督。
*   我们提出了一种新颖的两阶段BEV检测器，BEVFormer v2。它由一个透视视角3D检测头和一个BEV检测头组成，前者的提案与后者的物体查询相结合。
*   我们通过将我们的方法与最新开发的图像主干网络相结合，并在nuScenes数据集上取得了比以往SOTA结果显著的提升，从而突显了我们方法的有效性。

---
*: 同等贡献。
B: 通讯作者。
---

### 2. 相关工作

#### 2.1. BEV 3D物体检测器

鸟瞰图（BEV）物体检测由于其在自动驾驶系统中的巨大成功，近来吸引了越来越多的关注 [17, 22, 26, 28, 30, 36, 43]。

早期的工作，包括OFT [30]、Pseudo LiDAR [36]和VPN [26]，阐明了如何将透视特征转换为BEV特征，但它们要么是针对单个相机，要么是用于不太知名的任务。OFT [30] 开创性地采用了从2D图像特征到3D BEV特征的转换为单目3D物体检测。Pseudo LiDAR [36]，顾名思义，通过单目深度估计和相机内参创建伪点云，并随后在BEV空间中处理它们。VPN [26] 是第一个将多视角相机输入融合成一个俯视视角特征图用于语义分割的方法。

现代方法得益于2D-3D视图转换为整合来自不同透视视角传感器的特征提供了便利。LSS [28] 通过在池化BEV柱状特征时引入潜在深度分布，扩展了OFT。此外，LSS对六个环视图像进行池化，而OFT中只有一个。与LSS中的2D到3D提升或OFT中的3D到2D投影不同，CVT [43] 利用相机感知的位​​置编码和密集的交叉注意力来桥接透视视角和BEV视角的特征。PETR [22] 设计了一种无需显式构建BEV特征的方法。透视特征图与3D位置嵌入特征图逐元素融合，随后应用一个DETR风格的解码器进行物体检测。BEVFormer [17] 利用空间交叉注意力进行视图转换，并利用时间自注意力进行时间特征融合。BEVFormer完全基于Transformer的结构使其BEV特征比其他方法更具通用性，可以轻松支持非均匀和非规则的采样网格。此外，如SimpleBEV [7]所示，多尺度可变形注意力 [44] 在所有提升策略中表现出色。因此，我们选择基于BEVFormer构建我们的检测器，以利用上述优点。

除了已发表的工作，由于该领域的流行，还有许多同期的工作。BEVDet [10] 引入了丰富的图像级和BEV级增强进行训练。BEVStereo [14] 和STS [38] 都采用了时间立体匹配的范式以获得更好的深度估计。PolarFormer [11] 提出了一个非笛卡尔坐标的3D网格设置。SimpleBEV [7] 比较了不同的2D-3D提升方法。与现有主要探索检测器设计的工作不同，我们专注于将现代图像主干网络适配到BEV识别模型中。

#### 2.2. 相机3D物体检测中的辅助损失

辅助损失在单目3D物体检测中普遍存在，因为大多数方法 [15, 21, 27, 31, 33, 34, 41] 都建立在像RetinaNet [19]和FCOS [32]这样的2D检测器之上。但那些辅助损失很少为2D监督赋予任何明确的含义。MonoCon [21] 通过利用多达5种不同的2D监督，充分利用了2D辅助。至于BEV检测器，BEVDepth [15] 利用激光雷达点云来监督其-中间的深度网络。MV-FCOS3D++ [33] 引入了透视视角监督来训练其图像主干网络，但检测器本身仅由BEV损失监督。SimMOD [42] 为其单目提案头使用了2D辅助损失。

与以往的方法不同，我们的方法采用了一种端到端的透视视角监督方法，而无需使用如激光雷达点云等额外数据。

#### 2.3. 两阶段3D物体检测器

尽管两阶段检测器在基于激光雷达的3D物体检测中很常见 [1, 5, 12, 16, 29, 39, 42]，但它们在基于相机的3D检测中的应用却远不为人知。MonoDIS [31] 使用RoIAlign从2D框中提取图像特征，并随后回归3D框。SimMOD [42] 采用一个单目3D头来生成提案，并用一个DETR3D [37] 头进行最终检测。然而，在两个阶段都使用来自透视主干网络的相同特征，并没有为第二阶段的头提供信息增益。我们认为这是两阶段检测器在基于相机的3D检测中远不那么流行的主要原因。相反，我们的两阶段检测器利用来自透视和BEV两个视角的特征，因此能同时利用图像空间和BEV空间的信息。

---

### 3. BEVFormer v2

将现代2D图像主干网络适配于BEV识别，而无需繁琐的深度预训练，可以为下游的自动驾驶任务解锁许多可能性。在这项工作中，我们提出了BEVFormer v2，一个两阶段的BEV检测器，它结合了BEV和透视视角监督，以便轻松地在BEV检测中引入图像主干网络。

#### 3.1. 整体架构

如图1所示，BEVFormer v2主要由五个部分组成：一个图像主干网络、一个透视视角3D检测头、一个空间编码器、一个改进的时间编码器和一个BEV检测头。与原始BEVFormer [17]相比，除了空间编码器外，所有组件都进行了更改。具体来说，BEVFormer v2中使用的所有图像主干网络都没有在任何自动驾驶数据集或深度估计数据集上进行预训练。引入了一个透视视角3D检测头，以促进2D图像主干网络的适配，并为BEV检测头生成物体提案。采用了一个新的时间BEV编码器，以更好地融合长期时间信息。BEV检测头现在接受一组混合的物体查询作为输入。我们结合第一阶段的提案和可学习的物体查询，形成了新的混合物体查询，供第二阶段使用。

**(图1. BEVFormer v2的整体架构。图像主干网络生成多视角图像的特征。透视视角3D头进行透视预测，然后编码为物体查询。BEV头是编码器-解码器结构。空间编码器通过聚合多视角图像特征生成BEV特征，随后时间编码器收集历史BEV特征。解码器以混合物体查询为输入，并基于BEV特征进行最终的BEV预测。整个模型由两个检测头的两个损失项L_pers和L_bev共同训练。)**

#### 3.2. 透视视角监督

我们首先分析鸟瞰图模型的问题，以解释为什么需要额外的监督。一个典型的BEV模型维护附着在BEV平面上的网格状特征，其中每个网格聚合了来自多视角图像相应2D像素特征的3D信息。它基于BEV特征预测目标物体的3D边界框，我们称这种施加在BEV特征上的监督为**BEV监督**。

以BEVFormer [17]为例，它使用编码器-解码器结构来生成和利用BEV特征。编码器为BEV平面上的每个网格单元分配一组3D参考点，并将它们投影到多视角图像上作为2D参考点。之后，它对2D参考点周围的图像特征进行采样，并利用空间交叉注意力将它们聚合成BEV特征。解码器是一个Deformable DETR [44]头，它用少量固定的物体查询在BEV坐标系中预测3D边界框。图2显示了由3D到2D视图转换和DETR [3]头引入的BEV监督的两个潜在问题：
*   **监督相对于图像特征是隐式的。** 损失直接应用于BEV特征，而在经过3D到2D投影和对图像特征的注意力采样后，它变得间接。
*   **监督对于图像特征是稀疏的。** 只有少数被物体查询关注的BEV网格对损失有贡献。因此，只有那些网格的2D参考点周围的稀疏像素获得监督信号。

**(图2. 透视视角监督(a)和BEV监督(b)的比较。透视检测器的监督信号是密集的，并直接作用于图像特征，而BEV检测器的监督信号是稀疏和间接的。)**

因此，在训练中出现了不一致性：BEV检测头依赖于图像特征中包含的3D信息，但它为骨干网络如何编码这些信息提供的指导不足。

以往的BEV方法并没有严重受到这种不一致性的影响，它们甚至可能没有意识到这个问题。这是因为它们的主干网络要么规模相对较小，要么已经在一个单目检测头上进行了3D检测任务的预训练。与BEV头相反，透视视角3D头对图像特征进行逐像素的预测，为适配2D图像主干网络提供了更丰富的监督信号。我们将这种施加在图像特征上的监督定义为**透视视角监督**。如图2所示，与BEV监督不同，透视检测损失是直接且密集地应用于图像特征的。我们认为，透视视角监督明确地引导主干网络感知3D场景并提取有用的信息，例如物体的深度和方向，从而克服了BEV监督的缺点，因此在用现代图像主干网络训练BEV模型时至关重要。

#### 3.3. 透视损失

如前一节分析，透视视角监督是优化BEV模型的关键。在BEVFormer v2中，我们通过一个辅助的**透视损失**引入了透视视角监督。具体来说，在主干网络之上构建了一个透视视角3D检测头，用于在透视视角下检测目标物体。我们采用了一个类似FCOS3D [34]的检测头，它预测3D边界框的中心位置、尺寸、方向和投影中心度。该头的检测损失，表示为L_pers，作为BEV损失L_bev的补充，促进了主干网络的优化。整个模型通过总目标函数进行训练：
`L_total = λ_bev * L_bev + λ_pers * L_pers` (1)

#### 3.4. 改进的时间编码器

BEVFormer使用循环时间自注意力来融合历史BEV特征。但该时间编码器未能充分利用长期时间信息，简单地将循环步数从4增加到16并没有带来额外的性能提升。

我们通过使用一个简单的变换和拼接策略，为BEVFormer v2重新设计了时间编码器。给定一个不同帧k的BEV特征Bk，我们首先根据帧t和帧k之间的参考帧变换矩阵T_k^t = [R|t] ∈ SE³，将Bk双线性变换（warp）到当前帧，得到Bt_k。然后，我们将之前的BEV特征与当前的BEV特征沿通道维度拼接，并使用残差块进行维度缩减。为了保持与原始设计相似的计算复杂度，我们使用相同数量的历史BEV特征，但增加了采样间隔。除了受益于长期时间信息外，新的时间编码器也为在离线3D检测设置中利用未来BEV特征开启了可能性。

#### 3.5. 两阶段BEV检测器

尽管联合训练两个检测头已经提供了足够的监督，但我们从不同视角分别获得了两组检测结果。我们没有只取BEV头的预测而丢弃透视头的预测，也没有通过NMS启发式地组合两组预测，而是设计了一种新颖的结构，将两个头整合成一个两阶段的预测流程，即**两阶段BEV检测器**。BEV头中的物体解码器，一个DETR [3]解码器，使用一组可学习的嵌入作为物体查询，这些查询通过训练学习目标物体可能的位置。然而，随机初始化的嵌入需要很长时间才能学习到合适的位置。此外，可学习的物体查询在推理时对所有图像都是固定的，这可能不够准确，因为物体的空间分布可能会变化。为了解决这些问题，透视头的预测经过后处理筛选，然后融入解码器的物体查询中，形成一个两阶段过程。这些**混合物体查询**提供了高分（概率）的候选位置，使得BEV头在第二阶段更容易捕获目标物体。带有混合物体查询的解码器细节将在稍后描述。值得注意的是，第一阶段的提案不一定来自透视检测器，例如可以来自另一个BEV检测器，但实验表明，只有来自透视视角的预测对第二阶段的BEV头有帮助。

#### 3.6. 带有混合物体查询的解码器

为了将第一阶段的提案融入第二阶段的物体查询中，BEVFormer v2中BEV头的解码器在BEVFormer [17]使用的Deformable DETR [44]解码器基础上进行了修改。该解码器由堆叠的交替自注意力和交叉注意力层组成。

交叉注意力层是一个可变形注意力模块 [44]，它接收以下三个元素作为输入：（1）**内容查询**（Content queries），用于产生采样偏移和注意力权重的查询特征。（2）**参考点**（Reference points），值特征上的2D点，作为每个查询的采样参考。（3）**值特征**（Value features），被关注的BEV特征。在原始的BEVFormer [17]中，内容查询是一组可学习的嵌入，参考点是通过一个线性层从一组可学习的位置嵌入中预测出来的。在BEVFormer v2中，我们从透视头获得提案，并通过后处理选择其中一部分。如图3所示，所选提案在BEV平面上的投影框中心被用作**逐图像的参考点**，并与从位置嵌入生成的**逐数据集的参考点**相结合。逐图像的参考点直接指明了物体在BEV平面上可能的位置，使解码器更容易检测到目标物体。然而，一小部分物体可能由于遮挡或出现在两个相邻视图的边界而未被透视头检测到。为避免漏掉这些物体，我们也保留了原始的逐数据集的参考点，通过学习空间先验来捕获它们。

**(图3. BEVFormer v2中BEV头的解码器。第一阶段提案的投影中心被用作逐图像的参考点（紫色点），它们与逐数据集的可学习内容查询和位置嵌入（蓝色部分）结合，形成混合物体查询。)**

---

### 4. 实验

#### 4.1. 数据集和指标
nuScenes 3D检测基准 [2] 包含1000个约20秒长的多模态视频，关键样本以2Hz的频率进行标注。每个样本包含来自6个摄像头的图像，覆盖了完整的360度视野。视频被分为700个用于训练，150个用于验证，150个用于测试。检测任务包含140万个已标注的10类物体的3D边界框。nuScenes使用地面平面上的中心距离，在四个不同阈值下计算平均精度均值（mAP），并包含五个真阳性指标，即ATE、ASE、AOE、AVE和AAE，分别用于衡量平移、尺度、方向、速度和属性误差。此外，它还通过结合检测精度（mAP）和五个真阳性指标定义了nuScenes检测分数（NDS）。

#### 4.2. 实验设置
我们使用多种类型的主干网络进行实验：ResNet [8]、DLA [40]、VoVNet [13]和InternImage [35]。所有主干网络都使用在COCO数据集 [20]的2D检测任务上预训练的检查点进行初始化。除了我们的修改，我们遵循BEVFormer [17]的默认设置来构建BEV检测头。在表1和表6中，BEV头利用新的时间编码器来使用时间信息。对于其他实验，我们采用单帧版本，即仅使用当前帧，类似BEVFormer-S [17]。对于透视视角3D检测头，我们采用了DD3D [27]中的实现，并带有相机感知的深度参数化。透视损失和BEV损失的权重设置为λ_bev = λ_pers = 1。我们使用AdamW [25]优化器，并将基础学习率设置为4e-4。

#### 4.3. 基准测试结果
我们将我们提出的BEVFormer v2与现有的SOTA BEV检测器进行比较，包括BEVFormer [17]、PolarFormer [11]、PETRv2 [23]、BEVDepth [15]和BEVStereo [14]。我们在表1中报告了在nuScenes测试集上的3D物体检测结果。BEVFormer、PolarFormer、BEVDepth和BEVStereo使用的V2-99 [13]主干网络已经在深度估计任务上用额外数据进行了预训练，然后由DD3D [27]在nuScenes数据集[2]上进行了微调。相反，我们使用的InternImage [35]主干网络是用来自COCO [20]检测任务的检查点初始化的，没有任何3D预训练。InternImage-B的参数数量与V2-99相似，但更好地反映了现代图像主干网络设计的进步。我们可以观察到，带有InternImage-B主干网络的BEVFormer v2优于所有现有方法，表明在有透视视角监督的情况下，不再需要为单目3D任务预训练的主干网络。带有InternImage-XL的BEVFormer v2以63.4%的NDS和55.6%的mAP在nuScenes相机3D物体检测排行榜上超越了所有参赛者，比第二名的方法BEVStereo高出2.4%的NDS和3.1%的mAP。这一显著提升揭示了释放现代图像主干网络在BEV识别中潜力的巨大好处。

**(表1. BEVFormer v2及其他SOTA方法在nuScenes测试集上的3D检测结果。†表示V2-99 [13]已在深度估计任务上使用额外数据[27]进行预训练。‡表示使用CBGS的方法，这会将1个epoch延长为4.5个epoch。我们选择仅将BEVFormer v2训练24个epoch，以便与先前方法进行公平比较。)**

#### 4.4. 消融实验与分析

##### 4.4.1 透视视角监督的有效性
为了确认透视视角监督的有效性，我们在表2中比较了具有不同视图监督组合的3D检测器，包括：（1）**透视 & BEV**，即提出的BEVFormer v2，一个集成了透视头和BEV头的两阶段检测器。（2）**仅透视**，我们模型中的单阶段透视检测器。（3）**仅BEV**，我们模型中没有混合物体查询的单阶段BEV检测器。（4）**BEV & BEV**，一个带有两个BEV头的两阶段检测器，即用另一个利用BEV特征为混合物体查询提供提案的BEV头替换我们模型中的透视头。

**(表2. 不同视图监督组合的3D检测器在nuScenes验证集上的检测结果。所有模型均在没有时间信息的情况下训练。)**

与“仅透视”检测器相比，“仅BEV”检测器通过利用多视角图像获得了更好的NDS和mAP，但其mATE和mAOE更高，这表明了BEV监督的潜在问题。我们的“透视 & BEV”检测器取得了最佳性能，并以2.5%的NDS和1.9%的mAP超过了“仅BEV”检测器。具体来说，“透视 & BEV”检测器的mATE、mAOE和mAVE显著低于“仅BEV”检测器。这一显著提升主要来自以下两个方面：（1）在普通视觉任务上预训练的主干网络无法捕获3D场景中物体的某些属性，包括深度、方向和速度，而由透视视角监督引导的主干网络能够提取有关这些属性的信息。（2）与固定的物体查询集相比，我们的混合物体查询包含第一阶段的预测作为参考点，帮助BEV头定位目标物体。为了进一步确保提升不是由两阶段流程带来的，我们引入了“BEV & BEV”检测器进行比较。结果表明，“BEV & BEV”与“仅BEV”相当，且无法与“透视 & BEV”相比。因此，只有在透视视角下构建第一阶段头并应用辅助监督才对BEV模型有帮助。

##### 4.4.2 透视视角监督的泛化性
我们期望所提出的透视视角监督能惠及不同架构和尺寸的主干网络。我们在常用于3D物体检测任务的一系列主干网络上构建BEVFormer v2：ResNet [8]、DLA [40]、VoVNet [13]和InternImage [35]。结果报告在表3中。与纯BEV检测器相比，BEVFormer v2（BEV & 透视）对所有主干网络的NDS提升了约3%，mAP提升了约2%，表明它能泛化到不同的架构和模型尺寸。我们认为，额外的透视视角监督可以成为训练BEV模型的一种通用方案，尤其是在适配没有经过任何3D预训练的大规模图像主干网络时。

**(表3. 透视视角监督在不同2D图像主干网络上的结果（nuScenes验证集）。‘仅BEV’和‘透视 & BEV’与表2相同。所有主干网络均使用COCO[20]预训练权重初始化，所有模型均在没有时间信息的情况下训练。)**

##### 4.4.3 训练轮次的选择
我们对“仅BEV”模型和我们的BEVFormer v2（BEV & 透视）进行不同轮次的训练，以观察两种模型达到收敛需要多长时间。表4显示，我们的“BEV & 透视”模型比“仅BEV”模型收敛得更快，证实了辅助透视损失有助于优化。如果训练时间更长，“仅BEV”模型会获得边际提升。但即使在72个epoch时，两个模型之间的差距依然存在，并且可能在更长的训练中也无法消除，这表明仅靠BEV监督无法很好地适配图像主干网络。根据表4，训练48个epoch对我们的模型来说已经足够，我们在其他实验中也固定使用此设置，除非另有说明。

**(表4. 比较仅使用BEV监督和同时使用透视 & BEV监督的模型在不同训练轮次下的表现。模型在nuScenes验证集上评估。所有模型均在没有时间信息的情况下训练。)**

##### 4.4.4 检测头的选择
在我们的BEVFormer v2中可以使用各种类型的透视和BEV检测头。我们探索了几种有代表性的方法，为我们的模型选择最佳方案：对于透视头，候选者是DD3D [27]和DETR3D [37]；对于BEV头，候选者是Deformable DETR [44]和Group DETR [4]。DD3D是一个单阶段无锚框的透视头，对图像特征进行密集的逐像素预测。相反，DETR3D使用3D到2D的查询来采样图像特征并提出稀疏的集合预测。然而，根据我们的定义，它属于透视视角监督，因为它利用图像特征进行最终预测而没有生成BEV特征，即损失直接施加在图像特征上。如表5所示，DD3D在透视头上优于DETR3D，这支持了我们在3.2节的分析。DD3D提供的密集和直接的监督对BEV模型有帮助，而DETR3D的稀疏监督没有克服BEV头的缺点。Group DETR头是Deformable DETR头的扩展，它利用分组的物体查询和组内自注意力。Group DETR在BEV头上取得了更好的性能，但计算成本更高。因此，我们在表1中采用了DD3D头和Group DETR头，而在其他消融实验中保持与BEVFormer [17]相同的Deformable DETR头。

**(表5. BEVFormer v2中不同透视头和BEV头选择的比较。模型在nuScenes验证集上评估。所有模型均在没有时间信息的情况下训练。)**

##### 4.4.5 附加组件和技巧的消融实验
在表6中，我们对BEVFormer v2中使用的附加组件和技巧进行了消融实验，以确认它们对最终结果的贡献，包括：（1）**图像级数据增强（IDA）**。图像被随机水平翻转。（2）**更长的时间间隔**。BEVFormer v2采样历史BEV特征的间隔为2秒，而不是BEVFormer [17]中0.5秒的连续帧。（3）**双向时间编码器**。对于离线3D检测，我们BEVFormer v2中的时间编码器可以利用未来的BEV特征。有了更长的时间间隔，我们的模型可以从不同时间戳的更多自车位置收集信息，这有助于估计物体的方向，并导致mAOE大幅降低。在离线3D检测设置中，双向时间编码器可以提供来自未来帧的额外信息，并大幅提升模型性能。我们还在应用所有附加技巧的情况下对透视视角监督进行了消融。如表6所示，透视视角监督将NDS提升了2.2%，mAP提升了2.6%，这是主要的性能提升来源。

**(表6. BEVFormer v2附加组件和技巧在nuScenes验证集上的消融研究。所有模型均使用ResNet-50主干网络和时间信息进行训练。‘Pers’、‘IDA’、‘Long’和‘Bi’分别表示透视视角监督、图像级数据增强、长时序间隔和双向时间编码器。)**

---

### 5. 结论

现有工作在设计和改进鸟瞰图（BEV）识别模型的检测器方面付出了大量努力，但它们通常固守于特定的预训练主干网络，没有进一步探索。在本文中，我们的目标是在BEV模型上释放现代图像主干网络的全部潜力。我们将适配通用2D图像主干网络的困难归因于BEV检测器的优化问题。为解决此问题，我们通过一个额外的透视视角3D检测头引入辅助损失，从而将透视视角监督引入BEV模型。此外，我们将这两个检测头整合成一个两阶段检测器，即BEVFormer v2。功能完备的透视头提供第一阶段的物体提案，这些提案被编码为BEV头的物体查询，用于第二阶段的预测。广泛的实验验证了我们所提方法的有效性和通用性。透视视角监督引导2D图像主干网络感知自动驾驶的3D场景，并帮助BEV模型实现更快的收敛和更好的性能，并且它适用于广泛的主干网络。此外，我们成功地将大规模主干网络适配到BEVFormer v2，在nuScenes数据集上取得了新的SOTA结果。我们认为，我们的工作为未来研究者探索更好的BEV模型图像主干网络设计铺平了道路。

**局限性。** 由于计算和时间的限制，我们目前没有在更多大规模图像主干网络上测试我们的方法。我们已经在多种主干网络上完成了对我们方法的初步验证，并将在未来扩展模型尺寸。

---

（参考文献和附录部分由于格式复杂且主要为引用和实现细节，此处省略详细翻译，但保留了附录部分的标题和内容梗概。）

**附录 A. 实现细节**
本节介绍了所提方法和实验的更多实现细节。
A.1. 训练设置
A.2. 网络架构
A.3. 第一阶段提案的后处理

**附录 B. 可视化**
我们展示了BEVFormer v2检测器3D物体检测结果的可视化。我们的模型为目标物体预测了准确的3D边界框，即使对于远距离或有遮挡的困难样本也是如此。例如，我们的模型成功检测到右前方相机的远处行人、后方相机中与多辆车重叠的卡车，以及右后方相机中被树遮挡的自行车。