---
{"dg-publish":true,"permalink":"/wiki/notion/theoretical-knowledge/computer-science/paper-intensive-reading/4/"}
---



### 各定理解决的核心问题

1.  **Theorem 1 (Bochner's Theorem - 波赫纳定理):**
    *   **解决的问题：** 提供理论基础。它本身不直接降低复杂度，但它揭示了（特定类型的）核函数计算等价于一个傅里叶变换下的期望。这个发现是“可被近似”的理论基石，它允许我们将一个复杂的、成对的（$O(N^2)$）核函数计算问题，**转化为一个线性（$O(N)$）的特征映射和内积问题**。它告诉我们“为什么”可以这么做。

2.  **Theorem 2 (Positive Fixed Features - PFF - 正固定特征):**
    *   **解决的问题：** 提供一种具体的、可操作的降维方法。它基于 Bochner 定理，给出了如何通过蒙特卡洛或准蒙特卡洛方法进行随机特征采样，来构造一个无偏估计，从而将注意力机制的复杂度从 $O(N^2)$ **降低到** $O(N \cdot m)$（其中 $m$ 是特征维度，$m \ll N$）。它告诉我们“如何”去做这个近似。

3.  **Theorem 3 (Weighted Positive Fixed Features - WPFF - 加权正固定特征):**
    *   **解决的问题：** 提高近似的精度。PFF 使用的是固定的随机特征，其近似效果有好有坏。WPFF 通过引入一个**可学习的参数** $\lambda(x)$，允许模型根据输入数据动态地“加权”这些特征，从而在不增加计算复杂度的情况下，获得一个**误差更小、更精确的近似**。它解决了“如何让近似更有效”的问题。

---

### 各定理的详细解析

### Theorem 1: Bochner's Theorem (波赫纳定理)

**定理陈述：** 连续移位不变核函数 $k(x, y) = k(x-y)$ 是正定的，当且仅当它是在 $\mathbb{R}^d$ 上唯一有限概率测度 $p(\omega)$ 的傅里叶变换。
$$
k(x-y) = \int_{\mathbb{R}^d} p(\omega) e^{i\omega^T(x-y)}d\omega = \mathbb{E}_{\omega \sim p(\omega)}[e^{i\omega^T x} (e^{i\omega^T y})^*]
$$
其中，$^*$ 代表复数共轭。

#### 定理详解

该定理在数学上建立了一座桥梁，连接了两个看似无关的概念：
1.  **核函数的正定性** (一个代数概念，保证了核矩阵的性质)
2.  **概率密度的傅里叶变换** (一个分析概念)

核心思想是，任何满足特定条件的核函数（如高斯核），都可以被看作是许多简单复指数函数的加权和（积分形式），而这个“权重”就是一个概率分布 $p(\omega)$。

这个期望形式 $\mathbb{E}_{\omega \sim p(\omega)}[\dots]$ 是最关键的一步。它暗示我们可以用**采样求平均**的方式来近似计算这个积分，也就是用蒙特卡洛方法。

#### 与复杂度降低的关系

Bochner 定理是后续所有操作的**理论许可证**。它告诉我们，像高斯核这样的 $O(N^2)$ 计算，原则上可以被拆解。我们可以不直接计算 $k(x,y)$，而是通过从 $p(\omega)$ 中采样 $\omega$，然后计算一个低维的特征图 $\phi(x)$，最后通过计算 $\phi(x)^T \phi(y)$ 来近似 $k(x,y)$。这就为将二次复杂度转为线性复杂度铺平了道路。

---

### Theorem 2: Positive Fixed Features (PFF - 正固定特征)

**定理陈述：** PFF 可以表示为高斯核 $k(x,y)$ 的一个无偏估计 $\hat{k}(x,y)$：
$$
\hat{k}(x, y) = \mathbb{E}_{w \sim D}[f(x, w)^T f(y, w)]
$$
式中，$D$ 是渐近均匀分布，$f(x,w)$ 是一个显式的有限维特征图。

#### 定理详解

这个定理是 Bochner 定理思想的具体实践。

*   **无偏估计 (Unbiased Estimate):** 这意味着虽然我们用有限的采样来近似，单次近似的结果 $\hat{k}(x,y)$ 可能有误差，但只要我们采样的次数足够多，其平均值会无限趋近于真实的高斯核函数值 $k(x,y)$。这为近似的准确性提供了数学保证。
*   **特征图 $f(x,w)$:** 这是将原始数据 $x$ 映射到低维空间的“转换器”。这个转换器依赖于从分布 $D$ 中采出的随机变量 $w$。
*   **渐近均匀分布与准蒙特卡洛:** 相比于纯随机的蒙特卡洛采样，准蒙特卡洛使用更均匀的采样点（低差异序列），能用更少的样本点达到更高的积分精度，从而让近似更高效。

#### 与复杂度降低的关系

PFF 将注意力计算的核心步骤从计算一个 $N \times N$ 的相似度矩阵 $A_{ij} = k(q_i, k_j)$，转变为：
1.  计算 Query 和 Key 的特征图：$\Phi(Q) \in \mathbb{R}^{N \times m}$ 和 $\Phi(K) \in \mathbb{R}^{N \times m}$。这一步的复杂度是 $O(N \cdot m \cdot d)$。
2.  计算近似的注意力矩阵：$A' = \Phi(Q) \Phi(K)^T$。

通过改变计算顺序，整个注意力的复杂度从 $O(N^2 d)$ 降至 $O(Nmd)$。当 $m \ll N$ 时，这是一个巨大的提升。

---

### Theorem 3: Weighted Positive Fixed Features (WPFF - 加权正固定特征)

**定理陈述：** WPFF 可以表示为：
$$
\hat{k}_{wpff}(x, y) = \mathbb{E}_{w \sim D}[\lambda(x)^T f(x, w)^T f(y, w) \lambda(y)]
$$
其中，$\lambda$ 是一个可通过输入 $x$ 进行优化的可学习参数。

#### 定理详解

WPFF 是对 PFF 的一个精细化改进。PFF 使用的随机特征是“固定”的（一旦采样就不再改变），这可能不是对所有数据都最优的。WPFF 引入了**可学习的权重** $\lambda(x)$ 和 $\lambda(y)$。

*   **可学习的权重 $\lambda$:** 这个权重允许模型在训练过程中，根据输入数据的特性，动态地调整不同随机特征的重要性。如果某个特征对当前的输入更重要，模型可以给它一个更大的权重。
*   **更小的误差上界:** 如文中提到，这种方法的积分估计误差的上界不大于 PFF。这意味着，在最坏的情况下，它的表现和 PFF 一样好，但通常情况下会因为自适应的学习而表现得**更好**。

#### 频域变换与离散余弦变换 (DCT)

为了进一步加速特征图 $f(x,w)$ 的计算，作者采用了**离散余弦变换 (DCT)**。
DCT 是一种傅里叶相关的变换，它有非常高效的算法（类似快速傅里叶变换FFT），可以将变换的计算复杂度从 $O(d^2)$ 或 $O(md)$ 降低到 $O(d \log d)$。

使用了 DCT 之后的加权映射（即特征图）可以写成：
$$
\phi(x) = \hat{\Lambda} \cdot \text{DCT}(x) \cdot D
$$
其中：
*   $\text{DCT}(x)$ 是对输入 $x$ 进行离散余弦变换。
*   $D$ 是一个随机对角矩阵。
*   $\hat{\Lambda}$ 是可学习的权重。

最终，频域中的核注意力机制可以写成：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{\phi(Q)\phi(K)^T}{\sqrt{d_k}}\right)V
$$
通过这一系列的转换和优化，模型最终实现了一个在计算上极为高效，同时在效果上又能很好地逼近原始注意力机制的线性注意力模型。