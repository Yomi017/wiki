---
{"dg-publish":true,"permalink":"/wiki/notion/theoretical-knowledge/computer-science/artificial-intelligence/concept/mc-based/"}
---

### **核心思想：从Q值的两种定义看Model-Free的实现路径**

我们之前讨论过，为了在没有环境模型（Model-Free）的情况下进行策略改进，关键在于计算动作价值函数 `Q(s, a)`。但问题是，我们究竟该如何计算它呢？

这页笔记将从 `Q(s, a)` 的两种数学表达出发，揭示为什么其中一种定义为我们打开了通往Model-Free强化学习的大门。

#### **定义一：基于模型的贝尔曼期望方程 (Expression 1)**

这个定义是我们最熟悉的贝尔曼方程，它从“一步之后”的情况来分解Q值。

`q_π(s, a) = Σ_r p(r|s, a)r + γ Σ_s' p(s'|s, a)v_π(s')`

*   **含义解读**：
    *   在状态 `s` 执行动作 `a` 的价值等于：
    *   **（所有可能的立即奖励的期望）** + **（折扣后的所有可能的下一状态价值的期望）**
*   **为什么它需要模型？**
    *   要进行这个计算，我们必须知道两个关键的概率分布，也就是**环境模型**：
        1.  `p(r|s, a)`：奖励函数（在s, a后得到奖励r的概率）。
        2.  `p(s'|s, a)`：状态转移函数（在s, a后转移到状态s'的概率）。
*   **结论**：
    *   在Model-Free的设定下，我们**不知道** `p(r|s, a)` 和 `p(s'|s, a)`。因此，这个公式虽然在理论上是正确的，但在实践中我们无法用它来直接计算Q值。这就是PPT上那个红色大叉的含义：**此路不通**。

---

#### **定义二：基于经验的期望回报 (Expression 2)**

这个定义回到了价值函数最原始、最根本的含义。

`q_π(s, a) = E[G_t | S_t = s, A_t = a]`

*   **含义解读**：
    *   `G_t` 代表从t时刻开始的**总回报**（Return），即 `G_t = R_{t+1} + γR_{t+2} + ...`。
    *   整个公式的意思是：“在状态 `s` 执行动作 `a` 的价值” **等于** “从这个时间点开始，我们能够获得的未来总回报的**期望值**”。
    *   它不关心下一步会具体发生什么，只关心从 `(s, a)` 这个起点出发，最终平均能拿到多少总分。
*   **为什么它不需要模型？**
    *   这个定义里完全没有出现 `p(...)` 这样的概率项。它只关心一个最终的统计结果——**期望**。
*   **结论**：
    *   这为我们指明了一条康庄大道！在统计学中，我们如何估算一个未知分布的期望值？答案是：通过**采样（Sampling）**然后求**平均（Averaging）**！

---

#### **从定义到算法：Model-Free的实现思路**

所有Model-Free算法（包括蒙特卡洛、TD学习）的根基，就在于此：

> **我们可以通过让智能体在环境中反复试验，来收集大量的“经验样本”，然后用这些样本的平均回报来近似估算Q值的期望。**

*   **智能体**：你去玩一局游戏吧（生成一个Episode）。
*   **记录员**：在第 `t` 步，你在状态 `s` 执行了动作 `a`。好的，我记下来了。
*   **结算员**：这局游戏结束了，从 `t` 时刻开始，你一共拿到了 `G_t` 的总回报。这也是一个关于 `q(s, a)` 的**样本**。
*   **分析师**：我们让智能体玩了很多很多局。现在，我们把所有在状态 `s` 执行动作 `a` 之后得到的 `G_t` 样本全部收集起来，取一个**平均值**。根据大数定律，这个平均值就会无限接近于真实的 `q(s, a)`！

`q(s, a) ≈ average(G_t)` (在所有访问过 `(s, a)` 的样本中)

这正是蒙特卡洛（MC）方法正在做的事情。它通过运行完整的Episode来获得`G_t`的无偏估计，然后求平均来更新Q值。这完美地实践了“定义二”所揭示的Model-Free学习路径。

**如何通过反复试验来估算Q值**

**我们想知道 `q(s, a)` 的值，但没法直接计算。所以，我们通过反复进行以 `(s, a)` 为起点的试验，记录每次试验的结果，然后用这些结果的平均值来近似 `q(s, a)`。**
#### **1. 生成一个样本 (Generating a Sample)**

*   `Starting from (s, a), following policy π_k, generate an episode.`
    *   **含义**：这是我们进行的一次“**受控实验**”。我们强制让智能体（Agent）从一个特定的状态 `s` 开始，并强制它执行一个特定的动作 `a`。在这之后，智能体就按照它当前的策略 `π_k` 一直玩下去，直到这个回合（episode）结束。
    *   **例子**：在下棋时，我们想评估在某个特定棋盘局面 `s` 下，走“炮二平五”这一步 `a` 的价值。我们就先摆好这个局面，然后让AI走“炮二平五”，之后让AI用它现有的棋力（策略 `π_k`）和自己对弈，直到分出胜负。

#### **2. 记录本次试验的结果：`g(s, a)`**

*   `The return of this episode is g(s, a)`
*   `g(s, a) is a sample of G_t`
*   讲者提问：`我用g(s,a)来表示，g(s,a)是什么呢？`

这里是理解的关键！

*   **`G_t` 是什么？**
    *   `G_t` 是一个**随机变量**。它代表从 `(s, a)` 出发，未来可能获得的**所有**总回报的集合。因为策略 `π_k` 和环境本身都可能有随机性，所以即使起点相同，最终的回报也可能不同。
*   **`g(s, a)` 是什么？**
    *   `g(s, a)` 是我们**刚刚那一次**试验中，实际得到的那个**具体的回报数值**。它是随机变量 `G_t` 的一个**具体的实现**，或者说一个**样本（Sample）**。
*   **类比：抛硬币**
    *   我们想知道一枚硬币正面朝上的**概率 `p(正面)`**。这个 `p(正面)` 就像是 `q(s, a)`，是一个我们想知道但未知的理论值。
    *   我们抛一次硬币，得到的结果是“正面”。这个“正面”的结果，就像是我们做了一次实验得到的具体回报 `g(s, a)`。它只是一个**样本**。
    *   我们不能因为抛了一次是正面，就说 `p(正面)` 等于100%。同样，我们也不能因为一次实验得到了回报 `g(s, a)`，就说 `q(s, a)` 就等于 `g(s, a)`。

#### **3. 从单个样本到群体估计 (From Sample to Estimation)**

*   `Suppose we have a set of episodes and hence {g^(j)(s, a)}.`
    *   **含义**：我们重复了上面那个“受控实验” N 次。每一次我们都从 `(s, a)` 出发，然后让策略 `π_k` 走完，得到了 N 个不同的回报值：`g^(1)(s, a)`, `g^(2)(s, a)`, ..., `g^(N)(s, a)`。
    *   **类比：抛硬币**：我们把硬币抛了 N 次，记录下每一次的结果（正面、反面、正面、正面...）。

*   `q_π_k(s, a) = E[G_t | S_t=s, A_t=a] ≈ (1/N) * Σ g^(i)(s, a)`
    *   **含义**：这正是这页PPT的 punchline（点睛之笔）。
        *   **`q_π_k(s, a)` 的定义**：它的理论值是 `G_t` 的**期望（Expectation）**。
        *   **期望的估算**：根据**大数定律**，我们可以用**样本的平均值**来近似总体的期望。
        *   所以，我们将 N 次实验得到的所有回报 `g^(i)(s, a)` 加起来再除以 N，得到的这个**平均回报**，就是我们对 `q_π_k(s, a)` 的**估计值**。
    *   **类比：抛硬币**：我们抛了1000次硬币，其中有505次是正面。那么我们对 `p(正面)` 的估计就是 `505 / 1000 = 0.505`。这个值非常接近理论值0.5。我们抛的次数越多（N越大），这个估计就越准。

#### **4. 根本思想 (Fundamental Idea)**

*   `Fundamental idea: When model is unavailable, we can use data.`
    *   **总结**：这完美地概括了Model-Free的思想。我们没有地图（model），不知道走每条路会通向哪里。但我们可以亲自去开车（generate episodes），记录每次走完的时间（get samples `g(s,a)`），通过多次记录的平均时间（averaging），来判断哪条路更好（estimate `q(s,a)`）。

---

### **总结**

1.  **目标**：估算 `q_π_k(s, a)`，即动作价值。
2.  **理论**：`q_π_k(s, a)` 的定义是未来总回报 `G_t` 的期望 `E[G_t]`。
3.  **挑战**：我们没有模型，无法用贝尔曼方程直接计算这个期望。
4.  **方法**：利用统计学！期望可以通过**采样求平均**来近似。
5.  **实践**：
    *   做一次实验（生成一个Episode），得到一个回报的**样本** `g(s, a)`。
    *   做 N 次实验，得到 N 个**样本** `{g^(i)(s, a)}`。
    *   计算这 N 个样本的**平均值**，用它来作为 `q_π_k(s, a)` 的估计值。
    *   实验次数 N 越多，估计就越准确。