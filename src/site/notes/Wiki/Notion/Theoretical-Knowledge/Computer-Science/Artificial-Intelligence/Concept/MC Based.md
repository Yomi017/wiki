---
{"dg-publish":true,"permalink":"/wiki/notion/theoretical-knowledge/computer-science/artificial-intelligence/concept/mc-based/"}
---

### **核心思想：从Q值的两种定义看Model-Free的实现路径**

我们之前讨论过，为了在没有环境模型（Model-Free）的情况下进行策略改进，关键在于计算动作价值函数 `Q(s, a)`。但问题是，我们究竟该如何计算它呢？

这页笔记将从 `Q(s, a)` 的两种数学表达出发，揭示为什么其中一种定义为我们打开了通往Model-Free强化学习的大门。

#### **定义一：基于模型的贝尔曼期望方程 (Expression 1)**

这个定义是我们最熟悉的贝尔曼方程，它从“一步之后”的情况来分解Q值。

`q_π(s, a) = Σ_r p(r|s, a)r + γ Σ_s' p(s'|s, a)v_π(s')`

*   **含义解读**：
    *   在状态 `s` 执行动作 `a` 的价值等于：
    *   **（所有可能的立即奖励的期望）** + **（折扣后的所有可能的下一状态价值的期望）**
*   **为什么它需要模型？**
    *   要进行这个计算，我们必须知道两个关键的概率分布，也就是**环境模型**：
        1.  `p(r|s, a)`：奖励函数（在s, a后得到奖励r的概率）。
        2.  `p(s'|s, a)`：状态转移函数（在s, a后转移到状态s'的概率）。
*   **结论**：
    *   在Model-Free的设定下，我们**不知道** `p(r|s, a)` 和 `p(s'|s, a)`。因此，这个公式虽然在理论上是正确的，但在实践中我们无法用它来直接计算Q值。这就是PPT上那个红色大叉的含义：**此路不通**。

---

#### **定义二：基于经验的期望回报 (Expression 2)**

这个定义回到了价值函数最原始、最根本的含义。

`q_π(s, a) = E[G_t | S_t = s, A_t = a]`

*   **含义解读**：
    *   `G_t` 代表从t时刻开始的**总回报**（Return），即 `G_t = R_{t+1} + γR_{t+2} + ...`。
    *   整个公式的意思是：“在状态 `s` 执行动作 `a` 的价值” **等于** “从这个时间点开始，我们能够获得的未来总回报的**期望值**”。
    *   它不关心下一步会具体发生什么，只关心从 `(s, a)` 这个起点出发，最终平均能拿到多少总分。
*   **为什么它不需要模型？**
    *   这个定义里完全没有出现 `p(...)` 这样的概率项。它只关心一个最终的统计结果——**期望**。
*   **结论**：
    *   这为我们指明了一条康庄大道！在统计学中，我们如何估算一个未知分布的期望值？答案是：通过**采样（Sampling）**然后求**平均（Averaging）**！

---

#### **从定义到算法：Model-Free的实现思路**

所有Model-Free算法（包括蒙特卡洛、TD学习）的根基，就在于此：

> **我们可以通过让智能体在环境中反复试验，来收集大量的“经验样本”，然后用这些样本的平均回报来近似估算Q值的期望。**

*   **智能体**：你去玩一局游戏吧（生成一个Episode）。
*   **记录员**：在第 `t` 步，你在状态 `s` 执行了动作 `a`。好的，我记下来了。
*   **结算员**：这局游戏结束了，从 `t` 时刻开始，你一共拿到了 `G_t` 的总回报。这也是一个关于 `q(s, a)` 的**样本**。
*   **分析师**：我们让智能体玩了很多很多局。现在，我们把所有在状态 `s` 执行动作 `a` 之后得到的 `G_t` 样本全部收集起来，取一个**平均值**。根据大数定律，这个平均值就会无限接近于真实的 `q(s, a)`！

`q(s, a) ≈ average(G_t)` (在所有访问过 `(s, a)` 的样本中)

这正是蒙特卡洛（MC）方法正在做的事情。它通过运行完整的Episode来获得`G_t`的无偏估计，然后求平均来更新Q值。这完美地实践了“定义二”所揭示的Model-Free学习路径。