---
{"dg-publish":true,"permalink":"/wiki/notion/theoretical-knowledge/computer-science/artificial-intelligence/rl/"}
---

### 1. 核心要素与马尔可夫决策过程 (MDP)

强化学习 (Reinforcement Learning, RL) 的问题通常被数学化地建模为**马尔可夫决策过程 (Markov Decision Process, MDP)**。一个 MDP 由一个五元组 $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ 定义，它描述了智能体与环境交互的完整框架。

#### 1.1. 智能体 (Agent) 与环境 (Environment)
*   **智能体 (Agent)**: 学习者和决策者。它通过感知环境状态并选择行动来与环境交互。例如，玩游戏的 AI，自动驾驶的汽车，下棋的程序。
*   **环境 (Environment)**: 智能体所处的外部世界，它接收智能体的行动，并以新状态和奖励作为响应。例如，游戏本身，真实的道路，棋盘。

**交互循环过程**:
1.  在时间步 $t$，智能体观察到环境的**状态** $S_t=s$。
2.  智能体根据其**策略** $\pi$ 选择一个**行动** $A_t=a$。
3.  环境接收行动 $a$，发生状态转移，进入新的状态 $S_{t+1}=s'$。
4.  环境向智能体反馈一个即时**奖励** $R_{t+1}=r$。
5.  智能体进入新状态 $s'$，循环继续。

#### 1.2. MDP 的核心组成部分

##### **状态 (State, $s \in \mathcal{S}$)**
*   **状态空间 (State Space) $\mathcal{S}$**: 环境所有可能状态的集合。
*   **状态 (State)**: 对环境在某一时刻的完整描述。一个好的状态必须具备**马尔可夫性质**（见 1.3 节）。
*   **例子**: 在棋类游戏中，状态是棋盘上所有棋子的位置；在机器人导航中，状态可以是机器人的坐标和速度。

##### **行动 (Action, $a \in \mathcal{A}(s)$)**
*   **行动空间 (Action Space) $\mathcal{A}(s)$**: 在状态 $s$ 下，智能体可以采取的所有可能行动的集合。
*   **行动 (Action)**: 智能体可以做出的决策。
*   **例子**: 在棋类游戏中，行动是在某个位置落子；在机器人导航中，行动是“向前”、“向左转”、“向右转”。

##### **环境动态 (Environment Dynamics): 转移与奖励**
环境的动态描述了它如何根据智能体的行动而改变，这由状态转移和奖励两部分共同定义。

*   **状态转移概率 (State Transition Probability, $P$)**:
    *   $P(s' | s, a) = \mathbb{P}[S_{t+1}=s' | S_t=s, A_t=a]$
    *   它表示在状态 $s$ 采取行动 $a$ 后，环境转移到下一个状态 $s'$ 的概率。
    *   **确定性环境**: 如果 $P(s'|s,a)$ 对某个 $s'$ 恒为 1，则环境是确定的。
    *   **随机性环境**: 如果 $P(s'|s,a)$ 是一个概率分布，则环境是随机的（如掷骰子）。

*   **奖励 (Reward, $r \in \mathcal{R}$)**:
    *   奖励是 RL 的核心驱动力，是你为智能体设定的目标，可以被视为一种**人机交互接口 (Human-Machine Interface)**。
    *   在最通用的形式下，奖励由一个**奖励概率分布**描述：$p(r|s,a) = \mathbb{P}[R_{t+1}=r | S_t=s, A_t=a]$。
    *   更常见地，我们使用**奖励函数 (Reward Function)**，它代表期望奖励。根据定义的不同，它可以是：
        *   $R(s, a) = \mathbb{E}[R_{t+1} | S_t=s, A_t=a]$ (奖励只与当前状态和行动有关)
        *   $R(s, a, s') = \mathbb{E}[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$ (奖励与完整的状态转移有关)
    *   **目标**: 智能体的目标不是最大化单步的即时奖励，而是最大化未来的**累积奖励**。

##### **策略 (Policy, $\pi$)**
*   策略是智能体的“大脑”或“行为准则”，它定义了智能体在特定状态下如何选择行动。
    *   $\pi(a|s) = \mathbb{P}[A_t=a | S_t=s]$
*   **确定性策略 (Deterministic Policy)**: $\pi(s) = a$。在每个状态下，选择唯一的行动。
*   **随机性策略 (Stochastic Policy)**: $\pi(a|s)$ 是一个概率分布，在状态 $s$ 下以一定概率选择行动 $a$。
*   **强化学习的目标就是找到一个最优策略 $\pi^*$，使得累积奖励最大化。**

##### **最优策略与最优价值函数**
*   由于对称性等情况，**可能存在多个不同的最优策略**。
*   但是，对于一个给定的 MDP，**最优价值函数 $V^*(s)$ 和最优行动价值函数 $Q^*(s,a)$ 是唯一的**。这意味着能达到的最好结果是唯一的，但达到该结果的方法（策略）可以不唯一。

##### **折扣因子 (Discount Factor, $\gamma$)**
*   $\gamma \in [0, 1]$ 是一个用于衡量未来奖励重要性的超参数。它将未来的奖励进行折现，使得回报（Return）的总和是一个有限值。
    *   $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$
    *   $\gamma \to 0$ 使智能体“短视”，$\gamma \to 1$ 使智能体“有远见”。

#### 1.3. 马尔可夫性质 (The Markov Property)
马尔可夫性质是 MDP 的基石，也被称为**“无记忆性” (Memoryless Property)**。

*   **直观理解**: **未来只与当前有关，而与过去无关**。如果知道了系统当前的状态 $S_t$，那么所有之前的历史信息对于预测未来都不再提供任何额外的信息。

*   **数学定义**: 一个状态 $S_t$ 是马尔可夫的，当且仅当：
    *   $p(s_{t+1} | s_t, a_t) = p(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \dots, s_0, a_0)$
    *   $p(r_{t+1} | s_t, a_t) = p(r_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \dots, s_0, a_0)$

*   **重要性**: 它极大地简化了问题模型，使得我们可以仅根据当前状态来定义价值函数和制定决策。

---

### 2. 交互的实践: 轨迹与任务类型

#### 2.1. 轨迹 (Trajectory) / 经验 (Experience)
**轨迹** (也称 **Episode**, **Rollout**) 是智能体与环境进行一系列连续交互后产生的序列记录。它是所有学习算法赖以运行的数据基础。

*   **定义**: 一条轨迹 $\tau$ 是由状态、行动、奖励组成的序列：
    $$ \tau = (S_0, A_0, R_1, S_1, A_1, R_2, S_2, \dots) $$

#### 2.2. 任务类型与统一框架

*   **分幕式任务 (Episodic Tasks)**: 任务有明确的终点（**终止状态, Terminal State**）。例如，一局棋、一轮游戏。
*   **连续性任务 (Continuing Tasks)**: 任务没有终点，会无限进行下去。例如，机器人维持平衡。

##### **处理目标状态的两种哲学 (Option 1 vs. Option 2)**
我们可以通过不同的建模方式来处理任务的“目标”或“终点”。

*   **Option 1: 吸收状态 (Absorbing State)**:
    *   **做法**: 将所有终止状态转换为一个特殊的**吸收状态**。一旦进入，智能体将永远无法离开，且后续所有奖励均为零。
    *   **效果**: 这是一种将**分幕式任务**在数学上统一为**连续性任务**的技巧。无限回报公式 $G_t = \sum \gamma^k R_{t+k+1}$ 会在进入吸收状态后自然截断，极大地简化了理论和算法设计。

*   **Option 2: 普通奖励状态 (Normal Rewarding State)**:
    *   **做法**: 将目标状态视为一个普通的、可重复访问的状态。智能体在每次**进入**该状态时获得奖励，但任务不结束，它必须选择行动离开。
    *   **效果**: 这定义了一个纯粹的**连续性任务**，目标通常是最大化长期**平均回报率**。这适用于需要持续运行和优化的任务，如资源管理或机器人巡航。

| 特征 | Option 1: 吸收状态 | Option 2: 普通奖励状态 |
| :--- | :--- | :--- |
| **任务类型** | 分幕式任务 (Episodic) | 连续性任务 (Continuing) |
| **目标状态角色**| 任务的**终点 (Terminal State)** | 任务中的**可重复访问的奖励点** |
| **能否离开** | **不能** | **可以** |
| **优化目标** | 在一幕内最大化累积回报 | 在无限时间内最大化平均回报率 |
| **典型应用** | 游戏、有明确成败条件的任务 | 资源管理、过程控制、机器人巡航 |

#### 2.3. 轨迹在学习中的作用
**轨迹是智能体学习的原材料**。不同算法利用轨迹的方式不同：
*   **蒙特卡洛 (MC) 方法**: 使用完整的轨迹 (Episode) 来计算真实回报 $G_t$。
*   **时序差分 (TD) 方法**: 使用轨迹的单步片段 $(S_t, A_t, R_{t+1}, S_{t+1})$ 进行学习，无需等待一幕结束。
*   **策略梯度 (PG) 方法**: 通常也需要完整的轨迹来评估策略性能。
*   **经验回放 (Experience Replay)**: 将大量轨迹的单步片段存储在回放池中，随机采样进行训练，提高样本利用率。

### 2. 目标：回报与折扣因子
#### 2.1. 回报 (Return, $G_t$)

**回报 (Return, $G_t$)** 是在一个**给定的轨迹**中，从时间步 $t$ 开始，智能体未来能获得的所有奖励的总和。它衡量了从某个时间点开始，“后续的整个过程”有多好。

回报是根据智能体**实际经历过的一条轨迹** $\tau = (S_0, A_0, R_1, S_1, A_1, R_2, \dots)$ 来计算的。

**具体计算方式取决于任务类型：**

*   **对于分幕式任务 (Episodic Tasks)**:
    任务会在有限的时间步 $T$ 后结束。回报 $G_t$ 是从 $R_{t+1}$ 到最终奖励 $R_T$ 的简单求和。
    $$ G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T $$
    其中 $T$ 是终止状态对应的时间步。

*   **对于连续性任务 (Continuing Tasks)**:
    任务永远不会结束 ($T \to \infty$)。简单的求和可能会导致回报发散到无穷大。因此，我们必须使用下面将要介绍的**折扣回报**。

#### **示例：使用轨迹计算回报**

让我们回到之前的吃豆人游戏轨迹：
$$ \tau = (S_0, \text{向右}, R_1=+10, S_1, \text{向上}, R_2=+10, S_2, \text{向上}, R_3=-500, S_3) $$
这是一个分幕式任务，在时间步 $T=3$ 结束。我们可以计算这条轨迹中每个时间步的回报：

*   **$G_0$ (从起点开始的回报)**: 这是整个轨迹的总奖励。
    $G_0 = R_1 + R_2 + R_3 = 10 + 10 + (-500) = -480$

*   **$G_1$ (从状态 $S_1$ 开始的回报)**:
    $G_1 = R_2 + R_3 = 10 + (-500) = -490$

*   **$G_2$ (从状态 $S_2$ 开始的回报)**:
    $G_2 = R_3 = -500$

*   **$G_3$ (在终止状态 $S_3$ 的回报)**:
    按照惯例，终止状态之后没有未来奖励，所以回报为 0。

**重要区别**:
*   **回报 $G_t$**: 是从**一条实际发生的轨迹**中计算出的**具体数值**。它是一个样本 (sample)。
*   **价值函数 $V^\pi(s)$**: 是回报的**期望值** $\mathbb{E}[G_t|S_t=s]$。它代表了从状态 $s$ 出发，遵循策略 $\pi$ **平均**能获得的回报，考虑了所有可能的轨迹。

#### 2.2. 折扣因子 (Discount Factor, $\gamma$)

为了统一分幕式和连续性任务，并反映“未来的奖励不如眼前的奖励有价值”这一经济学直觉，我们引入**折扣因子 $\gamma \in [0, 1]$**。

*   **折扣回报 (Discounted Return)**:
    $$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$

    *   这个公式现在对**两种任务类型都适用**。在分幕式任务中，当 $k$ 使得 $t+k+1 > T$ 时，所有后续的 $R$ 都为 0，求和自动终止。
    *   在连续性任务中，只要奖励有界且 $\gamma < 1$，这个无穷级数就能保证收敛，使得回报是一个有限值。

*   **$\gamma$ 的作用**: 通过控制 $\gamma$ 我们可以使得
    *   **$\gamma \to 0$ (短视)**: 智能体只关心即时奖励 $R_{t+1}$。
    *   **$\gamma \to 1$ (远视)**: 智能体对未来奖励和即时奖励几乎同等看待。

**示例：计算折扣回报**

假设 $\gamma = 0.9$，我们重新计算吃豆人轨迹的回报：

*   $G_2 = R_3 = -500$
*   $G_1 = R_2 + \gamma G_2 = 10 + 0.9 \times (-500) = 10 - 450 = -440$
*   $G_0 = R_1 + \gamma G_1 = 10 + 0.9 \times (-440) = 10 - 396 = -386$

注意，回报的计算可以写成这种方便的递归形式：$G_t = R_{t+1} + \gamma G_{t+1}$。

### 3. 价值函数 (Value Function)

为了评估一个策略的好坏，我们需要知道在某个状态或采取某个行动后，预期能获得多少回报。这就是价值函数的作用。

#### 3.1. 状态价值函数 (State-Value Function, $V^\pi(s)$)
*   **定义**: 从状态 $s$ 开始，遵循策略 $\pi$，能够获得的**期望回报**。
*   **公式**:
    $$ V^\pi(s) = \mathbb{E}_\pi [G_t | S_t = s] $$
*   **解读**: $V^\pi(s)$ 回答了“在策略 $\pi$ 下，处于状态 $s$ 有多好？”这个问题。

#### 3.2. 状态-行动价值函数 (Action-Value Function, $Q^\pi(s, a)$)
*   **定义**: 在状态 $s$ 下，采取行动 $a$，然后遵循策略 $\pi$，能够获得的**期望回报**。
*   **公式**:
    $$ Q^\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a] $$
*   **解读**: $Q^\pi(s, a)$ 回答了“在策略 $\pi$ 下，于状态 $s$ 采取行动 $a$ 有多好？”这个问题。$Q$ 函数通常比 $V$ 函数更有用，因为它直接告诉我们选择哪个动作更好。

#### 3.3 联系
*  $$E[G_t|S_t=s]=\sum_aE[G_t|S_t=s,A_t=a]\pi(a|s)$$
$$v_\pi(s)=\sum_a\pi(a|s)q_\pi(s,a)$$

[[Wiki/Notion/Theoretical-Knowledge/Computer-Science/Artificial-Intelligence/Question/策略、轨迹与价值函数的关系 (The relationship between Policy, Trajectory and State Value)\|策略、轨迹与价值函数的关系 (The relationship between Policy, Trajectory and State Value)]]

#### 3.4. 价值函数的估计：从贝尔曼方程到自举

既然价值函数是回报的期望，我们该如何从智能体的经验（轨迹）中估计它呢？核心在于利用价值函数内在的**递归结构**，这个结构由**贝尔曼方程 (Bellman Equation)** 描述。

##### **贝尔曼期望方程 (Bellman Expectation Equation)**

我们首先从折扣回报 $G_t$ 的定义出发：
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
$$
通过提取公因子 $\gamma$，我们可以发现一个关键的递归关系：
$$
G_t = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) = R_{t+1} + \gamma G_{t+1}
$$
这个简单的关系是所有贝尔曼方程的基石。

现在，我们对这个等式两边取期望，就能得到**状态价值函数 $V^\pi(s)$** 之间的关系，这便是**贝尔曼期望方程**：

*   **标量形式 (Scalar Form)**:
    对于任何状态 $s$，其价值等于在该状态下遵循策略 $\pi$ 能获得的期望立即奖励，加上所有可能后继状态 $s'$ 的期望折扣价值。
    $$
    V^\pi(s) = \mathbb{E}_\pi [G_t | S_t = s] = \mathbb{E}_\pi [R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s]
    $$
    更具体地展开：
    $$
    V^\pi(s) = \sum_{a} \pi(a|s) \left( R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \right)
    $$
    这个方程揭示了**一个状态的真实价值 $V^\pi(s)$** 与其**后继状态的真实价值 $V^\pi(s')$** 之间的内在联系。

*   **向量形式 (Vector Form)**:
    如果我们将所有状态的价值视为一个向量 $\mathbf{v}^\pi$，将期望立即奖励视为向量 $\mathbf{r}^\pi$，将策略下的状态转移概率视为矩阵 $\mathbf{P}^\pi$，那么整个系统的贝尔曼期望方程可以被优雅地写成一个线性方程：
    $$
    \mathbf{v}^\pi = \mathbf{r}^\pi + \gamma \mathbf{P}^\pi \mathbf{v}^\pi
    $$
    这个方程存在一个**解析解**: $\mathbf{v}^\pi = (\mathbf{I} - \gamma \mathbf{P}^\pi)^{-1} \mathbf{r}^\pi$。这表明，如果环境模型已知，我们可以直接计算出价值函数。但在大多数RL问题中，模型是未知的，因此我们需要基于样本的学习方法。

##### **方法一：蒙特卡洛 (MC) - 基于样本回报**
MC 方法忽略了贝尔曼方程提供的状态间关系，它直接使用回报 $G_t$ 的定义来估计 $V^\pi(s)$。它通过采样大量完整的 episodes，并对每个状态访问的回报求平均值。
*   **核心思想**: 运行一个完整的 episode，记录下整条轨迹。对于轨迹中的每一个状态 $S_t$，我们都计算出了它后面跟着的真实回报 $G_t$。那么，这个 $G_t$ 就是对 $V^\pi(S_t)$ 的一个无偏估计。
*   **更新方式**: $V(S_t) \leftarrow V(S_t) + \alpha(G_t - V(S_t))$
*   **特点**:
    *   **优点**: **无偏 (Unbiased)**。因为 $G_t$ 是对 $V^\pi(S_t)$ 的真实、无偏差的采样。
    *   **缺点**: **高方差 (High Variance)**。单次轨迹的随机性很大，可能导致估计值剧烈波动。必须**等待一个 episode 结束**才能进行学习。
##### **方法二：自举 (Bootstrapping) - 基于贝尔曼方程**
Bootstrapping 的意思是“拔靴带”，引申为“自我提升”。在RL中，它指**利用贝尔曼方程的结构，使用当前的价值估计来更新价值估计**。这是**时序差分 (Temporal Difference, TD)** 学习方法的核心。

*   **核心思想**:
    贝尔曼方程告诉我们 $V^\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s]$。TD 学习正是利用这一点。它不再等待整个 episode 结束来获得完整的 $G_t$，而是只走一步得到一个样本 $(S_t, A_t, R_{t+1}, S_{t+1})$。然后，它用**当前的、不完美的价值估计 $V(S_{t+1})$** 来构造一个对 $V^\pi(S_t)$ 的新估计，这个新估计被称为 **TD 目标 (TD Target)**。

*   **学习目标 (TD Target)**:
    $$
    \text{TD Target} = R_{t+1} + \gamma V(S_{t+1})
    $$
    这个目标是**自举**的，因为它部分基于真实的样本 ($R_{t+1}$)，部分基于一个**自身的旧有估计** ($V(S_{t+1})$)。

*   **更新方式**:
    TD 算法通过让当前的价值估计 $V(S_t)$ 朝着这个 TD 目标移动一小步来进行学习。
    $$
    V(S_t) \leftarrow V(S_t) + \alpha \left( \underbrace{(R_{t+1} + \gamma V(S_{t+1}))}_{\text{TD Target}} - V(S_t) \right)
    $$
    括号中的部分 $(R_{t+1} + \gamma V(S_{t+1})) - V(S_t)$ 被称为**时序差分误差 (TD Error)**。

*   **特点**:
    *   **优点**: **低方差 (Low Variance)**。更新不依赖于一条完整的随机轨迹，更平滑。可以**在线学习 (Online)**，每一步都更新，效率高。
    *   **缺点**: **有偏 (Biased)**。因为学习目标 $V(S_{t+1})$ 本身就是一个不准确的估计。你在用一个估计去更新另一个估计。当初始估计不准时，这个偏差可能会在系统中传播。

**总结**:
*   **MC**: 使用**实际回报** $G_t$ 作为更新目标，直接对 $V(s) = \mathbb{E}[G_t]$ 进行采样。
*   **TD (Bootstrapping)**: 使用**估计的回报** $R_{t+1} + \gamma V(S_{t+1})$ 作为更新目标，利用了 $V(s) = \mathbb{E}[R_{t+1} + \gamma V(S_{t+1})]$ 的贝尔曼关系。

这种“用估计更新估计”的自举思想是现代强化学习算法（如Q-Learning, SARSA, Actor-Critic）的基石。

##### 对比蒙特卡洛 (MC) 与自举 (TD): 同一公式，两种哲学

我们的目标是估计状态 $S_t$ 的价值 $V(S_t)$。我们知道它的真实定义是 $V(S_t) = \mathbb{E}[G_t]$，并且回报 $G_t$ 满足递归关系：

$$
G_t = R_{t+1} + \gamma G_{t+1}
$$

假设我们当前处于状态 $S_t$，想要更新 $V(S_t)$。

#### 1. 蒙特卡洛 (MC) 的计算方式：“求到底”

蒙特卡洛方法看到这个递归公式，它的处理方式是**彻底展开，直到终点**。

对于 MC 来说， $G_{t+1}$ 不是一个可以直接使用的值，它只是一个中间符号。要得到 $G_t$ 的具体数值，必须不断地把递归式展开：
$$
\begin{aligned}
G_t &= R_{t+1} + \gamma G_{t+1} \\
    &= R_{t+1} + \gamma (R_{t+2} + \gamma G_{t+2}) \\
    &= R_{t+1} + \gamma R_{t+2} + \gamma^2 G_{t+2} \\
    &= R_{t+1} + \gamma R_{t+2} + \gamma^2 (R_{t+3} + \dots) \\
    &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{T-t-1} R_T
\end{aligned}
$$

**MC 的计算流程**:
1.  **等待**: 从 $S_t$ 开始，必须**完成整个 episode**，收集到未来所有的奖励 $R_{t+1}, R_{t+2}, \dots, R_T$。
2.  **求和**: 将所有收集到的未来奖励进行折扣求和，计算出 $G_t$ 的**具体、完整的样本值**。
3.  **更新**: 使用这个完整的样本值 $G_t$ 作为 $V(S_t)$ 的学习目标。
    $$
    V(S_t) \leftarrow V(S_t) + \alpha ( \underbrace{(R_{t+1} + \gamma R_{t+2} + \dots)}_{\text{完整的真实回报 } G_t} - V(S_t) )
    $$

**MC 的哲学**: 要想知道 $G_t$ 是多少，就必须老老实实地把未来的每一步都走完，拿到每一个真实的奖励，然后加起来。它是对回报定义的**直接采样**。

---

#### 2. 时序差分 (TD) / 自举 (Bootstrapping) 的计算方式：“信一步，靠估计”

时序差分 (TD) 方法看到同一个递归公式 $G_t = R_{t+1} + \gamma G_{t+1}$，它的处理方式是**只走一步，然后用现有的估计值来近似后续部分**。

对于 TD 来说，它并不关心 $G_{t+1}$ 的内部结构是什么。它做了一个大胆的假设：

> "虽然我不知道 $G_{t+1}$ 的真实值是多少，但我有一个对它的期望值的估计，那就是我当前的价值函数 $V(S_{t+1})$。我就用这个**估计**来代替**真实的** $G_{t+1}$ 吧！"

**TD 的计算流程**:
1.  **走一步**: 从 $S_t$ 开始，只需要执行一个行动，获得**一个**立即奖励 $R_{t+1}$ 和**一个**下一个状态 $S_{t+1}$。
2.  **查表/估计**: 查找或计算出下一个状态的**当前价值估计** $V(S_{t+1})$。
3.  **构建目标**: 将真实的立即奖励 $R_{t+1}$ 和对未来的价值估计 $V(S_{t+1})$ 结合起来，构建一个**自举**的学习目标 (TD Target)。
    $$
    \text{TD Target} = R_{t+1} + \gamma V(S_{t+1})
    $$
    在这里，$V(S_{t+1})$ 被用作对 $G_{t+1}$ 的一个**代理 (proxy)** 或 **估计 (estimate)**。
4.  **更新**: 使用这个混合了真实与估计的目标来更新 $V(S_t)$。
    $$
    V(S_t) \leftarrow V(S_t) + \alpha ( \underbrace{(R_{t+1} + \gamma V(S_{t+1}))}_{\text{TD Target, 一个估计的回报}} - V(S_t) )
    $$

**TD 的哲学**: 我不需要知道未来的所有细节。我只需要知道我下一步能得到的真实奖励，以及我**相信**的、从那以后能得到的总价值。我用我**当前的信念 (current belief)** 来更新我**之前的信念 (previous belief)**。

---

### 总结对比

| 特征 | 蒙特卡洛 (MC) | 时序差分 (TD) / 自举 |
| :--- | :--- | :--- |
| **对 $G_t = R_{t+1} + \gamma G_{t+1}$ 的处理** | **完全展开**，计算 $G_t$ 的真实样本值。 | **只展开一步**，用 $V(S_{t+1})$ 估计 $G_{t+1}$。 |
| **更新目标 (Target)** | $G_t$ (完整的、实际的回报) | $R_{t+1} + \gamma V(S_{t+1})$ (单步真实奖励 + 未来价值的估计) |
| **何时更新** | **Episode 结束时** | **每走一步 (online)** |
| **偏差 (Bias)** | **无偏** (目标是真实回报的无偏样本) | **有偏** (目标依赖于一个可能不准的估计 $V(S_{t+1})$) |
| **方差 (Variance)** | **高方差** (依赖于一整条随机轨迹) | **低方差** (只依赖于一步的随机性) |


---

### 4. 贝尔曼方程 (Bellman Equations)

贝尔曼方程是 RL 中最重要的公式之一，它将一个状态的价值与其后继状态的价值关联起来，提供了迭代求解价值函数的方法。

#### 4.1. 贝尔曼期望方程 (Bellman Expectation Equation)
*   **$V^\pi$ 的贝尔曼方程**:
    $$ V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V^\pi(s') \right] $$
    $$\Rightarrow V_\pi(s)=\sum_a\pi(a|s)[\sum_rp(r|s,a)r+\gamma\sum_{s'}p(s'|s,a)V_\pi(s')]$$
    $$\Rightarrow q_\pi(s,a)=\sum_rp(r|s,a)r+\gamma\sum_{s'}p(s'|s,a)V_\pi(s')$$
    $$V^\pi(s) = \underbrace{\sum_a \pi(a|s) \left[ \sum_r p(r|s,a)r \right]}_{\text{期望的立即奖励}} + \gamma \underbrace{\sum_a \pi(a|s) \sum_{s'} p(s'|s,a)V^\pi(s')}_{\text{期望的未来价值}}$$
    $$=\sum_a \pi(a|s) \left( \sum_r p(r|s,a)r + \gamma{ \sum_{s'} p(s'|s,a)V^\pi(s')}\right)$$
    $$=r_\pi(s)+\gamma\sum_{s'}p_\pi(s'|s)v_\pi(s')$$

	  [[Wiki/Notion/Theoretical-Knowledge/Computer-Science/Artificial-Intelligence/Question/两个贝尔曼公式的互推 (The mutual derivation of two Bellman formulas)\|两个贝尔曼公式的互推 (The mutual derivation of two Bellman formulas)]]
	*   当前状态的价值 = 所有可能行动的期望价值。
    *   某个行动的价值 = 所有可能后继状态的（即时奖励 + 折扣后的未来价值）的期望。

*   **$Q^\pi$ 的贝尔曼方程**:
    $$ Q^\pi(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a') \right] $$

#### 4.2. 贝尔曼最优方程 (Bellman Optimality Equation)
最优策略 $\pi^*$ 对应最优价值函数 $V^*$ 和 $Q^*$。
*   **$V^*$ 的贝尔曼最优方程**:
    $$ V^*(s) = \max_{a} \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right] $$
    *   最优状态价值等于所有行动中能带来的**最大**期望回报。

*   **$Q^*$ 的贝尔曼最优方程**:
    $$ Q^*(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \max_{a'} Q^*(s', a') \right] $$
    *   如果知道了最优 $Q^*$ 函数，那么最优策略就是**贪心策略 (Greedy Policy)**：
        $$ \pi^*(s) = \arg\max_{a} Q^*(s, a) $$

**RL 算法的核心任务，就是通过各种方法求解贝尔曼最优方程，找到 $Q^*$ 或 $\pi^*$。**

---

### 5. RL 算法分类



#### 5.1. Model-Based vs. Model-Free
*   **基于模型 (Model-Based)**: 智能体尝试学习或已知环境的模型（即状态转移概率 $P$ 和奖励函数 $R$），然后利用模型进行规划（如动态规划）。
*   **无模型 (Model-Free)**: 智能体不学习环境模型，直接通过与环境的交互经验来学习策略或价值函数。这是目前更主流的方法。

#### 5.2. Value-Based vs. Policy-Based
*   **基于价值 (Value-Based)**: 学习价值函数（通常是 $Q$ 函数），然后根据价值函数隐式地得到策略（如贪心策略）。
    *   **代表算法**: Q-Learning, Sarsa, DQN。
*   **基于策略 (Policy-Based)**: 直接学习策略函数 $\pi(a|s)$，即直接参数化策略。
    *   **代表算法**: REINFORCE, Policy Gradients。
*   **演员-评论家 (Actor-Critic)**: 结合以上两者。**Actor** (演员) 负责学习策略，**Critic** (评论家) 负责学习价值函数来评估 Actor 的表现，并指导其更新。
    *   **代表算法**: A2C, A3C, DDPG, SAC。

#### 5.3. On-Policy vs. Off-Policy
*   **同策略 (On-Policy)**: 学习和决策使用同一个策略。即用来评估和改进的策略，就是当前智能体正在执行的策略。
    *   **特点**: 稳健，但探索性差，样本利用率低。
    *   **代表算法**: Sarsa, A2C。
*   **异策略 (Off-Policy)**: 学习和决策使用不同的策略。智能体可以利用过去（甚至其他智能体）的经验数据来学习当前的目标策略。
    *   **特点**: 样本利用率高，探索性强，但可能不稳定。
    *   **代表算法**: Q-Learning, DQN, DDPG。
