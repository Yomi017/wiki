---
{"dg-publish":true,"permalink":"/wiki/notion/theoretical-knowledge/computer-science/artificial-intelligence/rl/"}
---

### 1. 核心要素与马尔可夫决策过程 (MDP)

RL 的问题通常被建模为**马尔可夫决策过程 (Markov Decision Process, MDP)**。一个 MDP 由以下五个核心要素组成元组 $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$。

#### 1.1. 智能体 (Agent) 与环境 (Environment)
*   **智能体 (Agent)**: 学习者和决策者。例如，玩游戏的 AI，自动驾驶的汽车，下棋的程序。
*   **环境 (Environment)**: 智能体所处的外部世界，智能体与之交互。例如，游戏本身，真实的道路，棋盘。

交互过程是一个循环：
1.  智能体在状态 $s$ 观察环境。
2.  智能体根据其策略 $\pi$ 选择一个行动 $a$。
3.  环境接收行动 $a$，更新其状态，从 $s$ 转移到新状态 $s'$。
4.  环境向智能体反馈一个**奖励 (Reward)** $r$。
5.  智能体进入新状态 $s'$，循环继续。



#### 1.2. 状态 (State, $s \in \mathcal{S}$)
你已经提到了**状态空间 (State Space) $\mathcal{S}$**，它是所有可能状态的集合。
*   **状态 (State)** 是对环境在某一时刻的完整描述。一个好的状态需要具备**马尔可夫性质 (Markov Property)**，即**当前状态 $s_t$ 包含了所有与未来决策相关的历史信息**。换句话说，$P(s_{t+1}|s_t, a_t) = P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...)$。知道了当前状态，过去的历史就不再重要。
*   **例子**:
    *   在棋类游戏中，状态是棋盘上所有棋子的位置。
    *   在机器人导航中，状态可以是机器人的坐标和速度。

#### 1.3. 行动 (Action, $a \in \mathcal{A}(s)$)
你提到了**行动空间 (Action Space) $\mathcal{A}(s)$**，它是在状态 $s$ 下，智能体可以采取的所有可能行动的集合。
*   **行动 (Action)** 是智能体可以做出的决策。
*   **例子**:
    *   在棋类游戏中，行动是在某个位置落子。
    *   在机器人导航中，行动是“向前”、“向左转”、“向右转”。

#### 1.4. 状态转移概率 (State Transition Probability, $P$)
你提到了**状态转移 (State Transition)** $s \overset{a}{\rightarrow} s'$。在 MDP 中，我们用一个概率函数来描述它。
*   **状态转移概率 $P(s' | s, a)$**: 表示在状态 $s$ 下采取行动 $a$ 后，转移到下一个状态 $s'$ 的概率。
*   $P(s' | s, a) = \mathbb{P}[S_{t+1}=s' | S_t=s, A_t=a]$
*   **确定性环境 (Deterministic)**: 如果采取某个动作后，下一个状态是唯一确定的，则 $P(s' | s, a)$ 对于某个 $s'$ 是 1，对其他所有状态是 0。
*   **随机性环境 (Stochastic)**: 如果采取某个动作后，下一个状态是随机的（例如，掷骰子），则 $P$ 是一个概率分布。

#### 1.5. 奖励 (Reward, $r \in R$)
这是 RL 的核心驱动力，实际上是一种 **human-machine interface**。
*   **奖励函数 (Reward Function) $R(s, a, s')$**: 表示智能体在状态 $s$ 采取行动 $a$ 并转移到状态 $s'$ 后，从环境获得的即时奖励。有时也简化为 $R(s, a)$ 或 $R(s)$。
*   **奖励信号 (Reward Signal)** 是一个标量反馈，它告诉智能体当前这一步做得“好”还是“不好”。
*   **目标**: 智能体的目标不是最大化单步的即时奖励，而是最大化**累积奖励**。
*   **例子**:
    *   在游戏中，击败敌人奖励 +10，死亡惩罚 -100，其他情况奖励 0。
    *   在下棋中，只有在游戏结束时才有奖励：赢了 +1，输了 -1，平局 0。
*   **奖励只和现在的状态和行为有关，和下一个状态无关**

#### 1.6. 策略 (Policy, $\pi$)
策略是智能体的“大脑”或“行为准则”，它定义了智能体在特定状态下如何选择行动。
*   **策略 $\pi$**: 是从状态到行动的映射。
*   **确定性策略 (Deterministic Policy)**: $\pi(s) = a$。在每个状态下，选择唯一的行动。
*   **随机性策略 (Stochastic Policy)**: $\pi(a|s) = P(A_t=a | S_t=s)$。在状态 $s$ 下，以一定概率选择某个行动 $a$。
*   **由于部分对称情况，可能有多个最优策略。但是对于一个给定的马尔可夫决策过程 (MDP)，最优价值函数 $V∗(s)$ 和最优行动价值函数 $Q∗(s,a)$  是唯一的**

**强化学习的目标就是找到一个最优策略 $\pi^*$，使得累积奖励最大化。**



#### 1.7. 轨迹 (Trajectory) / 经验 (Experience)

**轨迹** (也称为 **Episode**, **Rollout** 或 **Sample Path**) 是智能体与环境进行一系列连续交互后产生的序列记录。它完整地描述了从一个起始状态开始，直到某个时间点（或任务结束）所经历的一切。

*   **定义**: 一条轨迹 $\tau$ 是由状态、行动、奖励组成的序列：
    $$ \tau = (S_0, A_0, R_1, S_1, A_1, R_2, S_2, \dots) $$

*   **轨迹与任务类型**:
    *   **分幕式任务 (Episodic Tasks)**: 任务有明确的起点和终点（**终止状态, Terminal State**）。例如，一局棋、一轮游戏。在这种情况下，一条轨迹就构成一个完整的 **幕 (Episode)**。
    *   **连续性任务 (Continuing Tasks)**: 任务没有终点，会无限进行下去。例如，一个控制机器人保持平衡的程序。在这种情况下，轨迹是一个无限长的序列。

##### **吸收状态 (Absorbing State) 与任务的统一**

一个非常巧妙的技巧是，我们可以通过一种统一的数学框架来处理这两种任务：**将分幕式任务转换为一种特殊的连续性任务**。

这通过引入 **吸收状态 (Absorbing State)** 的概念来实现。我们可以将所有**终止状态 (Terminal State)** 都视作一个特殊的吸收状态，它具有以下特点：

1.  **永远无法离开**: 一旦智能体进入吸收状态，它就会永远停留在那里。对于任何行动 $a$，状态都会转移回自身。
    $$ P(s_{terminal} | s_{terminal}, a) = 1 \quad \text{for all } a $$

2.  **后续奖励为零**: 进入吸收状态后，所有后续的奖励都为零。
    $$ R(s_{terminal}, a, s_{terminal}) = 0 \quad \text{for all } a $$

**这样做的好处是什么？**

通过这种方式，一个分幕式任务的“结束”，就等价于在一个连续性任务中进入了一个永远循环且没有收益的状态。这使得用于计算折扣回报 $G_t$ 的公式在两种任务下都完全一致，无需特殊处理。

$$ G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$

当一个分幕式任务在时间步 $T$ 结束时，智能体进入吸收状态 $S_T$。那么对于 $t \ge T$ 的所有时间步，奖励 $R_{t+1}$ 都将是 0。因此，无限求和的公式自然地截断了，与分幕式任务的有限求和结果完全等价。这极大地简化了算法的设计和理论分析。

##### 轨迹在强化学习中的作用

**轨迹是智能体学习的原材料和数据来源。** 几乎所有的强化学习算法都依赖于从轨迹（或其片段）中学习。

1.  **数据基础**: 轨迹构成了智能体所能获得的全部“经验”。智能体的目标——最大化累积奖励——正是基于这些轨迹中的奖励来计算的。

2.  **算法的基石**:
    *   **蒙特卡洛 (Monte Carlo, MC) 方法**: 直接使用**完整的轨迹 (Episode)**。算法会等待一幕结束后，计算该幕中每个时间步的真实回报 $G_t$，然后用这个回报来更新价值函数。
    *   **时序差分 (Temporal Difference, TD) 方法**: 使用轨迹的**片段**，即**单步转移 (Transition)**。像 Q-Learning 和 Sarsa 这样的算法，每走一步 $(S_t, A_t, R_{t+1}, S_{t+1})$ 就会进行一次学习和更新，而不需要等待一幕结束。
    *   **策略梯度 (Policy Gradient) 方法**: 通常也需要完整的轨迹来估计策略的性能，并计算梯度以优化策略。例如，REINFORCE 算法就需要一条完整的轨迹来计算其中每一步行动的价值。
    *   **经验回放 (Experience Replay)**: 在像 DQN 这样的算法中，智能体将经历的单步转移 $(S_t, A_t, R_{t+1}, S_{t+1})$ 存储在一个大的“回放池”里。这个回放池本质上是**大量轨迹的碎片集合**。训练时，从中随机抽取小批量数据进行学习。

##### 简单示例

假设一个简单的吃豆人游戏：
*   $S_0$: 游戏开始，豆人在左下角。
*   $A_0$: `向右`
*   $R_1$: +10 (吃掉一个豆子)
*   $S_1$: 豆人向右移动一格，那个豆子消失了。
*   $A_1$: `向上`
*   $R_2$: +10 (又吃掉一个豆子)
*   $S_2$: 豆人向上移动一格。
*   $A_2$: `向上`
*   $R_3$: -500 (撞到鬼怪)
*   $S_3$: 游戏结束 (终止状态)。

那么，这一幕 (Episode) 对应的轨迹就是：
$$ \tau = (S_0, \text{向右}, +10, S_1, \text{向上}, +10, S_2, \text{向上}, -500, S_3) $$

这条轨迹就是智能体宝贵的学习经验，它告诉智能体：在 $S_2$ 状态下选择“向上”这个行动，最终导致了一个非常糟糕的结果。

 轨迹是连接所有基本概念的纽带。它是**智能体与环境交互过程的实例化记录**，是所有学习算法赖以运行的数据基础。在你的笔记体系中，可以把它放在描述完 MDP 的五元组之后，作为这些元素如何实际运作的第一个具体例子。

---

### 2. 目标：回报与折扣因子
#### 2.1. 回报 (Return, $G_t$)

**回报 (Return, $G_t$)** 是在一个**给定的轨迹**中，从时间步 $t$ 开始，智能体未来能获得的所有奖励的总和。它衡量了从某个时间点开始，“后续的整个过程”有多好。

回报是根据智能体**实际经历过的一条轨迹** $\tau = (S_0, A_0, R_1, S_1, A_1, R_2, \dots)$ 来计算的。

**具体计算方式取决于任务类型：**

*   **对于分幕式任务 (Episodic Tasks)**:
    任务会在有限的时间步 $T$ 后结束。回报 $G_t$ 是从 $R_{t+1}$ 到最终奖励 $R_T$ 的简单求和。
    $$ G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T $$
    其中 $T$ 是终止状态对应的时间步。

*   **对于连续性任务 (Continuing Tasks)**:
    任务永远不会结束 ($T \to \infty$)。简单的求和可能会导致回报发散到无穷大。因此，我们必须使用下面将要介绍的**折扣回报**。

#### **示例：使用轨迹计算回报**

让我们回到之前的吃豆人游戏轨迹：
$$ \tau = (S_0, \text{向右}, R_1=+10, S_1, \text{向上}, R_2=+10, S_2, \text{向上}, R_3=-500, S_3) $$
这是一个分幕式任务，在时间步 $T=3$ 结束。我们可以计算这条轨迹中每个时间步的回报：

*   **$G_0$ (从起点开始的回报)**: 这是整个轨迹的总奖励。
    $G_0 = R_1 + R_2 + R_3 = 10 + 10 + (-500) = -480$

*   **$G_1$ (从状态 $S_1$ 开始的回报)**:
    $G_1 = R_2 + R_3 = 10 + (-500) = -490$

*   **$G_2$ (从状态 $S_2$ 开始的回报)**:
    $G_2 = R_3 = -500$

*   **$G_3$ (在终止状态 $S_3$ 的回报)**:
    按照惯例，终止状态之后没有未来奖励，所以回报为 0。

**重要区别**:
*   **回报 $G_t$**: 是从**一条实际发生的轨迹**中计算出的**具体数值**。它是一个样本 (sample)。
*   **价值函数 $V^\pi(s)$**: 是回报的**期望值** $\mathbb{E}[G_t|S_t=s]$。它代表了从状态 $s$ 出发，遵循策略 $\pi$ **平均**能获得的回报，考虑了所有可能的轨迹。

#### 2.2. 折扣因子 (Discount Factor, $\gamma$)

为了统一分幕式和连续性任务，并反映“未来的奖励不如眼前的奖励有价值”这一经济学直觉，我们引入**折扣因子 $\gamma \in [0, 1]$**。

*   **折扣回报 (Discounted Return)**:
    $$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$

    *   这个公式现在对**两种任务类型都适用**。在分幕式任务中，当 $k$ 使得 $t+k+1 > T$ 时，所有后续的 $R$ 都为 0，求和自动终止。
    *   在连续性任务中，只要奖励有界且 $\gamma < 1$，这个无穷级数就能保证收敛，使得回报是一个有限值。

*   **$\gamma$ 的作用**: 通过控制 $\gamma$ 我们可以使得
    *   **$\gamma \to 0$ (短视)**: 智能体只关心即时奖励 $R_{t+1}$。
    *   **$\gamma \to 1$ (远视)**: 智能体对未来奖励和即时奖励几乎同等看待。

**示例：计算折扣回报**

假设 $\gamma = 0.9$，我们重新计算吃豆人轨迹的回报：

*   $G_2 = R_3 = -500$
*   $G_1 = R_2 + \gamma G_2 = 10 + 0.9 \times (-500) = 10 - 450 = -440$
*   $G_0 = R_1 + \gamma G_1 = 10 + 0.9 \times (-440) = 10 - 396 = -386$

注意，回报的计算可以写成这种方便的递归形式：$G_t = R_{t+1} + \gamma G_{t+1}$。

### 3. 价值函数 (Value Function)

为了评估一个策略的好坏，我们需要知道在某个状态或采取某个行动后，预期能获得多少回报。这就是价值函数的作用。

#### 3.1. 状态价值函数 (State-Value Function, $V^\pi(s)$)
*   **定义**: 从状态 $s$ 开始，遵循策略 $\pi$，能够获得的**期望回报**。
*   **公式**:
    $$ V^\pi(s) = \mathbb{E}_\pi [G_t | S_t = s] $$
*   **解读**: $V^\pi(s)$ 回答了“在策略 $\pi$ 下，处于状态 $s$ 有多好？”这个问题。

#### 3.2. 状态-行动价值函数 (Action-Value Function, $Q^\pi(s, a)$)
*   **定义**: 在状态 $s$ 下，采取行动 $a$，然后遵循策略 $\pi$，能够获得的**期望回报**。
*   **公式**:
    $$ Q^\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a] $$
*   **解读**: $Q^\pi(s, a)$ 回答了“在策略 $\pi$ 下，于状态 $s$ 采取行动 $a$ 有多好？”这个问题。$Q$ 函数通常比 $V$ 函数更有用，因为它直接告诉我们选择哪个动作更好。

---

### 4. 贝尔曼方程 (Bellman Equations)

贝尔曼方程是 RL 中最重要的公式之一，它将一个状态的价值与其后继状态的价值关联起来，提供了迭代求解价值函数的方法。

#### 4.1. 贝尔曼期望方程 (Bellman Expectation Equation)
*   **$V^\pi$ 的贝尔曼方程**:
    $$ V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V^\pi(s') \right] $$
    *   当前状态的价值 = 所有可能行动的期望价值。
    *   某个行动的价值 = 所有可能后继状态的（即时奖励 + 折扣后的未来价值）的期望。

*   **$Q^\pi$ 的贝尔曼方程**:
    $$ Q^\pi(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a') \right] $$

#### 4.2. 贝尔曼最优方程 (Bellman Optimality Equation)
最优策略 $\pi^*$ 对应最优价值函数 $V^*$ 和 $Q^*$。
*   **$V^*$ 的贝尔曼最优方程**:
    $$ V^*(s) = \max_{a} \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right] $$
    *   最优状态价值等于所有行动中能带来的**最大**期望回报。

*   **$Q^*$ 的贝尔曼最优方程**:
    $$ Q^*(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \max_{a'} Q^*(s', a') \right] $$
    *   如果知道了最优 $Q^*$ 函数，那么最优策略就是**贪心策略 (Greedy Policy)**：
        $$ \pi^*(s) = \arg\max_{a} Q^*(s, a) $$

**RL 算法的核心任务，就是通过各种方法求解贝尔曼最优方程，找到 $Q^*$ 或 $\pi^*$。**

---

### 5. RL 算法分类



#### 5.1. Model-Based vs. Model-Free
*   **基于模型 (Model-Based)**: 智能体尝试学习或已知环境的模型（即状态转移概率 $P$ 和奖励函数 $R$），然后利用模型进行规划（如动态规划）。
*   **无模型 (Model-Free)**: 智能体不学习环境模型，直接通过与环境的交互经验来学习策略或价值函数。这是目前更主流的方法。

#### 5.2. Value-Based vs. Policy-Based
*   **基于价值 (Value-Based)**: 学习价值函数（通常是 $Q$ 函数），然后根据价值函数隐式地得到策略（如贪心策略）。
    *   **代表算法**: Q-Learning, Sarsa, DQN。
*   **基于策略 (Policy-Based)**: 直接学习策略函数 $\pi(a|s)$，即直接参数化策略。
    *   **代表算法**: REINFORCE, Policy Gradients。
*   **演员-评论家 (Actor-Critic)**: 结合以上两者。**Actor** (演员) 负责学习策略，**Critic** (评论家) 负责学习价值函数来评估 Actor 的表现，并指导其更新。
    *   **代表算法**: A2C, A3C, DDPG, SAC。

#### 5.3. On-Policy vs. Off-Policy
*   **同策略 (On-Policy)**: 学习和决策使用同一个策略。即用来评估和改进的策略，就是当前智能体正在执行的策略。
    *   **特点**: 稳健，但探索性差，样本利用率低。
    *   **代表算法**: Sarsa, A2C。
*   **异策略 (Off-Policy)**: 学习和决策使用不同的策略。智能体可以利用过去（甚至其他智能体）的经验数据来学习当前的目标策略。
    *   **特点**: 样本利用率高，探索性强，但可能不稳定。
    *   **代表算法**: Q-Learning, DQN, DDPG。
