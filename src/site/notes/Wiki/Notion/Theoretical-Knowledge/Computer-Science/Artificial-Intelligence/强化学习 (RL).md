---
{"dg-publish":true,"permalink":"/wiki/notion/theoretical-knowledge/computer-science/artificial-intelligence/rl/"}
---

### 1. 核心要素与马尔可夫决策过程 (MDP)

RL 的问题通常被建模为**马尔可夫决策过程 (Markov Decision Process, MDP)**。一个 MDP 由以下五个核心要素组成元组 $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$。

#### 1.1. 智能体 (Agent) 与环境 (Environment)
*   **智能体 (Agent)**: 学习者和决策者。例如，玩游戏的 AI，自动驾驶的汽车，下棋的程序。
*   **环境 (Environment)**: 智能体所处的外部世界，智能体与之交互。例如，游戏本身，真实的道路，棋盘。

交互过程是一个循环：
1.  智能体在状态 $s$ 观察环境。
2.  智能体根据其策略 $\pi$ 选择一个行动 $a$。
3.  环境接收行动 $a$，更新其状态，从 $s$ 转移到新状态 $s'$。
4.  环境向智能体反馈一个**奖励 (Reward)** $r$。
5.  智能体进入新状态 $s'$，循环继续。



#### 1.2. 状态 (State, $s \in \mathcal{S}$)
你已经提到了**状态空间 (State Space) $\mathcal{S}$**，它是所有可能状态的集合。
*   **状态 (State)** 是对环境在某一时刻的完整描述。一个好的状态需要具备**马尔可夫性质 (Markov Property)**，即**当前状态 $s_t$ 包含了所有与未来决策相关的历史信息**。换句话说，$P(s_{t+1}|s_t, a_t) = P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...)$。知道了当前状态，过去的历史就不再重要。
*   **例子**:
    *   在棋类游戏中，状态是棋盘上所有棋子的位置。
    *   在机器人导航中，状态可以是机器人的坐标和速度。

#### 1.3. 行动 (Action, $a \in \mathcal{A}(s)$)
你提到了**行动空间 (Action Space) $\mathcal{A}(s)$**，它是在状态 $s$ 下，智能体可以采取的所有可能行动的集合。
*   **行动 (Action)** 是智能体可以做出的决策。
*   **例子**:
    *   在棋类游戏中，行动是在某个位置落子。
    *   在机器人导航中，行动是“向前”、“向左转”、“向右转”。

#### 1.4. 状态转移概率 (State Transition Probability, $P$)
你提到了**状态转移 (State Transition)** $s \overset{a}{\rightarrow} s'$。在 MDP 中，我们用一个概率函数来描述它。
*   **状态转移概率 $P(s' | s, a)$**: 表示在状态 $s$ 下采取行动 $a$ 后，转移到下一个状态 $s'$ 的概率。
*   $P(s' | s, a) = \mathbb{P}[S_{t+1}=s' | S_t=s, A_t=a]$
*   **确定性环境 (Deterministic)**: 如果采取某个动作后，下一个状态是唯一确定的，则 $P(s' | s, a)$ 对于某个 $s'$ 是 1，对其他所有状态是 0。
*   **随机性环境 (Stochastic)**: 如果采取某个动作后，下一个状态是随机的（例如，掷骰子），则 $P$ 是一个概率分布。

#### 1.5. 奖励 (Reward, $r \in R$)
这是 RL 的核心驱动力，也是你笔记中缺失的关键一环。
*   **奖励函数 (Reward Function) $R(s, a, s')$**: 表示智能体在状态 $s$ 采取行动 $a$ 并转移到状态 $s'$ 后，从环境获得的即时奖励。有时也简化为 $R(s, a)$ 或 $R(s)$。
*   **奖励信号 (Reward Signal)** 是一个标量反馈，它告诉智能体当前这一步做得“好”还是“不好”。
*   **目标**: 智能体的目标不是最大化单步的即时奖励，而是最大化**累积奖励**。
*   **例子**:
    *   在游戏中，击败敌人奖励 +10，死亡惩罚 -100，其他情况奖励 0。
    *   在下棋中，只有在游戏结束时才有奖励：赢了 +1，输了 -1，平局 0。

#### 1.6. 策略 (Policy, $\pi$)
策略是智能体的“大脑”或“行为准则”，它定义了智能体在特定状态下如何选择行动。
*   **策略 $\pi$**: 是从状态到行动的映射。
*   **确定性策略 (Deterministic Policy)**: $\pi(s) = a$。在每个状态下，选择唯一的行动。
*   **随机性策略 (Stochastic Policy)**: $\pi(a|s) = P(A_t=a | S_t=s)$。在状态 $s$ 下，以一定概率选择某个行动 $a$。

**强化学习的目标就是找到一个最优策略 $\pi^*$，使得累积奖励最大化。**

---

### 2. 目标：回报与折扣因子

#### 2.1. 回报 (Return, $G_t$)
**回报**是从时间步 $t$ 开始，未来所有奖励的总和。
$$ G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots $$

#### 2.2. 折扣因子 (Discount Factor, $\gamma$)
未来的奖励没有当前的奖励那么有价值。为了平衡短期和长期奖励，我们引入**折扣因子 $\gamma \in [0, 1]$**。
*   **折扣回报 (Discounted Return)**:
    $$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$
*   **$\gamma$ 的作用**:
    *   当 $\gamma \to 0$ 时，智能体变得“短视”，只关心即时奖励。
    *   当 $\gamma \to 1$ 时，智能体变得“有远见”，未来的奖励和当前的奖励几乎同等重要。
    *   数学上，$\gamma < 1$ 确保了在无限循环的任务中，回报 $G_t$ 是一个有限值。

---

### 3. 价值函数 (Value Function)

为了评估一个策略的好坏，我们需要知道在某个状态或采取某个行动后，预期能获得多少回报。这就是价值函数的作用。

#### 3.1. 状态价值函数 (State-Value Function, $V^\pi(s)$)
*   **定义**: 从状态 $s$ 开始，遵循策略 $\pi$，能够获得的**期望回报**。
*   **公式**:
    $$ V^\pi(s) = \mathbb{E}_\pi [G_t | S_t = s] $$
*   **解读**: $V^\pi(s)$ 回答了“在策略 $\pi$ 下，处于状态 $s$ 有多好？”这个问题。

#### 3.2. 状态-行动价值函数 (Action-Value Function, $Q^\pi(s, a)$)
*   **定义**: 在状态 $s$ 下，采取行动 $a$，然后遵循策略 $\pi$，能够获得的**期望回报**。
*   **公式**:
    $$ Q^\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a] $$
*   **解读**: $Q^\pi(s, a)$ 回答了“在策略 $\pi$ 下，于状态 $s$ 采取行动 $a$ 有多好？”这个问题。$Q$ 函数通常比 $V$ 函数更有用，因为它直接告诉我们选择哪个动作更好。

---

### 4. 贝尔曼方程 (Bellman Equations)

贝尔曼方程是 RL 中最重要的公式之一，它将一个状态的价值与其后继状态的价值关联起来，提供了迭代求解价值函数的方法。

#### 4.1. 贝尔曼期望方程 (Bellman Expectation Equation)
*   **$V^\pi$ 的贝尔曼方程**:
    $$ V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V^\pi(s') \right] $$
    *   当前状态的价值 = 所有可能行动的期望价值。
    *   某个行动的价值 = 所有可能后继状态的（即时奖励 + 折扣后的未来价值）的期望。

*   **$Q^\pi$ 的贝尔曼方程**:
    $$ Q^\pi(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a') \right] $$

#### 4.2. 贝尔曼最优方程 (Bellman Optimality Equation)
最优策略 $\pi^*$ 对应最优价值函数 $V^*$ 和 $Q^*$。
*   **$V^*$ 的贝尔曼最优方程**:
    $$ V^*(s) = \max_{a} \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right] $$
    *   最优状态价值等于所有行动中能带来的**最大**期望回报。

*   **$Q^*$ 的贝尔曼最优方程**:
    $$ Q^*(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \max_{a'} Q^*(s', a') \right] $$
    *   如果知道了最优 $Q^*$ 函数，那么最优策略就是**贪心策略 (Greedy Policy)**：
        $$ \pi^*(s) = \arg\max_{a} Q^*(s, a) $$

**RL 算法的核心任务，就是通过各种方法求解贝尔曼最优方程，找到 $Q^*$ 或 $\pi^*$。**

---

### 5. RL 算法分类



#### 5.1. Model-Based vs. Model-Free
*   **基于模型 (Model-Based)**: 智能体尝试学习或已知环境的模型（即状态转移概率 $P$ 和奖励函数 $R$），然后利用模型进行规划（如动态规划）。
*   **无模型 (Model-Free)**: 智能体不学习环境模型，直接通过与环境的交互经验来学习策略或价值函数。这是目前更主流的方法。

#### 5.2. Value-Based vs. Policy-Based
*   **基于价值 (Value-Based)**: 学习价值函数（通常是 $Q$ 函数），然后根据价值函数隐式地得到策略（如贪心策略）。
    *   **代表算法**: Q-Learning, Sarsa, DQN。
*   **基于策略 (Policy-Based)**: 直接学习策略函数 $\pi(a|s)$，即直接参数化策略。
    *   **代表算法**: REINFORCE, Policy Gradients。
*   **演员-评论家 (Actor-Critic)**: 结合以上两者。**Actor** (演员) 负责学习策略，**Critic** (评论家) 负责学习价值函数来评估 Actor 的表现，并指导其更新。
    *   **代表算法**: A2C, A3C, DDPG, SAC。

#### 5.3. On-Policy vs. Off-Policy
*   **同策略 (On-Policy)**: 学习和决策使用同一个策略。即用来评估和改进的策略，就是当前智能体正在执行的策略。
    *   **特点**: 稳健，但探索性差，样本利用率低。
    *   **代表算法**: Sarsa, A2C。
*   **异策略 (Off-Policy)**: 学习和决策使用不同的策略。智能体可以利用过去（甚至其他智能体）的经验数据来学习当前的目标策略。
    *   **特点**: 样本利用率高，探索性强，但可能不稳定。
    *   **代表算法**: Q-Learning, DQN, DDPG。

---

希望这份整理后的笔记能帮助你建立一个更清晰、更全面的强化学习知识体系！