---
{"dg-publish":true,"permalink":"/wiki/notion/theoretical-knowledge/computer-science/artificial-intelligence/rl/"}
---

### 1. 核心要素与马尔可夫决策过程 (MDP)

强化学习 (Reinforcement Learning, RL) 的问题通常被数学化地建模为**马尔可夫决策过程 (Markov Decision Process, MDP)**。一个 MDP 由一个五元组 $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ 定义，它描述了智能体与环境交互的完整框架。

#### 1.1. 智能体 (Agent) 与环境 (Environment)
*   **智能体 (Agent)**: 学习者和决策者。它通过感知环境状态并选择行动来与环境交互。例如，玩游戏的 AI，自动驾驶的汽车，下棋的程序。
*   **环境 (Environment)**: 智能体所处的外部世界，它接收智能体的行动，并以新状态和奖励作为响应。例如，游戏本身，真实的道路，棋盘。

**交互循环过程**:
1.  在时间步 $t$，智能体观察到环境的**状态** $S_t=s$。
2.  智能体根据其**策略** $\pi$ 选择一个**行动** $A_t=a$。
3.  环境接收行动 $a$，发生状态转移，进入新的状态 $S_{t+1}=s'$。
4.  环境向智能体反馈一个即时**奖励** $R_{t+1}=r$。
5.  智能体进入新状态 $s'$，循环继续。

#### 1.2. MDP 的核心组成部分

##### **状态 (State, $s \in \mathcal{S}$)**
*   **状态空间 (State Space) $\mathcal{S}$**: 环境所有可能状态的集合。
*   **状态 (State)**: 对环境在某一时刻的完整描述。一个好的状态必须具备**马尔可夫性质**（见 1.3 节）。
*   **例子**: 在棋类游戏中，状态是棋盘上所有棋子的位置；在机器人导航中，状态可以是机器人的坐标和速度。

##### **行动 (Action, $a \in \mathcal{A}(s)$)**
*   **行动空间 (Action Space) $\mathcal{A}(s)$**: 在状态 $s$ 下，智能体可以采取的所有可能行动的集合。
*   **行动 (Action)**: 智能体可以做出的决策。
*   **例子**: 在棋类游戏中，行动是在某个位置落子；在机器人导航中，行动是“向前”、“向左转”、“向右转”。

##### **环境动态 (Environment Dynamics): 转移与奖励**
环境的动态描述了它如何根据智能体的行动而改变，这由状态转移和奖励两部分共同定义。

*   **状态转移概率 (State Transition Probability, $P$)**:
    *   $P(s' | s, a) = \mathbb{P}[S_{t+1}=s' | S_t=s, A_t=a]$
    *   它表示在状态 $s$ 采取行动 $a$ 后，环境转移到下一个状态 $s'$ 的概率。
    *   **确定性环境**: 如果 $P(s'|s,a)$ 对某个 $s'$ 恒为 1，则环境是确定的。
    *   **随机性环境**: 如果 $P(s'|s,a)$ 是一个概率分布，则环境是随机的（如掷骰子）。

*   **奖励 (Reward, $r \in \mathcal{R}$)**:
    *   奖励是 RL 的核心驱动力，是你为智能体设定的目标，可以被视为一种**人机交互接口 (Human-Machine Interface)**。
    *   在最通用的形式下，奖励由一个**奖励概率分布**描述：$p(r|s,a) = \mathbb{P}[R_{t+1}=r | S_t=s, A_t=a]$。
    *   更常见地，我们使用**奖励函数 (Reward Function)**，它代表期望奖励。根据定义的不同，它可以是：
        *   $R(s, a) = \mathbb{E}[R_{t+1} | S_t=s, A_t=a]$ (奖励只与当前状态和行动有关)
        *   $R(s, a, s') = \mathbb{E}[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$ (奖励与完整的状态转移有关)
    *   **目标**: 智能体的目标不是最大化单步的即时奖励，而是最大化未来的**累积奖励**。

##### **策略 (Policy, $\pi$)**
*   策略是智能体的“大脑”或“行为准则”，它定义了智能体在特定状态下如何选择行动。
    *   $\pi(a|s) = \mathbb{P}[A_t=a | S_t=s]$
*   **确定性策略 (Deterministic Policy)**: $\pi(s) = a$。在每个状态下，选择唯一的行动。
*   **随机性策略 (Stochastic Policy)**: $\pi(a|s)$ 是一个概率分布，在状态 $s$ 下以一定概率选择行动 $a$。
*   **强化学习的目标就是找到一个最优策略 $\pi^*$，使得累积奖励最大化。**

##### **最优策略与最优价值函数**
*   由于对称性等情况，**可能存在多个不同的最优策略**。
*   但是，对于一个给定的 MDP，**最优价值函数 $V^*(s)$ 和最优行动价值函数 $Q^*(s,a)$ 是唯一的**。这意味着能达到的最好结果是唯一的，但达到该结果的方法（策略）可以不唯一。

##### **折扣因子 (Discount Factor, $\gamma$)**
*   $\gamma \in [0, 1]$ 是一个用于衡量未来奖励重要性的超参数。它将未来的奖励进行折现，使得回报（Return）的总和是一个有限值。
    *   $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$
    *   $\gamma \to 0$ 使智能体“短视”，$\gamma \to 1$ 使智能体“有远见”。

#### 1.3. 马尔可夫性质 (The Markov Property)
马尔可夫性质是 MDP 的基石，也被称为**“无记忆性” (Memoryless Property)**。

*   **直观理解**: **未来只与当前有关，而与过去无关**。如果知道了系统当前的状态 $S_t$，那么所有之前的历史信息对于预测未来都不再提供任何额外的信息。

*   **数学定义**: 一个状态 $S_t$ 是马尔可夫的，当且仅当：
    *   $p(s_{t+1} | s_t, a_t) = p(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \dots, s_0, a_0)$
    *   $p(r_{t+1} | s_t, a_t) = p(r_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \dots, s_0, a_0)$

*   **重要性**: 它极大地简化了问题模型，使得我们可以仅根据当前状态来定义价值函数和制定决策。

---

### 2. 交互的实践: 轨迹与任务类型

#### 2.1. 轨迹 (Trajectory) / 经验 (Experience)
**轨迹** (也称 **Episode**, **Rollout**) 是智能体与环境进行一系列连续交互后产生的序列记录。它是所有学习算法赖以运行的数据基础。

*   **定义**: 一条轨迹 $\tau$ 是由状态、行动、奖励组成的序列：
    $$ \tau = (S_0, A_0, R_1, S_1, A_1, R_2, S_2, \dots) $$

#### 2.2. 任务类型与统一框架

*   **分幕式任务 (Episodic Tasks)**: 任务有明确的终点（**终止状态, Terminal State**）。例如，一局棋、一轮游戏。
*   **连续性任务 (Continuing Tasks)**: 任务没有终点，会无限进行下去。例如，机器人维持平衡。

##### **处理目标状态的两种哲学 (Option 1 vs. Option 2)**
我们可以通过不同的建模方式来处理任务的“目标”或“终点”。

*   **Option 1: 吸收状态 (Absorbing State)**:
    *   **做法**: 将所有终止状态转换为一个特殊的**吸收状态**。一旦进入，智能体将永远无法离开，且后续所有奖励均为零。
    *   **效果**: 这是一种将**分幕式任务**在数学上统一为**连续性任务**的技巧。无限回报公式 $G_t = \sum \gamma^k R_{t+k+1}$ 会在进入吸收状态后自然截断，极大地简化了理论和算法设计。

*   **Option 2: 普通奖励状态 (Normal Rewarding State)**:
    *   **做法**: 将目标状态视为一个普通的、可重复访问的状态。智能体在每次**进入**该状态时获得奖励，但任务不结束，它必须选择行动离开。
    *   **效果**: 这定义了一个纯粹的**连续性任务**，目标通常是最大化长期**平均回报率**。这适用于需要持续运行和优化的任务，如资源管理或机器人巡航。

| 特征 | Option 1: 吸收状态 | Option 2: 普通奖励状态 |
| :--- | :--- | :--- |
| **任务类型** | 分幕式任务 (Episodic) | 连续性任务 (Continuing) |
| **目标状态角色**| 任务的**终点 (Terminal State)** | 任务中的**可重复访问的奖励点** |
| **能否离开** | **不能** | **可以** |
| **优化目标** | 在一幕内最大化累积回报 | 在无限时间内最大化平均回报率 |
| **典型应用** | 游戏、有明确成败条件的任务 | 资源管理、过程控制、机器人巡航 |

#### 2.3. 轨迹在学习中的作用
**轨迹是智能体学习的原材料**。不同算法利用轨迹的方式不同：
*   **蒙特卡洛 (MC) 方法**: 使用完整的轨迹 (Episode) 来计算真实回报 $G_t$。
*   **时序差分 (TD) 方法**: 使用轨迹的单步片段 $(S_t, A_t, R_{t+1}, S_{t+1})$ 进行学习，无需等待一幕结束。
*   **策略梯度 (PG) 方法**: 通常也需要完整的轨迹来评估策略性能。
*   **经验回放 (Experience Replay)**: 将大量轨迹的单步片段存储在回放池中，随机采样进行训练，提高样本利用率。

### 2. 目标：回报与折扣因子
#### 2.1. 回报 (Return, $G_t$)

**回报 (Return, $G_t$)** 是在一个**给定的轨迹**中，从时间步 $t$ 开始，智能体未来能获得的所有奖励的总和。它衡量了从某个时间点开始，“后续的整个过程”有多好。

回报是根据智能体**实际经历过的一条轨迹** $\tau = (S_0, A_0, R_1, S_1, A_1, R_2, \dots)$ 来计算的。

**具体计算方式取决于任务类型：**

*   **对于分幕式任务 (Episodic Tasks)**:
    任务会在有限的时间步 $T$ 后结束。回报 $G_t$ 是从 $R_{t+1}$ 到最终奖励 $R_T$ 的简单求和。
    $$ G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T $$
    其中 $T$ 是终止状态对应的时间步。

*   **对于连续性任务 (Continuing Tasks)**:
    任务永远不会结束 ($T \to \infty$)。简单的求和可能会导致回报发散到无穷大。因此，我们必须使用下面将要介绍的**折扣回报**。

#### **示例：使用轨迹计算回报**

让我们回到之前的吃豆人游戏轨迹：
$$ \tau = (S_0, \text{向右}, R_1=+10, S_1, \text{向上}, R_2=+10, S_2, \text{向上}, R_3=-500, S_3) $$
这是一个分幕式任务，在时间步 $T=3$ 结束。我们可以计算这条轨迹中每个时间步的回报：

*   **$G_0$ (从起点开始的回报)**: 这是整个轨迹的总奖励。
    $G_0 = R_1 + R_2 + R_3 = 10 + 10 + (-500) = -480$

*   **$G_1$ (从状态 $S_1$ 开始的回报)**:
    $G_1 = R_2 + R_3 = 10 + (-500) = -490$

*   **$G_2$ (从状态 $S_2$ 开始的回报)**:
    $G_2 = R_3 = -500$

*   **$G_3$ (在终止状态 $S_3$ 的回报)**:
    按照惯例，终止状态之后没有未来奖励，所以回报为 0。

**重要区别**:
*   **回报 $G_t$**: 是从**一条实际发生的轨迹**中计算出的**具体数值**。它是一个样本 (sample)。
*   **价值函数 $V^\pi(s)$**: 是回报的**期望值** $\mathbb{E}[G_t|S_t=s]$。它代表了从状态 $s$ 出发，遵循策略 $\pi$ **平均**能获得的回报，考虑了所有可能的轨迹。

#### 2.2. 折扣因子 (Discount Factor, $\gamma$)

为了统一分幕式和连续性任务，并反映“未来的奖励不如眼前的奖励有价值”这一经济学直觉，我们引入**折扣因子 $\gamma \in [0, 1]$**。

*   **折扣回报 (Discounted Return)**:
    $$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$

    *   这个公式现在对**两种任务类型都适用**。在分幕式任务中，当 $k$ 使得 $t+k+1 > T$ 时，所有后续的 $R$ 都为 0，求和自动终止。
    *   在连续性任务中，只要奖励有界且 $\gamma < 1$，这个无穷级数就能保证收敛，使得回报是一个有限值。

*   **$\gamma$ 的作用**: 通过控制 $\gamma$ 我们可以使得
    *   **$\gamma \to 0$ (短视)**: 智能体只关心即时奖励 $R_{t+1}$。
    *   **$\gamma \to 1$ (远视)**: 智能体对未来奖励和即时奖励几乎同等看待。

**示例：计算折扣回报**

假设 $\gamma = 0.9$，我们重新计算吃豆人轨迹的回报：

*   $G_2 = R_3 = -500$
*   $G_1 = R_2 + \gamma G_2 = 10 + 0.9 \times (-500) = 10 - 450 = -440$
*   $G_0 = R_1 + \gamma G_1 = 10 + 0.9 \times (-440) = 10 - 396 = -386$

注意，回报的计算可以写成这种方便的递归形式：$G_t = R_{t+1} + \gamma G_{t+1}$。

### 3. 价值函数 (Value Function)

为了评估一个策略的好坏，我们需要知道在某个状态或采取某个行动后，预期能获得多少回报。这就是价值函数的作用。

#### 3.1. 状态价值函数 (State-Value Function, $V^\pi(s)$)
*   **定义**: 从状态 $s$ 开始，遵循策略 $\pi$，能够获得的**期望回报**。
*   **公式**:
    $$ V^\pi(s) = \mathbb{E}_\pi [G_t | S_t = s] $$
*   **解读**: $V^\pi(s)$ 回答了“在策略 $\pi$ 下，处于状态 $s$ 有多好？”这个问题。

#### 3.2. 状态-行动价值函数 (Action-Value Function, $Q^\pi(s, a)$)
*   **定义**: 在状态 $s$ 下，采取行动 $a$，然后遵循策略 $\pi$，能够获得的**期望回报**。
*   **公式**:
    $$ Q^\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a] $$
*   **解读**: $Q^\pi(s, a)$ 回答了“在策略 $\pi$ 下，于状态 $s$ 采取行动 $a$ 有多好？”这个问题。$Q$ 函数通常比 $V$ 函数更有用，因为它直接告诉我们选择哪个动作更好。

---

### 4. 贝尔曼方程 (Bellman Equations)

贝尔曼方程是 RL 中最重要的公式之一，它将一个状态的价值与其后继状态的价值关联起来，提供了迭代求解价值函数的方法。

#### 4.1. 贝尔曼期望方程 (Bellman Expectation Equation)
*   **$V^\pi$ 的贝尔曼方程**:
    $$ V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V^\pi(s') \right] $$
    *   当前状态的价值 = 所有可能行动的期望价值。
    *   某个行动的价值 = 所有可能后继状态的（即时奖励 + 折扣后的未来价值）的期望。

*   **$Q^\pi$ 的贝尔曼方程**:
    $$ Q^\pi(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a') \right] $$

#### 4.2. 贝尔曼最优方程 (Bellman Optimality Equation)
最优策略 $\pi^*$ 对应最优价值函数 $V^*$ 和 $Q^*$。
*   **$V^*$ 的贝尔曼最优方程**:
    $$ V^*(s) = \max_{a} \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right] $$
    *   最优状态价值等于所有行动中能带来的**最大**期望回报。

*   **$Q^*$ 的贝尔曼最优方程**:
    $$ Q^*(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \max_{a'} Q^*(s', a') \right] $$
    *   如果知道了最优 $Q^*$ 函数，那么最优策略就是**贪心策略 (Greedy Policy)**：
        $$ \pi^*(s) = \arg\max_{a} Q^*(s, a) $$

**RL 算法的核心任务，就是通过各种方法求解贝尔曼最优方程，找到 $Q^*$ 或 $\pi^*$。**

---

### 5. RL 算法分类



#### 5.1. Model-Based vs. Model-Free
*   **基于模型 (Model-Based)**: 智能体尝试学习或已知环境的模型（即状态转移概率 $P$ 和奖励函数 $R$），然后利用模型进行规划（如动态规划）。
*   **无模型 (Model-Free)**: 智能体不学习环境模型，直接通过与环境的交互经验来学习策略或价值函数。这是目前更主流的方法。

#### 5.2. Value-Based vs. Policy-Based
*   **基于价值 (Value-Based)**: 学习价值函数（通常是 $Q$ 函数），然后根据价值函数隐式地得到策略（如贪心策略）。
    *   **代表算法**: Q-Learning, Sarsa, DQN。
*   **基于策略 (Policy-Based)**: 直接学习策略函数 $\pi(a|s)$，即直接参数化策略。
    *   **代表算法**: REINFORCE, Policy Gradients。
*   **演员-评论家 (Actor-Critic)**: 结合以上两者。**Actor** (演员) 负责学习策略，**Critic** (评论家) 负责学习价值函数来评估 Actor 的表现，并指导其更新。
    *   **代表算法**: A2C, A3C, DDPG, SAC。

#### 5.3. On-Policy vs. Off-Policy
*   **同策略 (On-Policy)**: 学习和决策使用同一个策略。即用来评估和改进的策略，就是当前智能体正在执行的策略。
    *   **特点**: 稳健，但探索性差，样本利用率低。
    *   **代表算法**: Sarsa, A2C。
*   **异策略 (Off-Policy)**: 学习和决策使用不同的策略。智能体可以利用过去（甚至其他智能体）的经验数据来学习当前的目标策略。
    *   **特点**: 样本利用率高，探索性强，但可能不稳定。
    *   **代表算法**: Q-Learning, DQN, DDPG。
